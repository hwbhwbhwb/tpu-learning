#loc = loc(unknown)
module @yolov5s attributes {module.FLOPs = 16683511600 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "bm1684x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.mode = "F16", module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_LOWERED", module.top_run_mode = "STATIC", module.weight_file = "yolov5s_tpu_lowered_bm1684x_f16_weight.npz"} {
  func.func @main(%arg0: tensor<1x3x640x640xf32> loc(unknown)) -> (tensor<1x3x80x80x85xf32>, tensor<1x3x40x40x85xf32>, tensor<1x3x20x20x85xf32>) {
    %0 = "top.None"() : () -> none loc(#loc)
    %1 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [640, 640], scale = [0.0039216000586748123, 0.0039216000586748123, 0.0039216000586748123]} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640xf32> loc(#loc1)
    %2 = "tpu.Cast"(%1) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640xf16> loc(#loc2)
    %3 = "top.Weight"() : () -> tensor<32xf32> loc(#loc3)
    %4 = "top.Weight"() : () -> tensor<32x3x6x6xf16> loc(#loc4)
    %5 = "tpu.Conv2D"(%2, %4, %3) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [6, 6], kernel_zp = 0 : i64, pads = [2, 2, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x640x640xf16>, tensor<32x3x6x6xf16>, tensor<32xf32>) -> tensor<1x32x320x320xf16> loc(#loc5)
    %6 = "tpu.Cast"(%5) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x320x320xf16>) -> tensor<1x32x320x320xf32> loc(#loc6)
    %7 = "tpu.Active"(%6) {mode = #tpu<active_mode SILU>} : (tensor<1x32x320x320xf32>) -> tensor<1x32x320x320xf32> loc(#loc7)
    %8 = "tpu.Cast"(%7) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x320x320xf32>) -> tensor<1x32x320x320xf16> loc(#loc8)
    %9 = "top.Weight"() : () -> tensor<64xf32> loc(#loc9)
    %10 = "top.Weight"() : () -> tensor<64x32x3x3xf16> loc(#loc10)
    %11 = "tpu.Conv2D"(%8, %10, %9) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x320x320xf16>, tensor<64x32x3x3xf16>, tensor<64xf32>) -> tensor<1x64x160x160xf16> loc(#loc11)
    %12 = "tpu.Cast"(%11) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf16>) -> tensor<1x64x160x160xf32> loc(#loc12)
    %13 = "tpu.Active"(%12) {mode = #tpu<active_mode SILU>} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf32> loc(#loc13)
    %14 = "tpu.Cast"(%13) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf16> loc(#loc14)
    %15 = "top.Weight"() : () -> tensor<32xf32> loc(#loc15)
    %16 = "top.Weight"() : () -> tensor<32x64x1x1xf16> loc(#loc16)
    %17 = "tpu.Conv2D"(%14, %16, %15) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<32x64x1x1xf16>, tensor<32xf32>) -> tensor<1x32x160x160xf16> loc(#loc17)
    %18 = "tpu.Cast"(%17) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc18)
    %19 = "tpu.Active"(%18) {mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc19)
    %20 = "tpu.Cast"(%19) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc20)
    %21 = "top.Weight"() : () -> tensor<32xf32> loc(#loc21)
    %22 = "top.Weight"() : () -> tensor<32x32x1x1xf16> loc(#loc22)
    %23 = "tpu.Conv2D"(%20, %22, %21) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160xf16>, tensor<32x32x1x1xf16>, tensor<32xf32>) -> tensor<1x32x160x160xf16> loc(#loc23)
    %24 = "tpu.Cast"(%23) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc24)
    %25 = "tpu.Active"(%24) {mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc25)
    %26 = "tpu.Cast"(%25) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc26)
    %27 = "top.Weight"() : () -> tensor<32xf32> loc(#loc27)
    %28 = "top.Weight"() : () -> tensor<32x32x3x3xf16> loc(#loc28)
    %29 = "tpu.Conv2D"(%26, %28, %27) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160xf16>, tensor<32x32x3x3xf16>, tensor<32xf32>) -> tensor<1x32x160x160xf16> loc(#loc29)
    %30 = "tpu.Cast"(%29) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc30)
    %31 = "tpu.Active"(%30) {mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc31)
    %32 = "tpu.Cast"(%31) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc32)
    %33 = "tpu.Add"(%20, %32) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x160x160xf16>, tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf16> loc(#loc33)
    %34 = "top.Weight"() : () -> tensor<32xf32> loc(#loc34)
    %35 = "top.Weight"() : () -> tensor<32x64x1x1xf16> loc(#loc35)
    %36 = "tpu.Conv2D"(%14, %35, %34) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<32x64x1x1xf16>, tensor<32xf32>) -> tensor<1x32x160x160xf16> loc(#loc36)
    %37 = "tpu.Cast"(%36) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc37)
    %38 = "tpu.Active"(%37) {mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc38)
    %39 = "tpu.Cast"(%38) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc39)
    %40 = "tpu.Concat"(%33, %39) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x160x160xf16>, tensor<1x32x160x160xf16>) -> tensor<1x64x160x160xf16> loc(#loc40)
    %41 = "top.Weight"() : () -> tensor<64xf32> loc(#loc41)
    %42 = "top.Weight"() : () -> tensor<64x64x1x1xf16> loc(#loc42)
    %43 = "tpu.Conv2D"(%40, %42, %41) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<64x64x1x1xf16>, tensor<64xf32>) -> tensor<1x64x160x160xf16> loc(#loc43)
    %44 = "tpu.Cast"(%43) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf16>) -> tensor<1x64x160x160xf32> loc(#loc44)
    %45 = "tpu.Active"(%44) {mode = #tpu<active_mode SILU>} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf32> loc(#loc45)
    %46 = "tpu.Cast"(%45) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf16> loc(#loc46)
    %47 = "top.Weight"() : () -> tensor<128xf32> loc(#loc47)
    %48 = "top.Weight"() : () -> tensor<128x64x3x3xf16> loc(#loc48)
    %49 = "tpu.Conv2D"(%46, %48, %47) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<128x64x3x3xf16>, tensor<128xf32>) -> tensor<1x128x80x80xf16> loc(#loc49)
    %50 = "tpu.Cast"(%49) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc50)
    %51 = "tpu.Active"(%50) {mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc51)
    %52 = "tpu.Cast"(%51) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc52)
    %53 = "top.Weight"() : () -> tensor<64xf32> loc(#loc53)
    %54 = "top.Weight"() : () -> tensor<64x128x1x1xf16> loc(#loc54)
    %55 = "tpu.Conv2D"(%52, %54, %53) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<64x128x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc55)
    %56 = "tpu.Cast"(%55) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc56)
    %57 = "tpu.Active"(%56) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc57)
    %58 = "tpu.Cast"(%57) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc58)
    %59 = "top.Weight"() : () -> tensor<64xf32> loc(#loc59)
    %60 = "top.Weight"() : () -> tensor<64x64x1x1xf16> loc(#loc60)
    %61 = "tpu.Conv2D"(%58, %60, %59) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc61)
    %62 = "tpu.Cast"(%61) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc62)
    %63 = "tpu.Active"(%62) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc63)
    %64 = "tpu.Cast"(%63) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc64)
    %65 = "top.Weight"() : () -> tensor<64xf32> loc(#loc65)
    %66 = "top.Weight"() : () -> tensor<64x64x3x3xf16> loc(#loc66)
    %67 = "tpu.Conv2D"(%64, %66, %65) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x3x3xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc67)
    %68 = "tpu.Cast"(%67) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc68)
    %69 = "tpu.Active"(%68) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc69)
    %70 = "tpu.Cast"(%69) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc70)
    %71 = "tpu.Add"(%58, %70) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf16> loc(#loc71)
    %72 = "top.Weight"() : () -> tensor<64xf32> loc(#loc72)
    %73 = "top.Weight"() : () -> tensor<64x64x1x1xf16> loc(#loc73)
    %74 = "tpu.Conv2D"(%71, %73, %72) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc74)
    %75 = "tpu.Cast"(%74) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc75)
    %76 = "tpu.Active"(%75) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc76)
    %77 = "tpu.Cast"(%76) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc77)
    %78 = "top.Weight"() : () -> tensor<64xf32> loc(#loc78)
    %79 = "top.Weight"() : () -> tensor<64x64x3x3xf16> loc(#loc79)
    %80 = "tpu.Conv2D"(%77, %79, %78) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x3x3xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc80)
    %81 = "tpu.Cast"(%80) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc81)
    %82 = "tpu.Active"(%81) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc82)
    %83 = "tpu.Cast"(%82) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc83)
    %84 = "tpu.Add"(%71, %83) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf16> loc(#loc84)
    %85 = "top.Weight"() : () -> tensor<64xf32> loc(#loc85)
    %86 = "top.Weight"() : () -> tensor<64x128x1x1xf16> loc(#loc86)
    %87 = "tpu.Conv2D"(%52, %86, %85) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<64x128x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc87)
    %88 = "tpu.Cast"(%87) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc88)
    %89 = "tpu.Active"(%88) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc89)
    %90 = "tpu.Cast"(%89) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc90)
    %91 = "tpu.Concat"(%84, %90) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x128x80x80xf16> loc(#loc91)
    %92 = "top.Weight"() : () -> tensor<128xf32> loc(#loc92)
    %93 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc93)
    %94 = "tpu.Conv2D"(%91, %93, %92) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x80x80xf16> loc(#loc94)
    %95 = "tpu.Cast"(%94) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc95)
    %96 = "tpu.Active"(%95) {mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc96)
    %97 = "tpu.Cast"(%96) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc97)
    %98 = "top.Weight"() : () -> tensor<256xf32> loc(#loc98)
    %99 = "top.Weight"() : () -> tensor<256x128x3x3xf16> loc(#loc99)
    %100 = "tpu.Conv2D"(%97, %99, %98) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<256x128x3x3xf16>, tensor<256xf32>) -> tensor<1x256x40x40xf16> loc(#loc100)
    %101 = "tpu.Cast"(%100) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc101)
    %102 = "tpu.Active"(%101) {mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc102)
    %103 = "tpu.Cast"(%102) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc103)
    %104 = "top.Weight"() : () -> tensor<128xf32> loc(#loc104)
    %105 = "top.Weight"() : () -> tensor<128x256x1x1xf16> loc(#loc105)
    %106 = "tpu.Conv2D"(%103, %105, %104) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<128x256x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc106)
    %107 = "tpu.Cast"(%106) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc107)
    %108 = "tpu.Active"(%107) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc108)
    %109 = "tpu.Cast"(%108) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc109)
    %110 = "top.Weight"() : () -> tensor<128xf32> loc(#loc110)
    %111 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc111)
    %112 = "tpu.Conv2D"(%109, %111, %110) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc112)
    %113 = "tpu.Cast"(%112) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc113)
    %114 = "tpu.Active"(%113) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc114)
    %115 = "tpu.Cast"(%114) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc115)
    %116 = "top.Weight"() : () -> tensor<128xf32> loc(#loc116)
    %117 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc117)
    %118 = "tpu.Conv2D"(%115, %117, %116) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc118)
    %119 = "tpu.Cast"(%118) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc119)
    %120 = "tpu.Active"(%119) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc120)
    %121 = "tpu.Cast"(%120) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc121)
    %122 = "tpu.Add"(%109, %121) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc122)
    %123 = "top.Weight"() : () -> tensor<128xf32> loc(#loc123)
    %124 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc124)
    %125 = "tpu.Conv2D"(%122, %124, %123) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc125)
    %126 = "tpu.Cast"(%125) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc126)
    %127 = "tpu.Active"(%126) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc127)
    %128 = "tpu.Cast"(%127) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc128)
    %129 = "top.Weight"() : () -> tensor<128xf32> loc(#loc129)
    %130 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc130)
    %131 = "tpu.Conv2D"(%128, %130, %129) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc131)
    %132 = "tpu.Cast"(%131) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc132)
    %133 = "tpu.Active"(%132) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc133)
    %134 = "tpu.Cast"(%133) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc134)
    %135 = "tpu.Add"(%122, %134) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc135)
    %136 = "top.Weight"() : () -> tensor<128xf32> loc(#loc136)
    %137 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc137)
    %138 = "tpu.Conv2D"(%135, %137, %136) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc138)
    %139 = "tpu.Cast"(%138) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc139)
    %140 = "tpu.Active"(%139) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc140)
    %141 = "tpu.Cast"(%140) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc141)
    %142 = "top.Weight"() : () -> tensor<128xf32> loc(#loc142)
    %143 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc143)
    %144 = "tpu.Conv2D"(%141, %143, %142) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc144)
    %145 = "tpu.Cast"(%144) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc145)
    %146 = "tpu.Active"(%145) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc146)
    %147 = "tpu.Cast"(%146) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc147)
    %148 = "tpu.Add"(%135, %147) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc148)
    %149 = "top.Weight"() : () -> tensor<128xf32> loc(#loc149)
    %150 = "top.Weight"() : () -> tensor<128x256x1x1xf16> loc(#loc150)
    %151 = "tpu.Conv2D"(%103, %150, %149) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<128x256x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc151)
    %152 = "tpu.Cast"(%151) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc152)
    %153 = "tpu.Active"(%152) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc153)
    %154 = "tpu.Cast"(%153) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc154)
    %155 = "tpu.Concat"(%148, %154) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc155)
    %156 = "top.Weight"() : () -> tensor<256xf32> loc(#loc156)
    %157 = "top.Weight"() : () -> tensor<256x256x1x1xf16> loc(#loc157)
    %158 = "tpu.Conv2D"(%155, %157, %156) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<256x256x1x1xf16>, tensor<256xf32>) -> tensor<1x256x40x40xf16> loc(#loc158)
    %159 = "tpu.Cast"(%158) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc159)
    %160 = "tpu.Active"(%159) {mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc160)
    %161 = "tpu.Cast"(%160) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc161)
    %162 = "top.Weight"() : () -> tensor<512xf32> loc(#loc162)
    %163 = "top.Weight"() : () -> tensor<512x256x3x3xf16> loc(#loc163)
    %164 = "tpu.Conv2D"(%161, %163, %162) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<512x256x3x3xf16>, tensor<512xf32>) -> tensor<1x512x20x20xf16> loc(#loc164)
    %165 = "tpu.Cast"(%164) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc165)
    %166 = "tpu.Active"(%165) {mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc166)
    %167 = "tpu.Cast"(%166) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc167)
    %168 = "top.Weight"() : () -> tensor<256xf32> loc(#loc168)
    %169 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc169)
    %170 = "tpu.Conv2D"(%167, %169, %168) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc170)
    %171 = "tpu.Cast"(%170) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc171)
    %172 = "tpu.Active"(%171) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc172)
    %173 = "tpu.Cast"(%172) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc173)
    %174 = "top.Weight"() : () -> tensor<256xf32> loc(#loc174)
    %175 = "top.Weight"() : () -> tensor<256x256x1x1xf16> loc(#loc175)
    %176 = "tpu.Conv2D"(%173, %175, %174) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<256x256x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc176)
    %177 = "tpu.Cast"(%176) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc177)
    %178 = "tpu.Active"(%177) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc178)
    %179 = "tpu.Cast"(%178) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc179)
    %180 = "top.Weight"() : () -> tensor<256xf32> loc(#loc180)
    %181 = "top.Weight"() : () -> tensor<256x256x3x3xf16> loc(#loc181)
    %182 = "tpu.Conv2D"(%179, %181, %180) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<256x256x3x3xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc182)
    %183 = "tpu.Cast"(%182) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc183)
    %184 = "tpu.Active"(%183) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc184)
    %185 = "tpu.Cast"(%184) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc185)
    %186 = "tpu.Add"(%173, %185) {do_relu = false, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc186)
    %187 = "top.Weight"() : () -> tensor<256xf32> loc(#loc187)
    %188 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc188)
    %189 = "tpu.Conv2D"(%167, %188, %187) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc189)
    %190 = "tpu.Cast"(%189) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc190)
    %191 = "tpu.Active"(%190) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc191)
    %192 = "tpu.Cast"(%191) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc192)
    %193 = "tpu.Concat"(%186, %192) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc193)
    %194 = "top.Weight"() : () -> tensor<512xf32> loc(#loc194)
    %195 = "top.Weight"() : () -> tensor<512x512x1x1xf16> loc(#loc195)
    %196 = "tpu.Conv2D"(%193, %195, %194) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<512x512x1x1xf16>, tensor<512xf32>) -> tensor<1x512x20x20xf16> loc(#loc196)
    %197 = "tpu.Cast"(%196) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc197)
    %198 = "tpu.Active"(%197) {mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc198)
    %199 = "tpu.Cast"(%198) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc199)
    %200 = "top.Weight"() : () -> tensor<256xf32> loc(#loc200)
    %201 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc201)
    %202 = "tpu.Conv2D"(%199, %201, %200) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc202)
    %203 = "tpu.Cast"(%202) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc203)
    %204 = "tpu.Active"(%203) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc204)
    %205 = "tpu.Cast"(%204) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc205)
    %206 = "tpu.Pool2D"(%205) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc206)
    %207 = "tpu.Pool2D"(%206) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc207)
    %208 = "tpu.Pool2D"(%207) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc208)
    %209 = "tpu.Concat"(%205, %206, %207, %208) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x1024x20x20xf16> loc(#loc209)
    %210 = "top.Weight"() : () -> tensor<512xf32> loc(#loc210)
    %211 = "top.Weight"() : () -> tensor<512x1024x1x1xf16> loc(#loc211)
    %212 = "tpu.Conv2D"(%209, %211, %210) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x1024x20x20xf16>, tensor<512x1024x1x1xf16>, tensor<512xf32>) -> tensor<1x512x20x20xf16> loc(#loc212)
    %213 = "tpu.Cast"(%212) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc213)
    %214 = "tpu.Active"(%213) {mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc214)
    %215 = "tpu.Cast"(%214) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc215)
    %216 = "top.Weight"() : () -> tensor<256xf32> loc(#loc216)
    %217 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc217)
    %218 = "tpu.Conv2D"(%215, %217, %216) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc218)
    %219 = "tpu.Cast"(%218) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc219)
    %220 = "tpu.Active"(%219) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc220)
    %221 = "tpu.Cast"(%220) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc221)
    %222 = "tpu.Upsample"(%221) {do_relu = false, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x256x20x20xf16>) -> tensor<1x256x40x40xf16> loc(#loc222)
    %223 = "tpu.Concat"(%222, %161) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x40x40xf16>, tensor<1x256x40x40xf16>) -> tensor<1x512x40x40xf16> loc(#loc223)
    %224 = "top.Weight"() : () -> tensor<128xf32> loc(#loc224)
    %225 = "top.Weight"() : () -> tensor<128x512x1x1xf16> loc(#loc225)
    %226 = "tpu.Conv2D"(%223, %225, %224) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40xf16>, tensor<128x512x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc226)
    %227 = "tpu.Cast"(%226) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc227)
    %228 = "tpu.Active"(%227) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc228)
    %229 = "tpu.Cast"(%228) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc229)
    %230 = "top.Weight"() : () -> tensor<128xf32> loc(#loc230)
    %231 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc231)
    %232 = "tpu.Conv2D"(%229, %231, %230) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc232)
    %233 = "tpu.Cast"(%232) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc233)
    %234 = "tpu.Active"(%233) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc234)
    %235 = "tpu.Cast"(%234) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc235)
    %236 = "top.Weight"() : () -> tensor<128xf32> loc(#loc236)
    %237 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc237)
    %238 = "tpu.Conv2D"(%235, %237, %236) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc238)
    %239 = "tpu.Cast"(%238) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc239)
    %240 = "tpu.Active"(%239) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc240)
    %241 = "tpu.Cast"(%240) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc241)
    %242 = "top.Weight"() : () -> tensor<128xf32> loc(#loc242)
    %243 = "top.Weight"() : () -> tensor<128x512x1x1xf16> loc(#loc243)
    %244 = "tpu.Conv2D"(%223, %243, %242) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40xf16>, tensor<128x512x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc244)
    %245 = "tpu.Cast"(%244) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc245)
    %246 = "tpu.Active"(%245) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc246)
    %247 = "tpu.Cast"(%246) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc247)
    %248 = "tpu.Concat"(%241, %247) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc248)
    %249 = "top.Weight"() : () -> tensor<256xf32> loc(#loc249)
    %250 = "top.Weight"() : () -> tensor<256x256x1x1xf16> loc(#loc250)
    %251 = "tpu.Conv2D"(%248, %250, %249) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<256x256x1x1xf16>, tensor<256xf32>) -> tensor<1x256x40x40xf16> loc(#loc251)
    %252 = "tpu.Cast"(%251) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc252)
    %253 = "tpu.Active"(%252) {mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc253)
    %254 = "tpu.Cast"(%253) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc254)
    %255 = "top.Weight"() : () -> tensor<128xf32> loc(#loc255)
    %256 = "top.Weight"() : () -> tensor<128x256x1x1xf16> loc(#loc256)
    %257 = "tpu.Conv2D"(%254, %256, %255) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<128x256x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc257)
    %258 = "tpu.Cast"(%257) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc258)
    %259 = "tpu.Active"(%258) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc259)
    %260 = "tpu.Cast"(%259) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc260)
    %261 = "tpu.Upsample"(%260) {do_relu = false, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x128x40x40xf16>) -> tensor<1x128x80x80xf16> loc(#loc261)
    %262 = "tpu.Concat"(%261, %97) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x80x80xf16>, tensor<1x128x80x80xf16>) -> tensor<1x256x80x80xf16> loc(#loc262)
    %263 = "top.Weight"() : () -> tensor<64xf32> loc(#loc263)
    %264 = "top.Weight"() : () -> tensor<64x256x1x1xf16> loc(#loc264)
    %265 = "tpu.Conv2D"(%262, %264, %263) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80xf16>, tensor<64x256x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc265)
    %266 = "tpu.Cast"(%265) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc266)
    %267 = "tpu.Active"(%266) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc267)
    %268 = "tpu.Cast"(%267) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc268)
    %269 = "top.Weight"() : () -> tensor<64xf32> loc(#loc269)
    %270 = "top.Weight"() : () -> tensor<64x64x1x1xf16> loc(#loc270)
    %271 = "tpu.Conv2D"(%268, %270, %269) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc271)
    %272 = "tpu.Cast"(%271) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc272)
    %273 = "tpu.Active"(%272) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc273)
    %274 = "tpu.Cast"(%273) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc274)
    %275 = "top.Weight"() : () -> tensor<64xf32> loc(#loc275)
    %276 = "top.Weight"() : () -> tensor<64x64x3x3xf16> loc(#loc276)
    %277 = "tpu.Conv2D"(%274, %276, %275) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<64x64x3x3xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc277)
    %278 = "tpu.Cast"(%277) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc278)
    %279 = "tpu.Active"(%278) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc279)
    %280 = "tpu.Cast"(%279) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc280)
    %281 = "top.Weight"() : () -> tensor<64xf32> loc(#loc281)
    %282 = "top.Weight"() : () -> tensor<64x256x1x1xf16> loc(#loc282)
    %283 = "tpu.Conv2D"(%262, %282, %281) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80xf16>, tensor<64x256x1x1xf16>, tensor<64xf32>) -> tensor<1x64x80x80xf16> loc(#loc283)
    %284 = "tpu.Cast"(%283) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc284)
    %285 = "tpu.Active"(%284) {mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc285)
    %286 = "tpu.Cast"(%285) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc286)
    %287 = "tpu.Concat"(%280, %286) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x128x80x80xf16> loc(#loc287)
    %288 = "top.Weight"() : () -> tensor<128xf32> loc(#loc288)
    %289 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc289)
    %290 = "tpu.Conv2D"(%287, %289, %288) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x80x80xf16> loc(#loc290)
    %291 = "tpu.Cast"(%290) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc291)
    %292 = "tpu.Active"(%291) {mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc292)
    %293 = "tpu.Cast"(%292) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc293)
    %294 = "top.Weight"() : () -> tensor<128xf32> loc(#loc294)
    %295 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc295)
    %296 = "tpu.Conv2D"(%293, %295, %294) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc296)
    %297 = "tpu.Cast"(%296) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc297)
    %298 = "tpu.Active"(%297) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc298)
    %299 = "tpu.Cast"(%298) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc299)
    %300 = "tpu.Concat"(%299, %260) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc300)
    %301 = "top.Weight"() : () -> tensor<128xf32> loc(#loc301)
    %302 = "top.Weight"() : () -> tensor<128x256x1x1xf16> loc(#loc302)
    %303 = "tpu.Conv2D"(%300, %302, %301) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<128x256x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc303)
    %304 = "tpu.Cast"(%303) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc304)
    %305 = "tpu.Active"(%304) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc305)
    %306 = "tpu.Cast"(%305) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc306)
    %307 = "top.Weight"() : () -> tensor<128xf32> loc(#loc307)
    %308 = "top.Weight"() : () -> tensor<128x128x1x1xf16> loc(#loc308)
    %309 = "tpu.Conv2D"(%306, %308, %307) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc309)
    %310 = "tpu.Cast"(%309) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc310)
    %311 = "tpu.Active"(%310) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc311)
    %312 = "tpu.Cast"(%311) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc312)
    %313 = "top.Weight"() : () -> tensor<128xf32> loc(#loc313)
    %314 = "top.Weight"() : () -> tensor<128x128x3x3xf16> loc(#loc314)
    %315 = "tpu.Conv2D"(%312, %314, %313) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<128x128x3x3xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc315)
    %316 = "tpu.Cast"(%315) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc316)
    %317 = "tpu.Active"(%316) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc317)
    %318 = "tpu.Cast"(%317) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc318)
    %319 = "top.Weight"() : () -> tensor<128xf32> loc(#loc319)
    %320 = "top.Weight"() : () -> tensor<128x256x1x1xf16> loc(#loc320)
    %321 = "tpu.Conv2D"(%300, %320, %319) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<128x256x1x1xf16>, tensor<128xf32>) -> tensor<1x128x40x40xf16> loc(#loc321)
    %322 = "tpu.Cast"(%321) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc322)
    %323 = "tpu.Active"(%322) {mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc323)
    %324 = "tpu.Cast"(%323) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc324)
    %325 = "tpu.Concat"(%318, %324) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc325)
    %326 = "top.Weight"() : () -> tensor<256xf32> loc(#loc326)
    %327 = "top.Weight"() : () -> tensor<256x256x1x1xf16> loc(#loc327)
    %328 = "tpu.Conv2D"(%325, %327, %326) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<256x256x1x1xf16>, tensor<256xf32>) -> tensor<1x256x40x40xf16> loc(#loc328)
    %329 = "tpu.Cast"(%328) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc329)
    %330 = "tpu.Active"(%329) {mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc330)
    %331 = "tpu.Cast"(%330) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc331)
    %332 = "top.Weight"() : () -> tensor<256xf32> loc(#loc332)
    %333 = "top.Weight"() : () -> tensor<256x256x3x3xf16> loc(#loc333)
    %334 = "tpu.Conv2D"(%331, %333, %332) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<256x256x3x3xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc334)
    %335 = "tpu.Cast"(%334) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc335)
    %336 = "tpu.Active"(%335) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc336)
    %337 = "tpu.Cast"(%336) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc337)
    %338 = "tpu.Concat"(%337, %221) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc338)
    %339 = "top.Weight"() : () -> tensor<256xf32> loc(#loc339)
    %340 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc340)
    %341 = "tpu.Conv2D"(%338, %340, %339) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc341)
    %342 = "tpu.Cast"(%341) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc342)
    %343 = "tpu.Active"(%342) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc343)
    %344 = "tpu.Cast"(%343) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc344)
    %345 = "top.Weight"() : () -> tensor<256xf32> loc(#loc345)
    %346 = "top.Weight"() : () -> tensor<256x256x1x1xf16> loc(#loc346)
    %347 = "tpu.Conv2D"(%344, %346, %345) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<256x256x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc347)
    %348 = "tpu.Cast"(%347) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc348)
    %349 = "tpu.Active"(%348) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc349)
    %350 = "tpu.Cast"(%349) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc350)
    %351 = "top.Weight"() : () -> tensor<256xf32> loc(#loc351)
    %352 = "top.Weight"() : () -> tensor<256x256x3x3xf16> loc(#loc352)
    %353 = "tpu.Conv2D"(%350, %352, %351) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<256x256x3x3xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc353)
    %354 = "tpu.Cast"(%353) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc354)
    %355 = "tpu.Active"(%354) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc355)
    %356 = "tpu.Cast"(%355) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc356)
    %357 = "top.Weight"() : () -> tensor<256xf32> loc(#loc357)
    %358 = "top.Weight"() : () -> tensor<256x512x1x1xf16> loc(#loc358)
    %359 = "tpu.Conv2D"(%338, %358, %357) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<256x512x1x1xf16>, tensor<256xf32>) -> tensor<1x256x20x20xf16> loc(#loc359)
    %360 = "tpu.Cast"(%359) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc360)
    %361 = "tpu.Active"(%360) {mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc361)
    %362 = "tpu.Cast"(%361) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc362)
    %363 = "tpu.Concat"(%356, %362) {axis = 1 : si32, do_relu = false, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc363)
    %364 = "top.Weight"() : () -> tensor<512xf32> loc(#loc364)
    %365 = "top.Weight"() : () -> tensor<512x512x1x1xf16> loc(#loc365)
    %366 = "tpu.Conv2D"(%363, %365, %364) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<512x512x1x1xf16>, tensor<512xf32>) -> tensor<1x512x20x20xf16> loc(#loc366)
    %367 = "tpu.Cast"(%366) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc367)
    %368 = "tpu.Active"(%367) {mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc368)
    %369 = "tpu.Cast"(%368) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc369)
    %370 = "top.Weight"() : () -> tensor<255xf32> loc(#loc370)
    %371 = "top.Weight"() : () -> tensor<255x128x1x1xf16> loc(#loc371)
    %372 = "tpu.Conv2D"(%293, %371, %370) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<255x128x1x1xf16>, tensor<255xf32>) -> tensor<1x255x80x80xf16> loc(#loc372)
    %373 = "tpu.Reshape"(%372) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 80, 80]} : (tensor<1x255x80x80xf16>) -> tensor<1x3x85x80x80xf16> loc(#loc373)
    %374 = "tpu.Permute"(%373, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x80x80xf16>, none) -> tensor<1x3x80x80x85xf16> loc(#loc374)
    %375 = "tpu.Cast"(%374) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x80x80x85xf16>) -> tensor<1x3x80x80x85xf32> loc(#loc375)
    %376 = "top.Weight"() : () -> tensor<255xf32> loc(#loc376)
    %377 = "top.Weight"() : () -> tensor<255x256x1x1xf16> loc(#loc377)
    %378 = "tpu.Conv2D"(%331, %377, %376) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<255x256x1x1xf16>, tensor<255xf32>) -> tensor<1x255x40x40xf16> loc(#loc378)
    %379 = "tpu.Reshape"(%378) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 40, 40]} : (tensor<1x255x40x40xf16>) -> tensor<1x3x85x40x40xf16> loc(#loc379)
    %380 = "tpu.Permute"(%379, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x40x40xf16>, none) -> tensor<1x3x40x40x85xf16> loc(#loc380)
    %381 = "tpu.Cast"(%380) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x40x40x85xf16>) -> tensor<1x3x40x40x85xf32> loc(#loc381)
    %382 = "top.Weight"() : () -> tensor<255xf32> loc(#loc382)
    %383 = "top.Weight"() : () -> tensor<255x512x1x1xf16> loc(#loc383)
    %384 = "tpu.Conv2D"(%369, %383, %382) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<255x512x1x1xf16>, tensor<255xf32>) -> tensor<1x255x20x20xf16> loc(#loc384)
    %385 = "tpu.Reshape"(%384) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 20, 20]} : (tensor<1x255x20x20xf16>) -> tensor<1x3x85x20x20xf16> loc(#loc385)
    %386 = "tpu.Permute"(%385, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x20x20xf16>, none) -> tensor<1x3x20x20x85xf16> loc(#loc386)
    %387 = "tpu.Cast"(%386) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x20x20x85xf16>) -> tensor<1x3x20x20x85xf32> loc(#loc387)
    return %375, %381, %387 : tensor<1x3x80x80x85xf32>, tensor<1x3x40x40x85xf32>, tensor<1x3x20x20x85xf32> loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("images")
#loc2 = loc("images122_Conv_f16")
#loc3 = loc("model.0.conv.bias")
#loc4 = loc("122_Convmodel.0.conv.weight_f16")
#loc5 = loc("122_Conv")
#loc6 = loc("122_Conv124_Mul_f32")
#loc7 = loc("124_Mul")
#loc8 = loc("124_Mul125_Conv_f16")
#loc9 = loc("model.1.conv.bias")
#loc10 = loc("125_Convmodel.1.conv.weight_f16")
#loc11 = loc("125_Conv")
#loc12 = loc("125_Conv127_Mul_f32")
#loc13 = loc("127_Mul")
#loc14 = loc("127_Mul128_Conv_f16")
#loc15 = loc("model.2.cv1.conv.bias")
#loc16 = loc("128_Convmodel.2.cv1.conv.weight_f16")
#loc17 = loc("128_Conv")
#loc18 = loc("128_Conv130_Mul_f32")
#loc19 = loc("130_Mul")
#loc20 = loc("130_Mul131_Conv_f16")
#loc21 = loc("model.2.m.0.cv1.conv.bias")
#loc22 = loc("131_Convmodel.2.m.0.cv1.conv.weight_f16")
#loc23 = loc("131_Conv")
#loc24 = loc("131_Conv133_Mul_f32")
#loc25 = loc("133_Mul")
#loc26 = loc("133_Mul134_Conv_f16")
#loc27 = loc("model.2.m.0.cv2.conv.bias")
#loc28 = loc("134_Convmodel.2.m.0.cv2.conv.weight_f16")
#loc29 = loc("134_Conv")
#loc30 = loc("134_Conv136_Mul_f32")
#loc31 = loc("136_Mul")
#loc32 = loc("136_Mul137_Add_f16")
#loc33 = loc("137_Add")
#loc34 = loc("model.2.cv2.conv.bias")
#loc35 = loc("138_Convmodel.2.cv2.conv.weight_f16")
#loc36 = loc("138_Conv")
#loc37 = loc("138_Conv140_Mul_f32")
#loc38 = loc("140_Mul")
#loc39 = loc("140_Mul141_Concat_f16")
#loc40 = loc("141_Concat")
#loc41 = loc("model.2.cv3.conv.bias")
#loc42 = loc("142_Convmodel.2.cv3.conv.weight_f16")
#loc43 = loc("142_Conv")
#loc44 = loc("142_Conv144_Mul_f32")
#loc45 = loc("144_Mul")
#loc46 = loc("144_Mul145_Conv_f16")
#loc47 = loc("model.3.conv.bias")
#loc48 = loc("145_Convmodel.3.conv.weight_f16")
#loc49 = loc("145_Conv")
#loc50 = loc("145_Conv147_Mul_f32")
#loc51 = loc("147_Mul")
#loc52 = loc("147_Mul148_Conv_f16")
#loc53 = loc("model.4.cv1.conv.bias")
#loc54 = loc("148_Convmodel.4.cv1.conv.weight_f16")
#loc55 = loc("148_Conv")
#loc56 = loc("148_Conv150_Mul_f32")
#loc57 = loc("150_Mul")
#loc58 = loc("150_Mul151_Conv_f16")
#loc59 = loc("model.4.m.0.cv1.conv.bias")
#loc60 = loc("151_Convmodel.4.m.0.cv1.conv.weight_f16")
#loc61 = loc("151_Conv")
#loc62 = loc("151_Conv153_Mul_f32")
#loc63 = loc("153_Mul")
#loc64 = loc("153_Mul154_Conv_f16")
#loc65 = loc("model.4.m.0.cv2.conv.bias")
#loc66 = loc("154_Convmodel.4.m.0.cv2.conv.weight_f16")
#loc67 = loc("154_Conv")
#loc68 = loc("154_Conv156_Mul_f32")
#loc69 = loc("156_Mul")
#loc70 = loc("156_Mul157_Add_f16")
#loc71 = loc("157_Add")
#loc72 = loc("model.4.m.1.cv1.conv.bias")
#loc73 = loc("158_Convmodel.4.m.1.cv1.conv.weight_f16")
#loc74 = loc("158_Conv")
#loc75 = loc("158_Conv160_Mul_f32")
#loc76 = loc("160_Mul")
#loc77 = loc("160_Mul161_Conv_f16")
#loc78 = loc("model.4.m.1.cv2.conv.bias")
#loc79 = loc("161_Convmodel.4.m.1.cv2.conv.weight_f16")
#loc80 = loc("161_Conv")
#loc81 = loc("161_Conv163_Mul_f32")
#loc82 = loc("163_Mul")
#loc83 = loc("163_Mul164_Add_f16")
#loc84 = loc("164_Add")
#loc85 = loc("model.4.cv2.conv.bias")
#loc86 = loc("165_Convmodel.4.cv2.conv.weight_f16")
#loc87 = loc("165_Conv")
#loc88 = loc("165_Conv167_Mul_f32")
#loc89 = loc("167_Mul")
#loc90 = loc("167_Mul168_Concat_f16")
#loc91 = loc("168_Concat")
#loc92 = loc("model.4.cv3.conv.bias")
#loc93 = loc("169_Convmodel.4.cv3.conv.weight_f16")
#loc94 = loc("169_Conv")
#loc95 = loc("169_Conv171_Mul_f32")
#loc96 = loc("171_Mul")
#loc97 = loc("171_Mul172_Conv_f16")
#loc98 = loc("model.5.conv.bias")
#loc99 = loc("172_Convmodel.5.conv.weight_f16")
#loc100 = loc("172_Conv")
#loc101 = loc("172_Conv174_Mul_f32")
#loc102 = loc("174_Mul")
#loc103 = loc("174_Mul175_Conv_f16")
#loc104 = loc("model.6.cv1.conv.bias")
#loc105 = loc("175_Convmodel.6.cv1.conv.weight_f16")
#loc106 = loc("175_Conv")
#loc107 = loc("175_Conv177_Mul_f32")
#loc108 = loc("177_Mul")
#loc109 = loc("177_Mul178_Conv_f16")
#loc110 = loc("model.6.m.0.cv1.conv.bias")
#loc111 = loc("178_Convmodel.6.m.0.cv1.conv.weight_f16")
#loc112 = loc("178_Conv")
#loc113 = loc("178_Conv180_Mul_f32")
#loc114 = loc("180_Mul")
#loc115 = loc("180_Mul181_Conv_f16")
#loc116 = loc("model.6.m.0.cv2.conv.bias")
#loc117 = loc("181_Convmodel.6.m.0.cv2.conv.weight_f16")
#loc118 = loc("181_Conv")
#loc119 = loc("181_Conv183_Mul_f32")
#loc120 = loc("183_Mul")
#loc121 = loc("183_Mul184_Add_f16")
#loc122 = loc("184_Add")
#loc123 = loc("model.6.m.1.cv1.conv.bias")
#loc124 = loc("185_Convmodel.6.m.1.cv1.conv.weight_f16")
#loc125 = loc("185_Conv")
#loc126 = loc("185_Conv187_Mul_f32")
#loc127 = loc("187_Mul")
#loc128 = loc("187_Mul188_Conv_f16")
#loc129 = loc("model.6.m.1.cv2.conv.bias")
#loc130 = loc("188_Convmodel.6.m.1.cv2.conv.weight_f16")
#loc131 = loc("188_Conv")
#loc132 = loc("188_Conv190_Mul_f32")
#loc133 = loc("190_Mul")
#loc134 = loc("190_Mul191_Add_f16")
#loc135 = loc("191_Add")
#loc136 = loc("model.6.m.2.cv1.conv.bias")
#loc137 = loc("192_Convmodel.6.m.2.cv1.conv.weight_f16")
#loc138 = loc("192_Conv")
#loc139 = loc("192_Conv194_Mul_f32")
#loc140 = loc("194_Mul")
#loc141 = loc("194_Mul195_Conv_f16")
#loc142 = loc("model.6.m.2.cv2.conv.bias")
#loc143 = loc("195_Convmodel.6.m.2.cv2.conv.weight_f16")
#loc144 = loc("195_Conv")
#loc145 = loc("195_Conv197_Mul_f32")
#loc146 = loc("197_Mul")
#loc147 = loc("197_Mul198_Add_f16")
#loc148 = loc("198_Add")
#loc149 = loc("model.6.cv2.conv.bias")
#loc150 = loc("199_Convmodel.6.cv2.conv.weight_f16")
#loc151 = loc("199_Conv")
#loc152 = loc("199_Conv201_Mul_f32")
#loc153 = loc("201_Mul")
#loc154 = loc("201_Mul202_Concat_f16")
#loc155 = loc("202_Concat")
#loc156 = loc("model.6.cv3.conv.bias")
#loc157 = loc("203_Convmodel.6.cv3.conv.weight_f16")
#loc158 = loc("203_Conv")
#loc159 = loc("203_Conv205_Mul_f32")
#loc160 = loc("205_Mul")
#loc161 = loc("205_Mul206_Conv_f16")
#loc162 = loc("model.7.conv.bias")
#loc163 = loc("206_Convmodel.7.conv.weight_f16")
#loc164 = loc("206_Conv")
#loc165 = loc("206_Conv208_Mul_f32")
#loc166 = loc("208_Mul")
#loc167 = loc("208_Mul209_Conv_f16")
#loc168 = loc("model.8.cv1.conv.bias")
#loc169 = loc("209_Convmodel.8.cv1.conv.weight_f16")
#loc170 = loc("209_Conv")
#loc171 = loc("209_Conv211_Mul_f32")
#loc172 = loc("211_Mul")
#loc173 = loc("211_Mul212_Conv_f16")
#loc174 = loc("model.8.m.0.cv1.conv.bias")
#loc175 = loc("212_Convmodel.8.m.0.cv1.conv.weight_f16")
#loc176 = loc("212_Conv")
#loc177 = loc("212_Conv214_Mul_f32")
#loc178 = loc("214_Mul")
#loc179 = loc("214_Mul215_Conv_f16")
#loc180 = loc("model.8.m.0.cv2.conv.bias")
#loc181 = loc("215_Convmodel.8.m.0.cv2.conv.weight_f16")
#loc182 = loc("215_Conv")
#loc183 = loc("215_Conv217_Mul_f32")
#loc184 = loc("217_Mul")
#loc185 = loc("217_Mul218_Add_f16")
#loc186 = loc("218_Add")
#loc187 = loc("model.8.cv2.conv.bias")
#loc188 = loc("219_Convmodel.8.cv2.conv.weight_f16")
#loc189 = loc("219_Conv")
#loc190 = loc("219_Conv221_Mul_f32")
#loc191 = loc("221_Mul")
#loc192 = loc("221_Mul222_Concat_f16")
#loc193 = loc("222_Concat")
#loc194 = loc("model.8.cv3.conv.bias")
#loc195 = loc("223_Convmodel.8.cv3.conv.weight_f16")
#loc196 = loc("223_Conv")
#loc197 = loc("223_Conv225_Mul_f32")
#loc198 = loc("225_Mul")
#loc199 = loc("225_Mul226_Conv_f16")
#loc200 = loc("model.9.cv1.conv.bias")
#loc201 = loc("226_Convmodel.9.cv1.conv.weight_f16")
#loc202 = loc("226_Conv")
#loc203 = loc("226_Conv228_Mul_f32")
#loc204 = loc("228_Mul")
#loc205 = loc("228_Mul229_MaxPool_f16")
#loc206 = loc("229_MaxPool")
#loc207 = loc("230_MaxPool")
#loc208 = loc("231_MaxPool")
#loc209 = loc("232_Concat")
#loc210 = loc("model.9.cv2.conv.bias")
#loc211 = loc("233_Convmodel.9.cv2.conv.weight_f16")
#loc212 = loc("233_Conv")
#loc213 = loc("233_Conv235_Mul_f32")
#loc214 = loc("235_Mul")
#loc215 = loc("235_Mul236_Conv_f16")
#loc216 = loc("model.10.conv.bias")
#loc217 = loc("236_Convmodel.10.conv.weight_f16")
#loc218 = loc("236_Conv")
#loc219 = loc("236_Conv238_Mul_f32")
#loc220 = loc("238_Mul")
#loc221 = loc("238_Mul243_Resize_f16")
#loc222 = loc("243_Resize")
#loc223 = loc("244_Concat")
#loc224 = loc("model.13.cv1.conv.bias")
#loc225 = loc("245_Convmodel.13.cv1.conv.weight_f16")
#loc226 = loc("245_Conv")
#loc227 = loc("245_Conv247_Mul_f32")
#loc228 = loc("247_Mul")
#loc229 = loc("247_Mul248_Conv_f16")
#loc230 = loc("model.13.m.0.cv1.conv.bias")
#loc231 = loc("248_Convmodel.13.m.0.cv1.conv.weight_f16")
#loc232 = loc("248_Conv")
#loc233 = loc("248_Conv250_Mul_f32")
#loc234 = loc("250_Mul")
#loc235 = loc("250_Mul251_Conv_f16")
#loc236 = loc("model.13.m.0.cv2.conv.bias")
#loc237 = loc("251_Convmodel.13.m.0.cv2.conv.weight_f16")
#loc238 = loc("251_Conv")
#loc239 = loc("251_Conv253_Mul_f32")
#loc240 = loc("253_Mul")
#loc241 = loc("253_Mul257_Concat_f16")
#loc242 = loc("model.13.cv2.conv.bias")
#loc243 = loc("254_Convmodel.13.cv2.conv.weight_f16")
#loc244 = loc("254_Conv")
#loc245 = loc("254_Conv256_Mul_f32")
#loc246 = loc("256_Mul")
#loc247 = loc("256_Mul257_Concat_f16")
#loc248 = loc("257_Concat")
#loc249 = loc("model.13.cv3.conv.bias")
#loc250 = loc("258_Convmodel.13.cv3.conv.weight_f16")
#loc251 = loc("258_Conv")
#loc252 = loc("258_Conv260_Mul_f32")
#loc253 = loc("260_Mul")
#loc254 = loc("260_Mul261_Conv_f16")
#loc255 = loc("model.14.conv.bias")
#loc256 = loc("261_Convmodel.14.conv.weight_f16")
#loc257 = loc("261_Conv")
#loc258 = loc("261_Conv263_Mul_f32")
#loc259 = loc("263_Mul")
#loc260 = loc("263_Mul268_Resize_f16")
#loc261 = loc("268_Resize")
#loc262 = loc("269_Concat")
#loc263 = loc("model.17.cv1.conv.bias")
#loc264 = loc("270_Convmodel.17.cv1.conv.weight_f16")
#loc265 = loc("270_Conv")
#loc266 = loc("270_Conv272_Mul_f32")
#loc267 = loc("272_Mul")
#loc268 = loc("272_Mul273_Conv_f16")
#loc269 = loc("model.17.m.0.cv1.conv.bias")
#loc270 = loc("273_Convmodel.17.m.0.cv1.conv.weight_f16")
#loc271 = loc("273_Conv")
#loc272 = loc("273_Conv275_Mul_f32")
#loc273 = loc("275_Mul")
#loc274 = loc("275_Mul276_Conv_f16")
#loc275 = loc("model.17.m.0.cv2.conv.bias")
#loc276 = loc("276_Convmodel.17.m.0.cv2.conv.weight_f16")
#loc277 = loc("276_Conv")
#loc278 = loc("276_Conv278_Mul_f32")
#loc279 = loc("278_Mul")
#loc280 = loc("278_Mul282_Concat_f16")
#loc281 = loc("model.17.cv2.conv.bias")
#loc282 = loc("279_Convmodel.17.cv2.conv.weight_f16")
#loc283 = loc("279_Conv")
#loc284 = loc("279_Conv281_Mul_f32")
#loc285 = loc("281_Mul")
#loc286 = loc("281_Mul282_Concat_f16")
#loc287 = loc("282_Concat")
#loc288 = loc("model.17.cv3.conv.bias")
#loc289 = loc("283_Convmodel.17.cv3.conv.weight_f16")
#loc290 = loc("283_Conv")
#loc291 = loc("283_Conv285_Mul_f32")
#loc292 = loc("285_Mul")
#loc293 = loc("285_Mul286_Conv_f16")
#loc294 = loc("model.18.conv.bias")
#loc295 = loc("286_Convmodel.18.conv.weight_f16")
#loc296 = loc("286_Conv")
#loc297 = loc("286_Conv288_Mul_f32")
#loc298 = loc("288_Mul")
#loc299 = loc("288_Mul289_Concat_f16")
#loc300 = loc("289_Concat")
#loc301 = loc("model.20.cv1.conv.bias")
#loc302 = loc("290_Convmodel.20.cv1.conv.weight_f16")
#loc303 = loc("290_Conv")
#loc304 = loc("290_Conv292_Mul_f32")
#loc305 = loc("292_Mul")
#loc306 = loc("292_Mul293_Conv_f16")
#loc307 = loc("model.20.m.0.cv1.conv.bias")
#loc308 = loc("293_Convmodel.20.m.0.cv1.conv.weight_f16")
#loc309 = loc("293_Conv")
#loc310 = loc("293_Conv295_Mul_f32")
#loc311 = loc("295_Mul")
#loc312 = loc("295_Mul296_Conv_f16")
#loc313 = loc("model.20.m.0.cv2.conv.bias")
#loc314 = loc("296_Convmodel.20.m.0.cv2.conv.weight_f16")
#loc315 = loc("296_Conv")
#loc316 = loc("296_Conv298_Mul_f32")
#loc317 = loc("298_Mul")
#loc318 = loc("298_Mul302_Concat_f16")
#loc319 = loc("model.20.cv2.conv.bias")
#loc320 = loc("299_Convmodel.20.cv2.conv.weight_f16")
#loc321 = loc("299_Conv")
#loc322 = loc("299_Conv301_Mul_f32")
#loc323 = loc("301_Mul")
#loc324 = loc("301_Mul302_Concat_f16")
#loc325 = loc("302_Concat")
#loc326 = loc("model.20.cv3.conv.bias")
#loc327 = loc("303_Convmodel.20.cv3.conv.weight_f16")
#loc328 = loc("303_Conv")
#loc329 = loc("303_Conv305_Mul_f32")
#loc330 = loc("305_Mul")
#loc331 = loc("305_Mul306_Conv_f16")
#loc332 = loc("model.21.conv.bias")
#loc333 = loc("306_Convmodel.21.conv.weight_f16")
#loc334 = loc("306_Conv")
#loc335 = loc("306_Conv308_Mul_f32")
#loc336 = loc("308_Mul")
#loc337 = loc("308_Mul309_Concat_f16")
#loc338 = loc("309_Concat")
#loc339 = loc("model.23.cv1.conv.bias")
#loc340 = loc("310_Convmodel.23.cv1.conv.weight_f16")
#loc341 = loc("310_Conv")
#loc342 = loc("310_Conv312_Mul_f32")
#loc343 = loc("312_Mul")
#loc344 = loc("312_Mul313_Conv_f16")
#loc345 = loc("model.23.m.0.cv1.conv.bias")
#loc346 = loc("313_Convmodel.23.m.0.cv1.conv.weight_f16")
#loc347 = loc("313_Conv")
#loc348 = loc("313_Conv315_Mul_f32")
#loc349 = loc("315_Mul")
#loc350 = loc("315_Mul316_Conv_f16")
#loc351 = loc("model.23.m.0.cv2.conv.bias")
#loc352 = loc("316_Convmodel.23.m.0.cv2.conv.weight_f16")
#loc353 = loc("316_Conv")
#loc354 = loc("316_Conv318_Mul_f32")
#loc355 = loc("318_Mul")
#loc356 = loc("318_Mul322_Concat_f16")
#loc357 = loc("model.23.cv2.conv.bias")
#loc358 = loc("319_Convmodel.23.cv2.conv.weight_f16")
#loc359 = loc("319_Conv")
#loc360 = loc("319_Conv321_Mul_f32")
#loc361 = loc("321_Mul")
#loc362 = loc("321_Mul322_Concat_f16")
#loc363 = loc("322_Concat")
#loc364 = loc("model.23.cv3.conv.bias")
#loc365 = loc("323_Convmodel.23.cv3.conv.weight_f16")
#loc366 = loc("323_Conv")
#loc367 = loc("323_Conv325_Mul_f32")
#loc368 = loc("325_Mul")
#loc369 = loc("325_Mul622_Conv_f16")
#loc370 = loc("model.24.m.0.bias")
#loc371 = loc("326_Convmodel.24.m.0.weight_f16")
#loc372 = loc("326_Conv")
#loc373 = loc("349_Reshape")
#loc374 = loc("350_Transpose")
#loc375 = loc("350_Transpose_f32")
#loc376 = loc("model.24.m.1.bias")
#loc377 = loc("474_Convmodel.24.m.1.weight_f16")
#loc378 = loc("474_Conv")
#loc379 = loc("497_Reshape")
#loc380 = loc("498_Transpose")
#loc381 = loc("498_Transpose_f32")
#loc382 = loc("model.24.m.2.bias")
#loc383 = loc("622_Convmodel.24.m.2.weight_f16")
#loc384 = loc("622_Conv")
#loc385 = loc("645_Reshape")
#loc386 = loc("646_Transpose")
#loc387 = loc("646_Transpose_f32")

