#loc = loc(unknown)
#loc1 = loc("images")
module @yolov5s attributes {module.FLOPs = 16683511600 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "bm1684x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["images"], module.mode = "INT8", module.outputs = ["350_Transpose_f32", "498_Transpose_f32", "646_Transpose_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "yolov5s_tpu_addressed_bm1684x_int8_sym_weight.npz"} {
  module @yolov5s attributes {module.coeff_addr = 4294967296 : i64, module.coeff_size = 8146944 : i64, module.device_id = 0 : i64, module.dynamic_coeff_offset = 8146944 : i64, module.neuron_addr = 4303114240 : i64, module.neuron_size = 13692928 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x640x640xf32> loc(unknown)) -> (tensor<1x3x80x80x85xf32, 4308643840 : i64>, tensor<1x3x40x40x85xf32, 4315172864 : i64>, tensor<1x3x20x20x85xf32, 4308234240 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [640, 640], scale = [0.0039216000586748123, 0.0039216000586748123, 0.0039216000586748123]} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640x!quant.calibrated<f32<0.000000e+00:1.000008>>, 4303114240 : i64> loc(#loc1)
      %1:3 = call @subfunc_0(%0) : (tensor<1x3x640x640x!quant.calibrated<f32<0.000000e+00:1.000008>>, 4303114240 : i64>) -> (tensor<1x3x80x80x85xf32, 4308643840 : i64>, tensor<1x3x40x40x85xf32, 4315172864 : i64>, tensor<1x3x20x20x85xf32, 4308234240 : i64>) loc(#loc)
      return %1#0, %1#1, %1#2 : tensor<1x3x80x80x85xf32, 4308643840 : i64>, tensor<1x3x40x40x85xf32, 4315172864 : i64>, tensor<1x3x20x20x85xf32, 4308234240 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x640x640x!quant.calibrated<f32<0.000000e+00:1.000008>>, 4303114240 : i64> loc("images")) -> (tensor<1x3x80x80x85xf32, 4308643840 : i64>, tensor<1x3x40x40x85xf32, 4315172864 : i64>, tensor<1x3x20x20x85xf32, 4308234240 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "tpu.Cast"(%arg0) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x640x640x!quant.calibrated<f32<0.000000e+00:1.000008>>, 4303114240 : i64>) -> tensor<1x3x640x640x!quant.uniform<u8:f32, 3.921600e-03>, 4308029440 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x32x1x448xsi8, 4294967296 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4294983680 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4294987776 : i64> loc(#loc5)
      %5 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295028736 : i64> loc(#loc6)
      %6 = "top.Weight"() : () -> tensor<1x32x1x128xsi8, 4295032832 : i64> loc(#loc7)
      %7 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295036928 : i64> loc(#loc8)
      %8 = "top.Weight"() : () -> tensor<1x32x1x128xsi8, 4295041024 : i64> loc(#loc9)
      %9 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295045120 : i64> loc(#loc10)
      %10 = "top.Weight"() : () -> tensor<1x32x1x640xsi8, 4295049216 : i64> loc(#loc11)
      %11 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295069696 : i64> loc(#loc12)
      %12 = "top.Weight"() : () -> tensor<1x32x1x128xsi8, 4295073792 : i64> loc(#loc13)
      %13 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295077888 : i64> loc(#loc14)
      %14 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 4295081984 : i64> loc(#loc15)
      %15 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295090176 : i64> loc(#loc16)
      %16 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4295094272 : i64> loc(#loc17)
      %17 = "tpu.Group"(%1) ({
        %132 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 93440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 116, 244, 372, 500], h_slice = [134, 146, 146, 146, 140], w_idx = [0], w_slice = [640], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 1 : i64} : (tensor<1x3x640x640x!quant.uniform<u8:f32, 3.921600e-03>, 4308029440 : i64>) -> tensor<1x3x640x640x!quant.uniform<u8:f32, 3.921600e-03>> loc(#loc19)
        %133 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 95488, out_size = 448, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [448], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x448xsi8, 4294967296 : i64>) -> tensor<1x32x1x448xsi8> loc(#loc20)
        %134 = "tpu.Load"(%3) {do_bcast = true, ginfo = #tpu.lg<out_addr = 95936, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4294983680 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc21)
        %135 = "tpu.Conv2D"(%132, %133, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 22720, buffer_addr = 121024, buffer_size = 136454, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 59, 123, 187, 251], h_slice = [66, 71, 71, 71, 69], w_idx = [0], w_slice = [320], id = 3, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [6, 6], kernel_zp = 0 : i64, pads = [2, 2, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 1 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x640x640x!quant.uniform<u8:f32, 3.921600e-03>>, tensor<1x32x1x448xsi8>, none) -> tensor<1x32x320x320x!quant.uniform<i8:f32, 0.32595742834645669>> loc(#loc22)
        %136 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 93440, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4294987776 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc23)
        %137 = "tpu.Lut"(%135, %134) {ginfo = #tpu.lg<out_addr = 0, out_size = 22720, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 59, 123, 187, 251], h_slice = [66, 71, 71, 71, 69], w_idx = [0], w_slice = [320], id = 5, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x320x320x!quant.uniform<i8:f32, 0.32595742834645669>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x320x320x!quant.uniform<i8:f32, 0.29535947637795273>> loc(#loc24)
        %138 = "tpu.Load"(%5) {do_bcast = true, ginfo = #tpu.lg<out_addr = 258816, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295028736 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc25)
        %139 = "tpu.Conv2D"(%137, %136, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 5632, buffer_addr = 49152, buffer_size = 22528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 7, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x320x320x!quant.uniform<i8:f32, 0.29535947637795273>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.37150840629921261>> loc(#loc26)
        %140 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 96192, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x128xsi8, 4295032832 : i64>) -> tensor<1x32x1x128xsi8> loc(#loc27)
        %141 = "tpu.Lut"(%139, %138) {ginfo = #tpu.lg<out_addr = 0, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 9, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.37150840629921261>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.31670154330708666>> loc(#loc28)
        %142 = "tpu.Load"(%7) {do_bcast = true, ginfo = #tpu.lg<out_addr = 94720, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295036928 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc29)
        %143 = "tpu.Conv2D"(%141, %140, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5632, buffer_addr = 32768, buffer_size = 22528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 11, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.31670154330708666>>, tensor<1x32x1x128xsi8>, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.24746363149606301>> loc(#loc30)
        %144 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 96320, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x128xsi8, 4295041024 : i64>) -> tensor<1x32x1x128xsi8> loc(#loc31)
        %145 = "tpu.Lut"(%143, %142) {ginfo = #tpu.lg<out_addr = 5632, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 13, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.24746363149606301>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.032856783464566927>> loc(#loc32)
        %146 = "tpu.Load"(%9) {do_bcast = true, ginfo = #tpu.lg<out_addr = 259072, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295045120 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc33)
        %147 = "tpu.Conv2D"(%145, %144, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 5632, buffer_addr = 16384, buffer_size = 22528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 15, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.032856783464566927>>, tensor<1x32x1x128xsi8>, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.11114269842519685>> loc(#loc34)
        %148 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 94080, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x640xsi8, 4295049216 : i64>) -> tensor<1x32x1x640xsi8> loc(#loc35)
        %149 = "tpu.Lut"(%147, %146) {ginfo = #tpu.lg<out_addr = 11264, out_size = 5632, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 30, 62, 94, 126], h_slice = [33, 35, 35, 35, 34], w_idx = [0], w_slice = [160], id = 17, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.11114269842519685>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.10140994409448818>> loc(#loc36)
        %150 = "tpu.Load"(%11) {do_bcast = true, ginfo = #tpu.lg<out_addr = 261888, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 18, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295069696 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc37)
        %151 = "tpu.Conv2D"(%149, %148, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 5312, buffer_addr = 49152, buffer_size = 21248, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 19, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.10140994409448818>>, tensor<1x32x1x640xsi8>, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.095083035433070867>> loc(#loc38)
        %152 = "tpu.Lut"(%151, %150) {ginfo = #tpu.lg<out_addr = 65536, out_size = 5312, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.095083035433070867>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.088719262204724411>> loc(#loc39)
        %153 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 96448, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x128xsi8, 4295073792 : i64>) -> tensor<1x32x1x128xsi8> loc(#loc40)
        %154 = "tpu.Add"(%145, %152) {do_relu = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 5312, buffer_addr = 32768, buffer_size = 21248, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 22, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [69, 47], relu_limit = -1.000000e+00 : f64, rshifts = [8, 6]} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.032856783464566927>>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.088719262204724411>>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.12114059212598426>> loc(#loc41)
        %155 = "tpu.Load"(%13) {do_bcast = true, ginfo = #tpu.lg<out_addr = 94976, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295077888 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc42)
        %156 = "tpu.Conv2D"(%141, %153, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 21696, out_size = 5312, buffer_addr = 32768, buffer_size = 21248, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 24, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.31670154330708666>>, tensor<1x32x1x128xsi8>, none) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.13293712362204724>> loc(#loc43)
        %157 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 96576, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 25, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 4295081984 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc44)
        %158 = "tpu.Lut"(%156, %155) {ginfo = #tpu.lg<out_addr = 0, out_size = 5312, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 26, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.13293712362204724>>, tensor<1x1x1x256xsi8>) -> tensor<1x32x160x160x!quant.uniform<i8:f32, 0.12114059212598426>> loc(#loc45)
        %159 = "tpu.Concat"(%154, %158) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 5312, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 27, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x160x160x!quant.uniform<i8:f32, 0.12114059212598426>>, tensor<1x32x160x160x!quant.uniform<i8:f32, 0.12114059212598426>>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12114059212598426>> loc(#loc46)
        %160 = "tpu.Load"(%15) {do_bcast = true, ginfo = #tpu.lg<out_addr = 95232, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295090176 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc47)
        %161 = "tpu.Conv2D"(%159, %157, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 5312, buffer_addr = 16384, buffer_size = 21248, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 29, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.12114059212598426>>, tensor<1x64x1x128xsi8>, none) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.079613174015748023>> loc(#loc48)
        %162 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 257536, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 30, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4295094272 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc49)
        %163 = "tpu.Lut"(%161, %160) {ginfo = #tpu.lg<out_addr = 98304, out_size = 5312, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 31, 63, 95, 127], h_slice = [32, 33, 33, 33, 33], w_idx = [0], w_slice = [160], id = 31, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.079613174015748023>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x160x160x!quant.uniform<i8:f32, 0.08493915826771653>> loc(#loc50)
        %164 = "tpu.Conv2D"(%163, %162, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 259328, out_size = 2560, buffer_addr = 114688, buffer_size = 10240, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 16, 32, 48, 64], h_slice = [16, 16, 16, 16, 16], w_idx = [0], w_slice = [80], id = 32, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160x!quant.uniform<i8:f32, 0.08493915826771653>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>> loc(#loc18)
        %165 = "tpu.Store"(%164, %0) {ginfo = #tpu.lg<out_addr = 259328, out_size = 2560, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 16, 32, 48, 64], h_slice = [16, 16, 16, 16, 16], w_idx = [0], w_slice = [80], id = 33, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>>, none) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>, 4310278144 : i64> loc(#loc18)
        "tpu.Yield"(%165) : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>, 4310278144 : i64>) -> () loc(#loc18)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, 33, -2, 5, 4, -3, 7, 6, -4, 9, 8, -5, 11, 10, -6, 13, 12, -7, 15, 14, -8, 17, 16, -9, 19, 18, -10, 20, -11, 22, 21, -12, 24, 23, -13, 26, 25, -14, 27, -15, 29, 28, -16, 31, 30, -17, 32, 0, 1], group_type = 0 : i64, hsecs = 5 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-14, 1], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x640x640x!quant.uniform<u8:f32, 3.921600e-03>, 4308029440 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>, 4310278144 : i64> loc(#loc18)
      %18 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295176192 : i64> loc(#loc51)
      %19 = "top.Weight"() : () -> tensor<1x64x1x192xsi8, 4295180288 : i64> loc(#loc52)
      %20 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295192576 : i64> loc(#loc53)
      %21 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 4295196672 : i64> loc(#loc54)
      %22 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295204864 : i64> loc(#loc55)
      %23 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4295208960 : i64> loc(#loc56)
      %24 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295249920 : i64> loc(#loc57)
      %25 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 4295254016 : i64> loc(#loc58)
      %26 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295262208 : i64> loc(#loc59)
      %27 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4295266304 : i64> loc(#loc60)
      %28 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295307264 : i64> loc(#loc61)
      %29 = "top.Weight"() : () -> tensor<1x64x1x192xsi8, 4295311360 : i64> loc(#loc62)
      %30 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295323648 : i64> loc(#loc63)
      %31 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4295327744 : i64> loc(#loc64)
      %32 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295352320 : i64> loc(#loc65)
      %33 = "top.Weight"() : () -> tensor<1x64x1x4864xsi8, 4295356416 : i64> loc(#loc66)
      %34 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295667712 : i64> loc(#loc67)
      %35 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4295671808 : i64> loc(#loc68)
      %36 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295712768 : i64> loc(#loc69)
      %37 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4295716864 : i64> loc(#loc70)
      %38 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295741440 : i64> loc(#loc71)
      %39 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4295745536 : i64> loc(#loc72)
      %40 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295901184 : i64> loc(#loc73)
      %41 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4295905280 : i64> loc(#loc74)
      %42 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4295929856 : i64> loc(#loc75)
      %43 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4295933952 : i64> loc(#loc76)
      %44 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4296089600 : i64> loc(#loc77)
      %45 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4296093696 : i64> loc(#loc78)
      %46 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4296118272 : i64> loc(#loc79)
      %47 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4296122368 : i64> loc(#loc80)
      %48 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4296278016 : i64> loc(#loc81)
      %49 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4296282112 : i64> loc(#loc82)
      %50 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4296323072 : i64> loc(#loc83)
      %51 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4296327168 : i64> loc(#loc84)
      %52 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4296409088 : i64> loc(#loc85)
      %53 = "top.Weight"() : () -> tensor<1x64x1x18944xsi8, 4296413184 : i64> loc(#loc86)
      %54 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4297625600 : i64> loc(#loc87)
      %55 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4297629696 : i64> loc(#loc88)
      %56 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4297777152 : i64> loc(#loc89)
      %57 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4297781248 : i64> loc(#loc90)
      %58 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4297863168 : i64> loc(#loc91)
      %59 = "top.Weight"() : () -> tensor<1x64x1x9472xsi8, 4297867264 : i64> loc(#loc92)
      %60 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4298473472 : i64> loc(#loc93)
      %61 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4298477568 : i64> loc(#loc94)
      %62 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4298625024 : i64> loc(#loc95)
      %63 = "top.Weight"() : () -> tensor<1x64x1x4608xsi8, 4298629120 : i64> loc(#loc96)
      %64 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4298924032 : i64> loc(#loc97)
      %65 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4298928128 : i64> loc(#loc98)
      %66 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4299075584 : i64> loc(#loc99)
      %67 = "top.Weight"() : () -> tensor<1x64x1x8704xsi8, 4299079680 : i64> loc(#loc100)
      %68 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4299636736 : i64> loc(#loc101)
      %69 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4299640832 : i64> loc(#loc102)
      %70 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4299788288 : i64> loc(#loc103)
      %71 = "top.Weight"() : () -> tensor<1x64x1x1152xsi8, 4299792384 : i64> loc(#loc104)
      %72 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4299866112 : i64> loc(#loc105)
      %73 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4299870208 : i64> loc(#loc106)
      %74 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4299894784 : i64> loc(#loc107)
      %75 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4299898880 : i64> loc(#loc108)
      %76 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300054528 : i64> loc(#loc109)
      %77 = "top.Weight"() : () -> tensor<1x64x1x1152xsi8, 4300058624 : i64> loc(#loc110)
      %78 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300132352 : i64> loc(#loc111)
      %79 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4300136448 : i64> loc(#loc112)
      %80 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300218368 : i64> loc(#loc113)
      %81 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4300222464 : i64> loc(#loc114)
      %82 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300263424 : i64> loc(#loc115)
      %83 = "top.Weight"() : () -> tensor<1x64x1x320xsi8, 4300267520 : i64> loc(#loc116)
      %84 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300288000 : i64> loc(#loc117)
      %85 = "top.Weight"() : () -> tensor<1x64x1x128xsi8, 4300292096 : i64> loc(#loc118)
      %86 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300300288 : i64> loc(#loc119)
      %87 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4300304384 : i64> loc(#loc120)
      %88 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300345344 : i64> loc(#loc121)
      %89 = "top.Weight"() : () -> tensor<1x64x1x320xsi8, 4300349440 : i64> loc(#loc122)
      %90 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300369920 : i64> loc(#loc123)
      %91 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4300374016 : i64> loc(#loc124)
      %92 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300398592 : i64> loc(#loc125)
      %93 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4300402688 : i64> loc(#loc126)
      %94 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300558336 : i64> loc(#loc127)
      %95 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4300562432 : i64> loc(#loc128)
      %96 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300603392 : i64> loc(#loc129)
      %97 = "top.Weight"() : () -> tensor<1x64x1x384xsi8, 4300607488 : i64> loc(#loc130)
      %98 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300632064 : i64> loc(#loc131)
      %99 = "top.Weight"() : () -> tensor<1x64x1x2432xsi8, 4300636160 : i64> loc(#loc132)
      %100 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300791808 : i64> loc(#loc133)
      %101 = "top.Weight"() : () -> tensor<1x64x1x640xsi8, 4300795904 : i64> loc(#loc134)
      %102 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300836864 : i64> loc(#loc135)
      %103 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4300840960 : i64> loc(#loc136)
      %104 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4300922880 : i64> loc(#loc137)
      %105 = "top.Weight"() : () -> tensor<1x64x1x9472xsi8, 4300926976 : i64> loc(#loc138)
      %106 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4301533184 : i64> loc(#loc139)
      %107 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4301537280 : i64> loc(#loc140)
      %108 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4301684736 : i64> loc(#loc141)
      %109 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4301688832 : i64> loc(#loc142)
      %110 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4301770752 : i64> loc(#loc143)
      %111 = "top.Weight"() : () -> tensor<1x64x1x9472xsi8, 4301774848 : i64> loc(#loc144)
      %112 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4302381056 : i64> loc(#loc145)
      %113 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4302385152 : i64> loc(#loc146)
      %114 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4302532608 : i64> loc(#loc147)
      %115 = "top.Weight"() : () -> tensor<1x64x1x4608xsi8, 4302536704 : i64> loc(#loc148)
      %116 = "top.Weight"() : () -> tensor<1x1x1x256xsi8, 4302831616 : i64> loc(#loc149)
      %117 = "top.Weight"() : () -> tensor<1x64x1x768xsi8, 4302835712 : i64> loc(#loc150)
      %118:3 = "tpu.Group"(%17) ({
        %132 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>, 4310278144 : i64>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>> loc(#loc154)
        %133 = "tpu.Load"(%18) {do_bcast = true, ginfo = #tpu.lg<out_addr = 212992, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 1, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295176192 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc155)
        %134 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x192xsi8, 4295180288 : i64>) -> tensor<1x64x1x192xsi8> loc(#loc156)
        %135 = "tpu.Lut"(%132, %133) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 3, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.039389724409448816>> loc(#loc157)
        %136 = "tpu.Load"(%20) {do_bcast = true, ginfo = #tpu.lg<out_addr = 49152, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295192576 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc158)
        %137 = "tpu.Conv2D"(%135, %134, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.039389724409448816>>, tensor<1x64x1x192xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.024539233858267718>> loc(#loc159)
        %138 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 4295196672 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc160)
        %139 = "tpu.Lut"(%137, %136) {ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 7, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.024539233858267718>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.01122580157480315>> loc(#loc161)
        %140 = "tpu.Load"(%22) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295204864 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc162)
        %141 = "tpu.Conv2D"(%139, %138, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 49152, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 9, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.01122580157480315>>, tensor<1x64x1x128xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.043872662204724411>> loc(#loc163)
        %142 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4295208960 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc164)
        %143 = "tpu.Lut"(%141, %140) {ginfo = #tpu.lg<out_addr = 19200, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 11, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.043872662204724411>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.047696196062992122>> loc(#loc165)
        %144 = "tpu.Load"(%24) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 12, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295249920 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc166)
        %145 = "tpu.Conv2D"(%143, %142, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 13, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.047696196062992122>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.03659276535433071>> loc(#loc167)
        %146 = "tpu.Lut"(%145, %144) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 14, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.03659276535433071>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.021455972440944885>> loc(#loc168)
        %147 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 4295254016 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc169)
        %148 = "tpu.Add"(%139, %146) {do_relu = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 81920, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 16, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [59, 113], relu_limit = -1.000000e+00 : f64, rshifts = [7, 7]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.01122580157480315>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.021455972440944885>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.024273014960629922>> loc(#loc170)
        %149 = "tpu.Load"(%26) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 17, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295262208 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc171)
        %150 = "tpu.Conv2D"(%148, %147, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.024273014960629922>>, tensor<1x64x1x128xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.041986418110236221>> loc(#loc172)
        %151 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 55552, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 19, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4295266304 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc173)
        %152 = "tpu.Lut"(%150, %149) {ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 20, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.041986418110236221>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.047403425984251971>> loc(#loc174)
        %153 = "tpu.Load"(%28) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 21, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295307264 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc175)
        %154 = "tpu.Conv2D"(%152, %151, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 22, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.047403425984251971>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048311879527559054>> loc(#loc176)
        %155 = "tpu.Lut"(%154, %153) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 23, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048311879527559054>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.039847262204724412>> loc(#loc177)
        %156 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 24, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x192xsi8, 4295311360 : i64>) -> tensor<1x64x1x192xsi8> loc(#loc178)
        %157 = "tpu.Add"(%148, %155) {do_relu = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6400, buffer_addr = 12800, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 25, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [1, 105], relu_limit = -1.000000e+00 : f64, rshifts = [1, 7]} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.024273014960629922>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.039847262204724412>>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048706242519685042>> loc(#loc179)
        %158 = "tpu.Load"(%30) {do_bcast = true, ginfo = #tpu.lg<out_addr = 32768, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 26, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295323648 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc180)
        %159 = "tpu.Conv2D"(%135, %156, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 49152, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 27, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.039389724409448816>>, tensor<1x64x1x192xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.038964575590551179>> loc(#loc181)
        %160 = "tpu.Load"(%31) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4295327744 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc182)
        %161 = "tpu.Lut"(%159, %158) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 29, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.038964575590551179>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048706242519685042>> loc(#loc183)
        %162 = "tpu.Load"(%32) {do_bcast = true, ginfo = #tpu.lg<out_addr = 25600, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 30, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295352320 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc184)
        %163 = "tpu.Concat"(%157, %161) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 31, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048706242519685042>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.048706242519685042>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.048706242519685042>> loc(#loc185)
        %164 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 4864, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [4864], id = 32, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x4864xsi8, 4295356416 : i64>) -> tensor<1x64x1x4864xsi8> loc(#loc186)
        %165 = "tpu.Load"(%34) {do_bcast = true, ginfo = #tpu.lg<out_addr = 56576, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 33, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295667712 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc187)
        %166 = "tpu.Load"(%53) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 18944, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [18944], id = 34, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x18944xsi8, 4296413184 : i64>) -> tensor<1x64x1x18944xsi8> loc(#loc188)
        %167 = "tpu.Conv2D"(%163, %160, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 12800, buffer_addr = 81920, buffer_size = 51200, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 35, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.048706242519685042>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.03965484409448819>> loc(#loc189)
        %168 = "tpu.Lut"(%167, %162) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 36, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.03965484409448819>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.030274422834645671>> loc(#loc190)
        %169 = "tpu.Load"(%35) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 37, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4295671808 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc191)
        %170 = "tpu.Conv2D"(%168, %164, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 38, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.030274422834645671>>, tensor<1x64x1x4864xsi8>, none) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.063787422834645668>> loc(#loc192)
        %171 = "tpu.Lut"(%170, %165) {ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 39, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.063787422834645668>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.033977317322834648>> loc(#loc193)
        %172 = "tpu.Load"(%36) {do_bcast = true, ginfo = #tpu.lg<out_addr = 114688, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 40, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295712768 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc194)
        %173 = "tpu.Conv2D"(%171, %169, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 51712, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 41, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.033977317322834648>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.030121476377952757>> loc(#loc195)
        %174 = "tpu.Load"(%37) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 42, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4295716864 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc196)
        %175 = "tpu.Lut"(%173, %172) {ginfo = #tpu.lg<out_addr = 51712, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 43, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.030121476377952757>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.014900897637795276>> loc(#loc197)
        %176 = "tpu.Load"(%38) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 44, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295741440 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc198)
        %177 = "tpu.Conv2D"(%175, %174, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 98304, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 45, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.014900897637795276>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.059924351181102363>> loc(#loc199)
        %178 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 46, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4295745536 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc200)
        %179 = "tpu.Lut"(%177, %176) {ginfo = #tpu.lg<out_addr = 54912, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 47, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.059924351181102363>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.042636174015748034>> loc(#loc201)
        %180 = "tpu.Load"(%40) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 48, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295901184 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc202)
        %181 = "tpu.Conv2D"(%179, %178, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 19200, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 49, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.042636174015748034>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.055917454330708667>> loc(#loc203)
        %182 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 50, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4295905280 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc204)
        %183 = "tpu.Lut"(%181, %180) {ginfo = #tpu.lg<out_addr = 22400, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 51, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.055917454330708667>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.018550628346456693>> loc(#loc205)
        %184 = "tpu.Add"(%175, %183) {do_relu = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 68736, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 52, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [95, 119], relu_limit = -1.000000e+00 : f64, rshifts = [7, 7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.014900897637795276>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.018550628346456693>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.02003296535433071>> loc(#loc206)
        %185 = "tpu.Load"(%42) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 53, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4295929856 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc207)
        %186 = "tpu.Conv2D"(%184, %182, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 54, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.02003296535433071>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.051305083464566928>> loc(#loc208)
        %187 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 55, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4295933952 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc209)
        %188 = "tpu.Lut"(%186, %185) {ginfo = #tpu.lg<out_addr = 22400, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 56, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.051305083464566928>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.039380272440944879>> loc(#loc210)
        %189 = "tpu.Load"(%44) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 57, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4296089600 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc211)
        %190 = "tpu.Conv2D"(%188, %187, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 3200, buffer_addr = 81920, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 58, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.039380272440944879>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045380907874015747>> loc(#loc212)
        %191 = "tpu.Lut"(%190, %189) {ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 59, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045380907874015747>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029157995275590549>> loc(#loc213)
        %192 = "tpu.Load"(%45) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 60, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4296093696 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc214)
        %193 = "tpu.Add"(%184, %191) {do_relu = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 3200, buffer_addr = 98304, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 61, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [67, 49], relu_limit = -1.000000e+00 : f64, rshifts = [7, 6]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.02003296535433071>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029157995275590549>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.038235908661417323>> loc(#loc215)
        %194 = "tpu.Load"(%46) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 62, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4296118272 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc216)
        %195 = "tpu.Conv2D"(%193, %192, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 63, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.038235908661417323>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.051927754330708661>> loc(#loc217)
        %196 = "tpu.Load"(%47) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 64, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4296122368 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc218)
        %197 = "tpu.Lut"(%195, %194) {ginfo = #tpu.lg<out_addr = 54912, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 65, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.051927754330708661>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045076109448818895>> loc(#loc219)
        %198 = "tpu.Load"(%48) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 66, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4296278016 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc220)
        %199 = "tpu.Conv2D"(%197, %196, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 81920, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 67, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045076109448818895>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.067552266141732278>> loc(#loc221)
        %200 = "tpu.Load"(%49) {do_bcast = false, ginfo = #tpu.lg<out_addr = 68736, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 68, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4296282112 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc222)
        %201 = "tpu.Lut"(%199, %198) {ginfo = #tpu.lg<out_addr = 81920, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 69, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.067552266141732278>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.043672233858267719>> loc(#loc223)
        %202 = "tpu.Add"(%193, %201) {do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 19200, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 70, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [93, 107], relu_limit = -1.000000e+00 : f64, rshifts = [7, 7]} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.038235908661417323>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.043672233858267719>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052360886614173228>> loc(#loc224)
        %203 = "tpu.Load"(%50) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 71, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4296323072 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc225)
        %204 = "tpu.Conv2D"(%171, %200, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 3200, buffer_addr = 81920, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 72, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.033977317322834648>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052100023622047249>> loc(#loc226)
        %205 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 73, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4296327168 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc227)
        %206 = "tpu.Lut"(%204, %203) {ginfo = #tpu.lg<out_addr = 81920, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 74, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052100023622047249>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052360886614173228>> loc(#loc228)
        %207 = "tpu.Concat"(%202, %206) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 75, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052360886614173228>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052360886614173228>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.052360886614173228>> loc(#loc229)
        %208 = "tpu.Load"(%52) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 76, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4296409088 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc230)
        %209 = "tpu.Conv2D"(%207, %205, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 51712, out_size = 6400, buffer_addr = 81920, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 77, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.052360886614173228>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.05543504566929134>> loc(#loc231)
        %210 = "tpu.Lut"(%209, %208) {ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 78, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.05543504566929134>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040043029133858273>> loc(#loc232)
        %211 = "tpu.Load"(%54) {do_bcast = true, ginfo = #tpu.lg<out_addr = 75008, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 79, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4297625600 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc233)
        %212 = "tpu.Load"(%59) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 9472, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9472], id = 80, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9472xsi8, 4297867264 : i64>) -> tensor<1x64x1x9472xsi8> loc(#loc234)
        %213 = "tpu.Load"(%63) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [4608], id = 81, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x4608xsi8, 4298629120 : i64>) -> tensor<1x64x1x4608xsi8> loc(#loc235)
        %214 = "tpu.Conv2D"(%210, %166, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 3584, buffer_addr = 114688, buffer_size = 14336, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 82, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040043029133858273>>, tensor<1x64x1x18944xsi8>, none) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.061178327559055119>> loc(#loc236)
        %215 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 83, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4297629696 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc237)
        %216 = "tpu.Lut"(%214, %211) {ginfo = #tpu.lg<out_addr = 32768, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 84, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.061178327559055119>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.040236619685039374>> loc(#loc238)
        %217 = "tpu.Load"(%56) {do_bcast = true, ginfo = #tpu.lg<out_addr = 102912, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 85, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4297777152 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc239)
        %218 = "tpu.Conv2D"(%216, %215, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 75008, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 86, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.040236619685039374>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.0523291874015748>> loc(#loc240)
        %219 = "tpu.Load"(%57) {do_bcast = false, ginfo = #tpu.lg<out_addr = 75008, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 87, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4297781248 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc241)
        %220 = "tpu.Lut"(%218, %217) {ginfo = #tpu.lg<out_addr = 36352, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 88, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.0523291874015748>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.021297727559055117>> loc(#loc242)
        %221 = "tpu.Load"(%58) {do_bcast = true, ginfo = #tpu.lg<out_addr = 49152, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 89, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4297863168 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc243)
        %222 = "tpu.Conv2D"(%220, %219, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 1792, buffer_addr = 81920, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 90, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.021297727559055117>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.083065092913385832>> loc(#loc244)
        %223 = "tpu.Lut"(%222, %221) {ginfo = #tpu.lg<out_addr = 38144, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 91, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.083065092913385832>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.066370716535433069>> loc(#loc245)
        %224 = "tpu.Load"(%60) {do_bcast = true, ginfo = #tpu.lg<out_addr = 102912, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 92, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4298473472 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc246)
        %225 = "tpu.Load"(%61) {do_bcast = false, ginfo = #tpu.lg<out_addr = 27904, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 93, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4298477568 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc247)
        %226 = "tpu.Load"(%67) {do_bcast = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 8704, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [8704], id = 94, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x8704xsi8, 4299079680 : i64>) -> tensor<1x64x1x8704xsi8> loc(#loc248)
        %227 = "tpu.Conv2D"(%223, %212, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 81920, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 95, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.066370716535433069>>, tensor<1x64x1x9472xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10008439527559056>> loc(#loc249)
        %228 = "tpu.Lut"(%227, %224) {ginfo = #tpu.lg<out_addr = 30208, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 96, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.10008439527559056>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.055023859055118106>> loc(#loc250)
        %229 = "tpu.Add"(%220, %228) {do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 1792, buffer_addr = 49152, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 97, stage = 1, slice_idx = 0, group_type = 0>, multipliers = [81, 105], relu_limit = -1.000000e+00 : f64, rshifts = [8, 7]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.021297727559055117>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.055023859055118106>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.067261059842519691>> loc(#loc251)
        %230 = "tpu.Load"(%62) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 98, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4298625024 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc252)
        %231 = "tpu.Conv2D"(%216, %225, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 67328, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 99, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.040236619685039374>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.066902498425196855>> loc(#loc253)
        %232 = "tpu.Lut"(%231, %230) {ginfo = #tpu.lg<out_addr = 32768, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 100, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.066902498425196855>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.067261059842519691>> loc(#loc254)
        %233 = "tpu.Concat"(%229, %232) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 27904, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 101, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.067261059842519691>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.067261059842519691>>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.067261059842519691>> loc(#loc255)
        %234 = "tpu.Load"(%64) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 102, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4298924032 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc256)
        %235 = "tpu.Conv2D"(%233, %213, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 3584, buffer_addr = 49152, buffer_size = 14336, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 103, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.067261059842519691>>, tensor<1x64x1x4608xsi8>, none) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.084654470078740162>> loc(#loc257)
        %236 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 104, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4298928128 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc258)
        %237 = "tpu.Lut"(%235, %234) {ginfo = #tpu.lg<out_addr = 27904, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 105, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.084654470078740162>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046091244094488192>> loc(#loc259)
        %238 = "tpu.Load"(%66) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 106, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4299075584 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc260)
        %239 = "tpu.Conv2D"(%237, %236, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 1792, buffer_addr = 65536, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 107, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046091244094488192>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.047517513385826778>> loc(#loc261)
        %240 = "tpu.Lut"(%239, %238) {ginfo = #tpu.lg<out_addr = 27904, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 108, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.047517513385826778>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>> loc(#loc262)
        %241 = "tpu.Pool2D"(%240) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 109, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>> loc(#loc263)
        %242 = "tpu.Pool2D"(%241) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 65536, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 110, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>> loc(#loc264)
        %243 = "tpu.Pool2D"(%242) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 81920, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 111, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>> loc(#loc265)
        %244 = "tpu.Concat"(%240, %241, %242, %243) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 7168, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1024], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 112, stage = 1, slice_idx = 0, group_type = 0>, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>) -> tensor<1x1024x20x20x!quant.uniform<i8:f32, 0.058765585039370079>> loc(#loc266)
        %245 = "tpu.Load"(%68) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 113, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4299636736 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc267)
        %246 = "tpu.Conv2D"(%244, %226, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 3584, buffer_addr = 65536, buffer_size = 14336, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 114, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x1024x20x20x!quant.uniform<i8:f32, 0.058765585039370079>>, tensor<1x64x1x8704xsi8>, none) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.062088748818897636>> loc(#loc268)
        %247 = "tpu.Load"(%69) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 115, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4299640832 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc269)
        %248 = "tpu.Lut"(%246, %245) {ginfo = #tpu.lg<out_addr = 19200, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 116, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.062088748818897636>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046367635433070861>> loc(#loc270)
        %249 = "tpu.Load"(%70) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 117, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4299788288 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc271)
        %250 = "tpu.Conv2D"(%248, %247, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 1792, buffer_addr = 49152, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 118, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046367635433070861>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.056583201574803149>> loc(#loc272)
        %251 = "tpu.Load"(%71) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 119, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1152xsi8, 4299792384 : i64>) -> tensor<1x64x1x1152xsi8> loc(#loc273)
        %252 = "tpu.Lut"(%250, %249) {ginfo = #tpu.lg<out_addr = 32768, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 120, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.056583201574803149>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.045868659842519685>> loc(#loc274)
        %253 = "tpu.MulShift"(%252) {ginfo = #tpu.lg<out_addr = 107776, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 121, stage = 1, slice_idx = 0, group_type = 0>, multiplier = 1 : si32, rshift = 0 : si32} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.045868659842519685>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046014477952755907>> loc(#loc275)
        %254 = "tpu.Upsample"(%252) {do_relu = false, ginfo = #tpu.lg<out_addr = 19200, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 122, stage = 1, slice_idx = 0, group_type = 0>, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.045868659842519685>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.045868659842519685>> loc(#loc276)
        %255 = "tpu.MulShift"(%254) {ginfo = #tpu.lg<out_addr = 50304, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 123, stage = 1, slice_idx = 0, group_type = 0>, multiplier = 73 : si32, rshift = 6 : si32} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.045868659842519685>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040043029133858273>> loc(#loc277)
        %256 = "tpu.Concat"(%255, %210) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 124, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040043029133858273>>, tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040043029133858273>>) -> tensor<1x512x40x40x!quant.uniform<i8:f32, 0.040043029133858273>> loc(#loc278)
        %257 = "tpu.Load"(%72) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 125, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4299866112 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc279)
        %258 = "tpu.Conv2D"(%256, %251, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 16384, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 126, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40x!quant.uniform<i8:f32, 0.040043029133858273>>, tensor<1x64x1x1152xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.05114162283464567>> loc(#loc280)
        %259 = "tpu.Load"(%73) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45568, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 127, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4299870208 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc281)
        %260 = "tpu.Lut"(%258, %257) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 128, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.05114162283464567>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.024342402362204724>> loc(#loc282)
        %261 = "tpu.Load"(%74) {do_bcast = true, ginfo = #tpu.lg<out_addr = 49152, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 129, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4299894784 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc283)
        %262 = "tpu.Conv2D"(%260, %259, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 130, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.024342402362204724>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.05376176771653543>> loc(#loc284)
        %263 = "tpu.Load"(%75) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 131, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4299898880 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc285)
        %264 = "tpu.Lut"(%262, %261) {ginfo = #tpu.lg<out_addr = 45568, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 132, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.05376176771653543>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.034454263779527559>> loc(#loc286)
        %265 = "tpu.Load"(%76) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 133, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300054528 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc287)
        %266 = "tpu.Conv2D"(%264, %263, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 3200, buffer_addr = 12800, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 134, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.034454263779527559>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.0477942968503937>> loc(#loc288)
        %267 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 135, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1152xsi8, 4300058624 : i64>) -> tensor<1x64x1x1152xsi8> loc(#loc289)
        %268 = "tpu.Lut"(%266, %265) {ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 136, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.0477942968503937>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029759533858267715>> loc(#loc290)
        %269 = "tpu.Load"(%78) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 137, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300132352 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc291)
        %270 = "tpu.Conv2D"(%256, %267, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16000, out_size = 3200, buffer_addr = 49152, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 138, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40x!quant.uniform<i8:f32, 0.040043029133858273>>, tensor<1x64x1x1152xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.050511762204724413>> loc(#loc292)
        %271 = "tpu.Load"(%79) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 139, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4300136448 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc293)
        %272 = "tpu.Lut"(%270, %269) {ginfo = #tpu.lg<out_addr = 49152, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 140, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.050511762204724413>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029759533858267715>> loc(#loc294)
        %273 = "tpu.Concat"(%268, %272) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 141, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029759533858267715>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.029759533858267715>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.029759533858267715>> loc(#loc295)
        %274 = "tpu.Load"(%80) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 142, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300218368 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc296)
        %275 = "tpu.Conv2D"(%273, %271, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 49152, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 143, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.029759533858267715>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.062721117322834641>> loc(#loc297)
        %276 = "tpu.Load"(%81) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 144, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4300222464 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc298)
        %277 = "tpu.Lut"(%275, %274) {ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 145, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.062721117322834641>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040151231496062992>> loc(#loc299)
        %278 = "tpu.Load"(%82) {do_bcast = true, ginfo = #tpu.lg<out_addr = 16384, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 146, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300263424 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc300)
        %279 = "tpu.Conv2D"(%277, %276, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 147, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.040151231496062992>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045750985826771652>> loc(#loc301)
        %280 = "tpu.Lut"(%279, %278) {ginfo = #tpu.lg<out_addr = 32768, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 148, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.045750985826771652>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.033892595275590548>> loc(#loc302)
        %281 = "tpu.MulShift"(%280) {ginfo = #tpu.lg<out_addr = 109568, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 149, stage = 1, slice_idx = 0, group_type = 0>, multiplier = 115 : si32, rshift = 7 : si32} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.033892595275590548>>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.037835844094488189>> loc(#loc303)
        %282 = "tpu.Upsample"(%280) {do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 150, stage = 1, slice_idx = 0, group_type = 0>, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.033892595275590548>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.033892595275590548>> loc(#loc304)
        %283 = "tpu.MulShift"(%282) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 151, stage = 1, slice_idx = 0, group_type = 0>, multiplier = 9 : si32, rshift = 3 : si32} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.033892595275590548>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.030274422834645671>> loc(#loc305)
        %284 = "tpu.Load"(%83) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [320], id = 152, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x320xsi8, 4300267520 : i64>) -> tensor<1x64x1x320xsi8> loc(#loc306)
        %285 = "tpu.Concat"(%283, %168) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 153, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.030274422834645671>>, tensor<1x128x80x80x!quant.uniform<i8:f32, 0.030274422834645671>>) -> tensor<1x256x80x80x!quant.uniform<i8:f32, 0.030274422834645671>> loc(#loc307)
        %286 = "tpu.Load"(%84) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 154, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300288000 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc308)
        %287 = "tpu.Conv2D"(%285, %284, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 65536, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 155, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80x!quant.uniform<i8:f32, 0.030274422834645671>>, tensor<1x64x1x320xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.042595983464566931>> loc(#loc309)
        %288 = "tpu.Load"(%85) {do_bcast = false, ginfo = #tpu.lg<out_addr = 58368, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 156, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xsi8, 4300292096 : i64>) -> tensor<1x64x1x128xsi8> loc(#loc310)
        %289 = "tpu.Lut"(%287, %286) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 157, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.042595983464566931>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.018695966929133858>> loc(#loc311)
        %290 = "tpu.Load"(%86) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 158, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300300288 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc312)
        %291 = "tpu.Conv2D"(%289, %288, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 81920, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 159, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.018695966929133858>>, tensor<1x64x1x128xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.039544574803149601>> loc(#loc313)
        %292 = "tpu.Load"(%87) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 160, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4300304384 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc314)
        %293 = "tpu.Lut"(%291, %290) {ginfo = #tpu.lg<out_addr = 58368, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 161, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.039544574803149601>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.027242776377952756>> loc(#loc315)
        %294 = "tpu.Load"(%88) {do_bcast = true, ginfo = #tpu.lg<out_addr = 98304, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 162, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300345344 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc316)
        %295 = "tpu.Conv2D"(%293, %292, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 163, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.027242776377952756>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071361091338582677>> loc(#loc317)
        %296 = "tpu.Load"(%89) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 320, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [320], id = 164, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x320xsi8, 4300349440 : i64>) -> tensor<1x64x1x320xsi8> loc(#loc318)
        %297 = "tpu.Lut"(%295, %294) {ginfo = #tpu.lg<out_addr = 0, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 165, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071361091338582677>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071617907086614169>> loc(#loc319)
        %298 = "tpu.Load"(%90) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 166, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300369920 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc320)
        %299 = "tpu.Conv2D"(%285, %296, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 6400, out_size = 6400, buffer_addr = 81920, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 167, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80x!quant.uniform<i8:f32, 0.030274422834645671>>, tensor<1x64x1x320xsi8>, none) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.049906219685039367>> loc(#loc321)
        %300 = "tpu.Load"(%91) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 168, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4300374016 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc322)
        %301 = "tpu.Lut"(%299, %298) {ginfo = #tpu.lg<out_addr = 16384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 169, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.049906219685039367>>, tensor<1x1x1x256xsi8>) -> tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071617907086614169>> loc(#loc323)
        %302 = "tpu.Concat"(%297, %301) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 170, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071617907086614169>>, tensor<1x64x80x80x!quant.uniform<i8:f32, 0.071617907086614169>>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.071617907086614169>> loc(#loc324)
        %303 = "tpu.Load"(%92) {do_bcast = true, ginfo = #tpu.lg<out_addr = 114688, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 171, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300398592 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc325)
        %304 = "tpu.Conv2D"(%302, %300, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 12800, buffer_addr = 0, buffer_size = 51200, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 172, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.071617907086614169>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.16584052440944883>> loc(#loc326)
        %305 = "tpu.Load"(%93) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 173, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4300402688 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc327)
        %306 = "tpu.Load"(%105) {do_bcast = false, ginfo = #tpu.lg<out_addr = 45568, out_size = 9472, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9472], id = 174, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9472xsi8, 4300926976 : i64>) -> tensor<1x64x1x9472xsi8> loc(#loc328)
        %307 = "tpu.Lut"(%304, %303) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 175, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.16584052440944883>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13672920708661418>> loc(#loc329)
        %308 = "tpu.Load"(%94) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 176, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300558336 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc330)
        %309 = "tpu.Load"(%111) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 9472, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [9472], id = 177, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x9472xsi8, 4301774848 : i64>) -> tensor<1x64x1x9472xsi8> loc(#loc331)
        %310 = "tpu.Conv2D"(%307, %305, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 3200, buffer_addr = 55040, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 178, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13672920708661418>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.053125216535433069>> loc(#loc332)
        %311 = "tpu.Load"(%95) {do_bcast = false, ginfo = #tpu.lg<out_addr = 55040, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 179, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4300562432 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc333)
        %312 = "tpu.Lut"(%310, %308) {ginfo = #tpu.lg<out_addr = 32768, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 180, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.053125216535433069>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.037835844094488189>> loc(#loc334)
        %313 = "tpu.Concat"(%312, %281) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 181, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.037835844094488189>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.037835844094488189>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.037835844094488189>> loc(#loc335)
        %314 = "tpu.Load"(%96) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 182, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300603392 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc336)
        %315 = "tpu.Conv2D"(%313, %311, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 183, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.037835844094488189>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052162012598425199>> loc(#loc337)
        %316 = "tpu.Load"(%97) {do_bcast = false, ginfo = #tpu.lg<out_addr = 55040, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [384], id = 184, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x384xsi8, 4300607488 : i64>) -> tensor<1x64x1x384xsi8> loc(#loc338)
        %317 = "tpu.Lut"(%315, %314) {ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 185, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.052162012598425199>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04747687401574803>> loc(#loc339)
        %318 = "tpu.Load"(%98) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 186, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300632064 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc340)
        %319 = "tpu.Conv2D"(%317, %316, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 187, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04747687401574803>>, tensor<1x64x1x384xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.054629622834645668>> loc(#loc341)
        %320 = "tpu.Load"(%99) {do_bcast = false, ginfo = #tpu.lg<out_addr = 55040, out_size = 2432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2432], id = 188, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2432xsi8, 4300636160 : i64>) -> tensor<1x64x1x2432xsi8> loc(#loc342)
        %321 = "tpu.Lut"(%319, %318) {ginfo = #tpu.lg<out_addr = 19200, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 189, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.054629622834645668>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.030710808661417323>> loc(#loc343)
        %322 = "tpu.Load"(%100) {do_bcast = true, ginfo = #tpu.lg<out_addr = 109568, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 190, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300791808 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc344)
        %323 = "tpu.Conv2D"(%321, %320, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 81920, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 191, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.030710808661417323>>, tensor<1x64x1x2432xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.082694203937007868>> loc(#loc345)
        %324 = "tpu.Load"(%101) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 640, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [640], id = 192, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x640xsi8, 4300795904 : i64>) -> tensor<1x64x1x640xsi8> loc(#loc346)
        %325 = "tpu.Lut"(%323, %322) {ginfo = #tpu.lg<out_addr = 55040, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 193, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.082694203937007868>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04510557401574803>> loc(#loc347)
        %326 = "tpu.Load"(%102) {do_bcast = true, ginfo = #tpu.lg<out_addr = 109568, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 194, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300836864 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc348)
        %327 = "tpu.Conv2D"(%313, %324, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 58240, out_size = 3200, buffer_addr = 65536, buffer_size = 12800, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 195, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.037835844094488189>>, tensor<1x64x1x640xsi8>, none) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.050311250393700789>> loc(#loc349)
        %328 = "tpu.Load"(%103) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 196, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4300840960 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc350)
        %329 = "tpu.Lut"(%327, %326) {ginfo = #tpu.lg<out_addr = 65536, out_size = 3200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 197, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.050311250393700789>>, tensor<1x1x1x256xsi8>) -> tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04510557401574803>> loc(#loc351)
        %330 = "tpu.Concat"(%325, %329) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 198, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04510557401574803>>, tensor<1x128x40x40x!quant.uniform<i8:f32, 0.04510557401574803>>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.04510557401574803>> loc(#loc352)
        %331 = "tpu.Load"(%104) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 199, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4300922880 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc353)
        %332 = "tpu.Conv2D"(%330, %328, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 55040, out_size = 6400, buffer_addr = 109568, buffer_size = 25600, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 200, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.04510557401574803>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.16530354173228345>> loc(#loc354)
        %333 = "tpu.Lut"(%332, %331) {ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 201, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.16530354173228345>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>> loc(#loc151)
        %334 = "tpu.Load"(%106) {do_bcast = true, ginfo = #tpu.lg<out_addr = 67840, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 202, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4301533184 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc355)
        %335 = "tpu.Load"(%107) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 204, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4301537280 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc356)
        %336 = "tpu.Store"(%333, %0) {ginfo = #tpu.lg<out_addr = 12800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 203, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>>, none) -> tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>, 4308029440 : i64> loc(#loc151)
        %337 = "tpu.Conv2D"(%333, %306, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 109568, out_size = 1792, buffer_addr = 81920, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 205, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>>, tensor<1x64x1x9472xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.056673917322834645>> loc(#loc357)
        %338 = "tpu.Lut"(%337, %334) {ginfo = #tpu.lg<out_addr = 12800, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 206, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.056673917322834645>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046181678740157481>> loc(#loc358)
        %339 = "tpu.MulShift"(%338) {ginfo = #tpu.lg<out_addr = 16384, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 207, stage = 1, slice_idx = 0, group_type = 0>, multiplier = 1 : si32, rshift = 0 : si32} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046181678740157481>>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046014477952755907>> loc(#loc359)
        %340 = "tpu.Concat"(%339, %253) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 208, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046014477952755907>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.046014477952755907>>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046014477952755907>> loc(#loc360)
        %341 = "tpu.Load"(%108) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 209, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4301684736 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc361)
        %342 = "tpu.Conv2D"(%340, %335, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1792, buffer_addr = 45568, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 210, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046014477952755907>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.073095148818897629>> loc(#loc362)
        %343 = "tpu.Load"(%109) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1280, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1280], id = 211, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1280xsi8, 4301688832 : i64>) -> tensor<1x64x1x1280xsi8> loc(#loc363)
        %344 = "tpu.Lut"(%342, %341) {ginfo = #tpu.lg<out_addr = 45568, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 212, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.073095148818897629>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.053212513385826769>> loc(#loc364)
        %345 = "tpu.Load"(%110) {do_bcast = true, ginfo = #tpu.lg<out_addr = 81920, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 213, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4301770752 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc365)
        %346 = "tpu.Conv2D"(%344, %343, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 1792, buffer_addr = 65536, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 214, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.053212513385826769>>, tensor<1x64x1x1280xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.069192503149606302>> loc(#loc366)
        %347 = "tpu.Lut"(%346, %345) {ginfo = #tpu.lg<out_addr = 45568, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 215, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.069192503149606302>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.044434648818897637>> loc(#loc367)
        %348 = "tpu.Load"(%112) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 216, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4302381056 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc368)
        %349 = "tpu.Load"(%113) {do_bcast = false, ginfo = #tpu.lg<out_addr = 22784, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 217, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x2304xsi8, 4302385152 : i64>) -> tensor<1x64x1x2304xsi8> loc(#loc369)
        %350 = "tpu.Load"(%115) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [4608], id = 218, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x4608xsi8, 4302536704 : i64>) -> tensor<1x64x1x4608xsi8> loc(#loc370)
        %351 = "tpu.Conv2D"(%347, %309, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 81920, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 219, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.044434648818897637>>, tensor<1x64x1x9472xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.090067345669291337>> loc(#loc371)
        %352 = "tpu.Lut"(%351, %348) {ginfo = #tpu.lg<out_addr = 20992, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 220, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.090067345669291337>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.054138767716535433>> loc(#loc372)
        %353 = "tpu.Load"(%114) {do_bcast = true, ginfo = #tpu.lg<out_addr = 65536, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 221, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4302532608 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc373)
        %354 = "tpu.Conv2D"(%340, %349, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 45568, out_size = 1792, buffer_addr = 49152, buffer_size = 7168, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 222, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.046014477952755907>>, tensor<1x64x1x2304xsi8>, none) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.059830785826771656>> loc(#loc374)
        %355 = "tpu.Lut"(%354, %353) {ginfo = #tpu.lg<out_addr = 49152, out_size = 1792, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 223, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.059830785826771656>>, tensor<1x1x1x256xsi8>) -> tensor<1x256x20x20x!quant.uniform<i8:f32, 0.054138767716535433>> loc(#loc375)
        %356 = "tpu.Concat"(%352, %355) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 12800, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 224, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20x!quant.uniform<i8:f32, 0.054138767716535433>>, tensor<1x256x20x20x!quant.uniform<i8:f32, 0.054138767716535433>>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.054138767716535433>> loc(#loc376)
        %357 = "tpu.Load"(%116) {do_bcast = true, ginfo = #tpu.lg<out_addr = 49152, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 225, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x1x1x256xsi8, 4302831616 : i64>) -> tensor<1x1x1x256xsi8> loc(#loc377)
        %358 = "tpu.Conv2D"(%356, %350, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 45568, out_size = 3584, buffer_addr = 65536, buffer_size = 14336, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 226, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.054138767716535433>>, tensor<1x64x1x4608xsi8>, none) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.10984663307086615>> loc(#loc378)
        %359 = "tpu.Load"(%117) {do_bcast = false, ginfo = #tpu.lg<out_addr = 196608, out_size = 768, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [768], id = 227, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x768xsi8, 4302835712 : i64>) -> tensor<1x64x1x768xsi8> loc(#loc379)
        %360 = "tpu.Lut"(%358, %357) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 228, stage = 1, slice_idx = 0, group_type = 0>} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.10984663307086615>>, tensor<1x1x1x256xsi8>) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>> loc(#loc152)
        %361 = "tpu.Store"(%360, %0) {ginfo = #tpu.lg<out_addr = 16384, out_size = 3584, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 229, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>>, none) -> tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>, 4308439040 : i64> loc(#loc152)
        %362 = "tpu.Conv2D"(%307, %359, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 25600, buffer_addr = 45568, buffer_size = 102400, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [255], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 230, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.13672920708661418>>, tensor<1x64x1x768xsi8>, none) -> tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>> loc(#loc153)
        %363 = "tpu.Store"(%362, %0) {ginfo = #tpu.lg<out_addr = 163840, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [255], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 231, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>>, none) -> tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64> loc(#loc153)
        "tpu.Yield"(%336, %361, %363) : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>, 4308029440 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>, 4308439040 : i64>, tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64>) -> () loc(#loc393)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 2, -2, 5, 4, -3, 7, 6, -4, 9, 8, -5, 11, 10, -6, 13, 12, -7, 14, -8, 16, 15, -9, 18, 17, -10, 20, 19, -11, 22, 21, -12, 23, -13, 25, 24, -14, 27, 26, -15, 29, 28, -16, 31, 30, -17, 35, 36, 32, 33, 34, -18, 38, 39, 231, 37, -19, 41, 40, -20, 43, 42, -21, 45, 44, -22, 47, 46, -23, 49, 48, -24, 51, 52, 50, -25, 54, 53, -26, 56, 55, -27, 58, 57, -28, 59, -29, 61, 60, -30, 63, 62, -31, 65, 64, -32, 67, 66, -33, 69, 68, -34, 70, -35, 72, 71, -36, 74, 73, -37, 75, -38, 77, 76, -39, 78, -40, 82, 79, 80, 81, -41, 84, 83, -42, 86, 85, -43, 88, 87, -44, 90, 89, -45, 91, -46, 95, 92, 93, 94, -47, 96, -48, 97, -49, 99, 98, -50, 100, -51, 101, -52, 103, 102, -53, 105, 104, -54, 107, 106, -55, 108, -56, 109, -57, 110, -58, 111, -59, 112, -60, 114, 113, -61, 116, 115, -62, 118, 117, -63, 120, 119, -64, 121, -65, 122, -66, 123, -67, 124, -68, 126, 125, -69, 128, 127, -70, 130, 129, -71, 132, 131, -72, 134, 133, -73, 136, 135, -74, 138, 137, -75, 140, 139, -76, 141, -77, 143, 142, -78, 145, 144, -79, 147, 146, -80, 148, -81, 149, -82, 150, -83, 151, -84, 153, 152, -85, 155, 154, -86, 157, 156, -87, 159, 158, -88, 161, 160, -89, 163, 162, -90, 165, 164, -91, 167, 166, -92, 169, 168, -93, 170, -94, 172, 171, -95, 175, 173, 174, -96, 178, 176, 177, -97, 180, 179, -98, 181, -99, 183, 182, -100, 185, 184, -101, 187, 186, -102, 189, 188, -103, 191, 190, 0, -104, 193, 192, -105, 195, 194, -106, 197, 196, -107, 198, -108, 200, 199, -109, 201, -110, 205, 202, 203, 204, -111, 206, -112, 207, -113, 208, -114, 210, 209, -115, 212, 211, -116, 214, 213, -117, 215, -118, 219, 216, 217, 218, -119, 220, -120, 222, 221, -121, 223, -122, 224, -123, 226, 225, -124, 228, 227, -125, 230, 229, 1], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [1], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x80x80x!quant.uniform<i8:f32, 0.052292868503937009>, 4310278144 : i64>) -> (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>, 4308029440 : i64>, tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>, 4308439040 : i64>, tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64>) loc(#loc393)
      %119 = "tpu.Reshape"(%118#2) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 80, 80]} : (tensor<1x255x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64>) -> tensor<1x3x85x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64> loc(#loc380)
      %120 = "tpu.Permute"(%119, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x80x80x!quant.uniform<i8:f32, 0.14791108661417321>, 4308643840 : i64>, none) -> tensor<1x3x80x80x85x!quant.uniform<i8:f32, 0.14791108661417321>, 4315172864 : i64> loc(#loc381)
      %121 = "tpu.Cast"(%120) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x80x80x85x!quant.uniform<i8:f32, 0.14791108661417321>, 4315172864 : i64>) -> tensor<1x3x80x80x85xf32, 4308643840 : i64> loc(#loc382)
      %122 = "top.Weight"() : () -> tensor<1x64x1x1280xsi8, 4302884864 : i64> loc(#loc383)
      %123 = "tpu.Conv2D"(%118#0, %122, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40x!quant.uniform<i8:f32, 0.13626622677165354>, 4308029440 : i64>, tensor<1x64x1x1280xsi8, 4302884864 : i64>, none) -> tensor<1x255x40x40x!quant.uniform<i8:f32, 0.12152110866141733>, 4315172864 : i64> loc(#loc384)
      %124 = "tpu.Reshape"(%123) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 40, 40]} : (tensor<1x255x40x40x!quant.uniform<i8:f32, 0.12152110866141733>, 4315172864 : i64>) -> tensor<1x3x85x40x40x!quant.uniform<i8:f32, 0.12152110866141733>, 4315172864 : i64> loc(#loc385)
      %125 = "tpu.Permute"(%124, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x40x40x!quant.uniform<i8:f32, 0.12152110866141733>, 4315172864 : i64>, none) -> tensor<1x3x40x40x85x!quant.uniform<i8:f32, 0.12152110866141733>, 4308029440 : i64> loc(#loc386)
      %126 = "tpu.Cast"(%125) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x40x40x85x!quant.uniform<i8:f32, 0.12152110866141733>, 4308029440 : i64>) -> tensor<1x3x40x40x85xf32, 4315172864 : i64> loc(#loc387)
      %127 = "top.Weight"() : () -> tensor<1x64x1x2304xsi8, 4302966784 : i64> loc(#loc388)
      %128 = "tpu.Conv2D"(%118#1, %127, %0) {coeff_merged = true, dilations = [1, 1], do_relu = false, do_winograd = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20x!quant.uniform<i8:f32, 0.087267577952755915>, 4308439040 : i64>, tensor<1x64x1x2304xsi8, 4302966784 : i64>, none) -> tensor<1x255x20x20x!quant.uniform<i8:f32, 0.11706777244094489>, 4308029440 : i64> loc(#loc389)
      %129 = "tpu.Reshape"(%128) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 20, 20]} : (tensor<1x255x20x20x!quant.uniform<i8:f32, 0.11706777244094489>, 4308029440 : i64>) -> tensor<1x3x85x20x20x!quant.uniform<i8:f32, 0.11706777244094489>, 4308029440 : i64> loc(#loc390)
      %130 = "tpu.Permute"(%129, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x20x20x!quant.uniform<i8:f32, 0.11706777244094489>, 4308029440 : i64>, none) -> tensor<1x3x20x20x85x!quant.uniform<i8:f32, 0.11706777244094489>, 4308131840 : i64> loc(#loc391)
      %131 = "tpu.Cast"(%130) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x20x20x85x!quant.uniform<i8:f32, 0.11706777244094489>, 4308131840 : i64>) -> tensor<1x3x20x20x85xf32, 4308234240 : i64> loc(#loc392)
      return %121, %126, %131 : tensor<1x3x80x80x85xf32, 4308643840 : i64>, tensor<1x3x40x40x85xf32, 4315172864 : i64>, tensor<1x3x20x20x85xf32, 4308234240 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("images122_Conv_ui8")
#loc3 = loc("122_Conv_merge")
#loc4 = loc("124_Mul_table")
#loc5 = loc("125_Conv_merge")
#loc6 = loc("127_Mul_table")
#loc7 = loc("128_Conv_merge")
#loc8 = loc("130_Mul_table")
#loc9 = loc("131_Conv_merge")
#loc10 = loc("133_Mul_table")
#loc11 = loc("134_Conv_merge")
#loc12 = loc("136_Mul_table")
#loc13 = loc("138_Conv_merge")
#loc14 = loc("140_Mul_table")
#loc15 = loc("142_Conv_merge")
#loc16 = loc("144_Mul_table")
#loc17 = loc("145_Conv_merge")
#loc18 = loc("145_Conv")
#loc19 = loc("load_images122_Conv_ui8")
#loc20 = loc("load_122_Conv_merge")
#loc21 = loc("load_124_Mul_table")
#loc22 = loc("122_Conv")
#loc23 = loc("load_125_Conv_merge")
#loc24 = loc("124_Mul")
#loc25 = loc("load_127_Mul_table")
#loc26 = loc("125_Conv")
#loc27 = loc("load_128_Conv_merge")
#loc28 = loc("127_Mul")
#loc29 = loc("load_130_Mul_table")
#loc30 = loc("128_Conv")
#loc31 = loc("load_131_Conv_merge")
#loc32 = loc("130_Mul")
#loc33 = loc("load_133_Mul_table")
#loc34 = loc("131_Conv")
#loc35 = loc("load_134_Conv_merge")
#loc36 = loc("133_Mul")
#loc37 = loc("load_136_Mul_table")
#loc38 = loc("134_Conv")
#loc39 = loc("136_Mul")
#loc40 = loc("load_138_Conv_merge")
#loc41 = loc("137_Add")
#loc42 = loc("load_140_Mul_table")
#loc43 = loc("138_Conv")
#loc44 = loc("load_142_Conv_merge")
#loc45 = loc("140_Mul")
#loc46 = loc("141_Concat")
#loc47 = loc("load_144_Mul_table")
#loc48 = loc("142_Conv")
#loc49 = loc("load_145_Conv_merge")
#loc50 = loc("144_Mul")
#loc51 = loc("147_Mul_table")
#loc52 = loc("148_Conv_merge")
#loc53 = loc("150_Mul_table")
#loc54 = loc("151_Conv_merge")
#loc55 = loc("153_Mul_table")
#loc56 = loc("154_Conv_merge")
#loc57 = loc("156_Mul_table")
#loc58 = loc("158_Conv_merge")
#loc59 = loc("160_Mul_table")
#loc60 = loc("161_Conv_merge")
#loc61 = loc("163_Mul_table")
#loc62 = loc("165_Conv_merge")
#loc63 = loc("167_Mul_table")
#loc64 = loc("169_Conv_merge")
#loc65 = loc("171_Mul_table")
#loc66 = loc("172_Conv_merge")
#loc67 = loc("174_Mul_table")
#loc68 = loc("175_Conv_merge")
#loc69 = loc("177_Mul_table")
#loc70 = loc("178_Conv_merge")
#loc71 = loc("180_Mul_table")
#loc72 = loc("181_Conv_merge")
#loc73 = loc("183_Mul_table")
#loc74 = loc("185_Conv_merge")
#loc75 = loc("187_Mul_table")
#loc76 = loc("188_Conv_merge")
#loc77 = loc("190_Mul_table")
#loc78 = loc("192_Conv_merge")
#loc79 = loc("194_Mul_table")
#loc80 = loc("195_Conv_merge")
#loc81 = loc("197_Mul_table")
#loc82 = loc("199_Conv_merge")
#loc83 = loc("201_Mul_table")
#loc84 = loc("203_Conv_merge")
#loc85 = loc("205_Mul_table")
#loc86 = loc("206_Conv_merge")
#loc87 = loc("208_Mul_table")
#loc88 = loc("209_Conv_merge")
#loc89 = loc("211_Mul_table")
#loc90 = loc("212_Conv_merge")
#loc91 = loc("214_Mul_table")
#loc92 = loc("215_Conv_merge")
#loc93 = loc("217_Mul_table")
#loc94 = loc("219_Conv_merge")
#loc95 = loc("221_Mul_table")
#loc96 = loc("223_Conv_merge")
#loc97 = loc("225_Mul_table")
#loc98 = loc("226_Conv_merge")
#loc99 = loc("228_Mul_table")
#loc100 = loc("233_Conv_merge")
#loc101 = loc("235_Mul_table")
#loc102 = loc("236_Conv_merge")
#loc103 = loc("238_Mul_table")
#loc104 = loc("245_Conv_merge")
#loc105 = loc("247_Mul_table")
#loc106 = loc("248_Conv_merge")
#loc107 = loc("250_Mul_table")
#loc108 = loc("251_Conv_merge")
#loc109 = loc("253_Mul_table")
#loc110 = loc("254_Conv_merge")
#loc111 = loc("256_Mul_table")
#loc112 = loc("258_Conv_merge")
#loc113 = loc("260_Mul_table")
#loc114 = loc("261_Conv_merge")
#loc115 = loc("263_Mul_table")
#loc116 = loc("270_Conv_merge")
#loc117 = loc("272_Mul_table")
#loc118 = loc("273_Conv_merge")
#loc119 = loc("275_Mul_table")
#loc120 = loc("276_Conv_merge")
#loc121 = loc("278_Mul_table")
#loc122 = loc("279_Conv_merge")
#loc123 = loc("281_Mul_table")
#loc124 = loc("283_Conv_merge")
#loc125 = loc("285_Mul_table")
#loc126 = loc("286_Conv_merge")
#loc127 = loc("288_Mul_table")
#loc128 = loc("290_Conv_merge")
#loc129 = loc("292_Mul_table")
#loc130 = loc("293_Conv_merge")
#loc131 = loc("295_Mul_table")
#loc132 = loc("296_Conv_merge")
#loc133 = loc("298_Mul_table")
#loc134 = loc("299_Conv_merge")
#loc135 = loc("301_Mul_table")
#loc136 = loc("303_Conv_merge")
#loc137 = loc("305_Mul_table")
#loc138 = loc("306_Conv_merge")
#loc139 = loc("308_Mul_table")
#loc140 = loc("310_Conv_merge")
#loc141 = loc("312_Mul_table")
#loc142 = loc("313_Conv_merge")
#loc143 = loc("315_Mul_table")
#loc144 = loc("316_Conv_merge")
#loc145 = loc("318_Mul_table")
#loc146 = loc("319_Conv_merge")
#loc147 = loc("321_Mul_table")
#loc148 = loc("323_Conv_merge")
#loc149 = loc("325_Mul_table")
#loc150 = loc("326_Conv_merge")
#loc151 = loc("305_Mul")
#loc152 = loc("325_Mul")
#loc153 = loc("326_Conv")
#loc154 = loc("load_145_Conv")
#loc155 = loc("load_147_Mul_table")
#loc156 = loc("load_148_Conv_merge")
#loc157 = loc("147_Mul")
#loc158 = loc("load_150_Mul_table")
#loc159 = loc("148_Conv")
#loc160 = loc("load_151_Conv_merge")
#loc161 = loc("150_Mul")
#loc162 = loc("load_153_Mul_table")
#loc163 = loc("151_Conv")
#loc164 = loc("load_154_Conv_merge")
#loc165 = loc("153_Mul")
#loc166 = loc("load_156_Mul_table")
#loc167 = loc("154_Conv")
#loc168 = loc("156_Mul")
#loc169 = loc("load_158_Conv_merge")
#loc170 = loc("157_Add")
#loc171 = loc("load_160_Mul_table")
#loc172 = loc("158_Conv")
#loc173 = loc("load_161_Conv_merge")
#loc174 = loc("160_Mul")
#loc175 = loc("load_163_Mul_table")
#loc176 = loc("161_Conv")
#loc177 = loc("163_Mul")
#loc178 = loc("load_165_Conv_merge")
#loc179 = loc("164_Add")
#loc180 = loc("load_167_Mul_table")
#loc181 = loc("165_Conv")
#loc182 = loc("load_169_Conv_merge")
#loc183 = loc("167_Mul")
#loc184 = loc("load_171_Mul_table")
#loc185 = loc("168_Concat")
#loc186 = loc("load_172_Conv_merge")
#loc187 = loc("load_174_Mul_table")
#loc188 = loc("load_206_Conv_merge")
#loc189 = loc("169_Conv")
#loc190 = loc("171_Mul")
#loc191 = loc("load_175_Conv_merge")
#loc192 = loc("172_Conv")
#loc193 = loc("174_Mul")
#loc194 = loc("load_177_Mul_table")
#loc195 = loc("175_Conv")
#loc196 = loc("load_178_Conv_merge")
#loc197 = loc("177_Mul")
#loc198 = loc("load_180_Mul_table")
#loc199 = loc("178_Conv")
#loc200 = loc("load_181_Conv_merge")
#loc201 = loc("180_Mul")
#loc202 = loc("load_183_Mul_table")
#loc203 = loc("181_Conv")
#loc204 = loc("load_185_Conv_merge")
#loc205 = loc("183_Mul")
#loc206 = loc("184_Add")
#loc207 = loc("load_187_Mul_table")
#loc208 = loc("185_Conv")
#loc209 = loc("load_188_Conv_merge")
#loc210 = loc("187_Mul")
#loc211 = loc("load_190_Mul_table")
#loc212 = loc("188_Conv")
#loc213 = loc("190_Mul")
#loc214 = loc("load_192_Conv_merge")
#loc215 = loc("191_Add")
#loc216 = loc("load_194_Mul_table")
#loc217 = loc("192_Conv")
#loc218 = loc("load_195_Conv_merge")
#loc219 = loc("194_Mul")
#loc220 = loc("load_197_Mul_table")
#loc221 = loc("195_Conv")
#loc222 = loc("load_199_Conv_merge")
#loc223 = loc("197_Mul")
#loc224 = loc("198_Add")
#loc225 = loc("load_201_Mul_table")
#loc226 = loc("199_Conv")
#loc227 = loc("load_203_Conv_merge")
#loc228 = loc("201_Mul")
#loc229 = loc("202_Concat")
#loc230 = loc("load_205_Mul_table")
#loc231 = loc("203_Conv")
#loc232 = loc("205_Mul")
#loc233 = loc("load_208_Mul_table")
#loc234 = loc("load_215_Conv_merge")
#loc235 = loc("load_223_Conv_merge")
#loc236 = loc("206_Conv")
#loc237 = loc("load_209_Conv_merge")
#loc238 = loc("208_Mul")
#loc239 = loc("load_211_Mul_table")
#loc240 = loc("209_Conv")
#loc241 = loc("load_212_Conv_merge")
#loc242 = loc("211_Mul")
#loc243 = loc("load_214_Mul_table")
#loc244 = loc("212_Conv")
#loc245 = loc("214_Mul")
#loc246 = loc("load_217_Mul_table")
#loc247 = loc("load_219_Conv_merge")
#loc248 = loc("load_233_Conv_merge")
#loc249 = loc("215_Conv")
#loc250 = loc("217_Mul")
#loc251 = loc("218_Add")
#loc252 = loc("load_221_Mul_table")
#loc253 = loc("219_Conv")
#loc254 = loc("221_Mul")
#loc255 = loc("222_Concat")
#loc256 = loc("load_225_Mul_table")
#loc257 = loc("223_Conv")
#loc258 = loc("load_226_Conv_merge")
#loc259 = loc("225_Mul")
#loc260 = loc("load_228_Mul_table")
#loc261 = loc("226_Conv")
#loc262 = loc("228_Mul")
#loc263 = loc("229_MaxPool")
#loc264 = loc("230_MaxPool")
#loc265 = loc("231_MaxPool")
#loc266 = loc("232_Concat")
#loc267 = loc("load_235_Mul_table")
#loc268 = loc("233_Conv")
#loc269 = loc("load_236_Conv_merge")
#loc270 = loc("235_Mul")
#loc271 = loc("load_238_Mul_table")
#loc272 = loc("236_Conv")
#loc273 = loc("load_245_Conv_merge")
#loc274 = loc("238_Mul")
#loc275 = loc("238_Mul_to_309_Concat")
#loc276 = loc("243_Resize")
#loc277 = loc("243_Resize_to_244_Concat")
#loc278 = loc("244_Concat")
#loc279 = loc("load_247_Mul_table")
#loc280 = loc("245_Conv")
#loc281 = loc("load_248_Conv_merge")
#loc282 = loc("247_Mul")
#loc283 = loc("load_250_Mul_table")
#loc284 = loc("248_Conv")
#loc285 = loc("load_251_Conv_merge")
#loc286 = loc("250_Mul")
#loc287 = loc("load_253_Mul_table")
#loc288 = loc("251_Conv")
#loc289 = loc("load_254_Conv_merge")
#loc290 = loc("253_Mul")
#loc291 = loc("load_256_Mul_table")
#loc292 = loc("254_Conv")
#loc293 = loc("load_258_Conv_merge")
#loc294 = loc("256_Mul")
#loc295 = loc("257_Concat")
#loc296 = loc("load_260_Mul_table")
#loc297 = loc("258_Conv")
#loc298 = loc("load_261_Conv_merge")
#loc299 = loc("260_Mul")
#loc300 = loc("load_263_Mul_table")
#loc301 = loc("261_Conv")
#loc302 = loc("263_Mul")
#loc303 = loc("263_Mul_to_289_Concat")
#loc304 = loc("268_Resize")
#loc305 = loc("268_Resize_to_269_Concat")
#loc306 = loc("load_270_Conv_merge")
#loc307 = loc("269_Concat")
#loc308 = loc("load_272_Mul_table")
#loc309 = loc("270_Conv")
#loc310 = loc("load_273_Conv_merge")
#loc311 = loc("272_Mul")
#loc312 = loc("load_275_Mul_table")
#loc313 = loc("273_Conv")
#loc314 = loc("load_276_Conv_merge")
#loc315 = loc("275_Mul")
#loc316 = loc("load_278_Mul_table")
#loc317 = loc("276_Conv")
#loc318 = loc("load_279_Conv_merge")
#loc319 = loc("278_Mul")
#loc320 = loc("load_281_Mul_table")
#loc321 = loc("279_Conv")
#loc322 = loc("load_283_Conv_merge")
#loc323 = loc("281_Mul")
#loc324 = loc("282_Concat")
#loc325 = loc("load_285_Mul_table")
#loc326 = loc("283_Conv")
#loc327 = loc("load_286_Conv_merge")
#loc328 = loc("load_306_Conv_merge")
#loc329 = loc("285_Mul")
#loc330 = loc("load_288_Mul_table")
#loc331 = loc("load_316_Conv_merge")
#loc332 = loc("286_Conv")
#loc333 = loc("load_290_Conv_merge")
#loc334 = loc("288_Mul")
#loc335 = loc("289_Concat")
#loc336 = loc("load_292_Mul_table")
#loc337 = loc("290_Conv")
#loc338 = loc("load_293_Conv_merge")
#loc339 = loc("292_Mul")
#loc340 = loc("load_295_Mul_table")
#loc341 = loc("293_Conv")
#loc342 = loc("load_296_Conv_merge")
#loc343 = loc("295_Mul")
#loc344 = loc("load_298_Mul_table")
#loc345 = loc("296_Conv")
#loc346 = loc("load_299_Conv_merge")
#loc347 = loc("298_Mul")
#loc348 = loc("load_301_Mul_table")
#loc349 = loc("299_Conv")
#loc350 = loc("load_303_Conv_merge")
#loc351 = loc("301_Mul")
#loc352 = loc("302_Concat")
#loc353 = loc("load_305_Mul_table")
#loc354 = loc("303_Conv")
#loc355 = loc("load_308_Mul_table")
#loc356 = loc("load_310_Conv_merge")
#loc357 = loc("306_Conv")
#loc358 = loc("308_Mul")
#loc359 = loc("308_Mul_to_309_Concat")
#loc360 = loc("309_Concat")
#loc361 = loc("load_312_Mul_table")
#loc362 = loc("310_Conv")
#loc363 = loc("load_313_Conv_merge")
#loc364 = loc("312_Mul")
#loc365 = loc("load_315_Mul_table")
#loc366 = loc("313_Conv")
#loc367 = loc("315_Mul")
#loc368 = loc("load_318_Mul_table")
#loc369 = loc("load_319_Conv_merge")
#loc370 = loc("load_323_Conv_merge")
#loc371 = loc("316_Conv")
#loc372 = loc("318_Mul")
#loc373 = loc("load_321_Mul_table")
#loc374 = loc("319_Conv")
#loc375 = loc("321_Mul")
#loc376 = loc("322_Concat")
#loc377 = loc("load_325_Mul_table")
#loc378 = loc("323_Conv")
#loc379 = loc("load_326_Conv_merge")
#loc380 = loc("349_Reshape")
#loc381 = loc("350_Transpose")
#loc382 = loc("350_Transpose_f32")
#loc383 = loc("474_Conv_merge")
#loc384 = loc("474_Conv")
#loc385 = loc("497_Reshape")
#loc386 = loc("498_Transpose")
#loc387 = loc("498_Transpose_f32")
#loc388 = loc("622_Conv_merge")
#loc389 = loc("622_Conv")
#loc390 = loc("645_Reshape")
#loc391 = loc("646_Transpose")
#loc392 = loc("646_Transpose_f32")
#loc393 = loc(fused[#loc151, #loc152, #loc153])

