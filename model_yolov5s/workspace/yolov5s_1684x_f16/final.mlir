#loc = loc(unknown)
#loc1 = loc("images")
module @yolov5s attributes {module.FLOPs = 16683511600 : i64, module.addr_mode = "basic", module.asymmetric = false, module.chip = "bm1684x", module.cores = 1 : i64, module.devices = 1 : i64, module.high_precision = false, module.inputs = ["images"], module.mode = "F16", module.outputs = ["350_Transpose_f32", "498_Transpose_f32", "646_Transpose_f32"], module.platform = "ONNX", module.q_group_size = 0 : i64, module.state = "TPU_ADDRESSED", module.top_run_mode = "STATIC", module.weight_file = "yolov5s_tpu_addressed_bm1684x_f16_weight.npz"} {
  module @yolov5s attributes {module.coeff_addr = 4294967296 : i64, module.coeff_size = 14688256 : i64, module.device_id = 0 : i64, module.dynamic_coeff_offset = 14688256 : i64, module.neuron_addr = 4309655552 : i64, module.neuron_size = 15937536 : i64, module.step = 0 : i64} {
    func.func @main(%arg0: tensor<1x3x640x640xf32> loc(unknown)) -> (tensor<1x3x80x80x85xf32, 4319064064 : i64>, tensor<1x3x40x40x85xf32, 4316209152 : i64>, tensor<1x3x20x20x85xf32, 4314980352 : i64>) {
      %0 = "top.Input"(%arg0) {channel_format = "nchw", do_preprocess = true, keep_aspect_ratio = true, keep_ratio_mode = "letterbox", mean = [0.000000e+00, 0.000000e+00, 0.000000e+00], pad_type = "center", pad_value = 0 : i64, pixel_format = "rgb", resize_dims = [640, 640], scale = [0.0039216000586748123, 0.0039216000586748123, 0.0039216000586748123]} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640xf32, 4309655552 : i64> loc(#loc1)
      %1:3 = call @subfunc_0(%0) : (tensor<1x3x640x640xf32, 4309655552 : i64>) -> (tensor<1x3x80x80x85xf32, 4319064064 : i64>, tensor<1x3x40x40x85xf32, 4316209152 : i64>, tensor<1x3x20x20x85xf32, 4314980352 : i64>) loc(#loc)
      return %1#0, %1#1, %1#2 : tensor<1x3x80x80x85xf32, 4319064064 : i64>, tensor<1x3x40x40x85xf32, 4316209152 : i64>, tensor<1x3x20x20x85xf32, 4314980352 : i64> loc(#loc)
    } loc(#loc)
    func.func @subfunc_0(%arg0: tensor<1x3x640x640xf32, 4309655552 : i64> loc("images")) -> (tensor<1x3x80x80x85xf32, 4319064064 : i64>, tensor<1x3x40x40x85xf32, 4316209152 : i64>, tensor<1x3x20x20x85xf32, 4314980352 : i64>) attributes {id = 0 : i64, mode = #tpu<run_mode TPU_STATIC>, next_index = array<i32: -1>} {
      %0 = "top.None"() : () -> none loc(#loc)
      %1 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 4294967296 : i64> loc(#loc2)
      %2 = "top.Weight"() : () -> tensor<1x32x1x192xf16, 4294971392 : i64> loc(#loc3)
      %3 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4294983680 : i64> loc(#loc4)
      %4 = "top.Weight"() : () -> tensor<1x64x1x288xf16, 4294987776 : i64> loc(#loc5)
      %5 = "tpu.Group"(%arg0) ({
        %136 = "tpu.Load"(%arg0) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 128000, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 40, 84, 128, 172, 216, 260, 304, 348, 392, 436, 476, 516, 556, 596], h_slice = [46, 50, 50, 50, 50, 50, 50, 50, 50, 50, 46, 46, 46, 46, 44], w_idx = [0], w_slice = [640], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x3x640x640xf32, 4309655552 : i64>) -> tensor<1x3x640x640xf32> loc(#loc7)
        %137 = "tpu.Load"(%2) {do_bcast = false, ginfo = #tpu.lg<out_addr = 259136, out_size = 384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [192], id = 1, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x192xf16, 4294971392 : i64>) -> tensor<1x32x1x192xf16> loc(#loc8)
        %138 = "tpu.Load"(%1) {do_bcast = false, ginfo = #tpu.lg<out_addr = 195712, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 4294967296 : i64>) -> tensor<1x32x1x1xf32> loc(#loc9)
        %139 = "tpu.Cast"(%136) {ginfo = #tpu.lg<out_addr = 131072, out_size = 64000, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [3], d_idx = [0], d_slice = [1], h_idx = [0, 40, 84, 128, 172, 216, 260, 304, 348, 392, 436, 476, 516, 556, 596], h_slice = [46, 50, 50, 50, 50, 50, 50, 50, 50, 50, 46, 46, 46, 46, 44], w_idx = [0], w_slice = [640], id = 3, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x640x640xf32>) -> tensor<1x3x640x640xf16> loc(#loc10)
        %140 = "tpu.Conv2D"(%139, %137, %138) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 196608, out_size = 14720, buffer_addr = 212992, buffer_size = 29580, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 21, 43, 65, 87, 109, 131, 153, 175, 197, 219, 239, 259, 279, 299], h_slice = [22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21], w_idx = [0], w_slice = [320], id = 4, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [6, 6], kernel_zp = 0 : i64, pads = [2, 2, 2, 2], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 17 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x3x640x640xf16>, tensor<1x32x1x192xf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x320x320xf16> loc(#loc11)
        %141 = "tpu.Cast"(%140) {ginfo = #tpu.lg<out_addr = 128000, out_size = 29440, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 21, 43, 65, 87, 109, 131, 153, 175, 197, 219, 239, 259, 279, 299], h_slice = [22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21], w_idx = [0], w_slice = [320], id = 5, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x320x320xf16>) -> tensor<1x32x320x320xf32> loc(#loc12)
        %142 = "tpu.Load"(%3) {do_bcast = false, ginfo = #tpu.lg<out_addr = 195072, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 6, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4294983680 : i64>) -> tensor<1x64x1x1xf32> loc(#loc13)
        %143 = "tpu.Active"(%141) {ginfo = #tpu.lg<out_addr = 163840, out_size = 29440, buffer_addr = 196608, buffer_size = 59008, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 21, 43, 65, 87, 109, 131, 153, 175, 197, 219, 239, 259, 279, 299], h_slice = [22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21], w_idx = [0], w_slice = [320], id = 7, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x32x320x320xf32>) -> tensor<1x32x320x320xf32> loc(#loc14)
        %144 = "tpu.Load"(%4) {do_bcast = false, ginfo = #tpu.lg<out_addr = 195136, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [288], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x288xf16, 4294987776 : i64>) -> tensor<1x64x1x288xf16> loc(#loc15)
        %145 = "tpu.Cast"(%143) {ginfo = #tpu.lg<out_addr = 128000, out_size = 14720, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 21, 43, 65, 87, 109, 131, 153, 175, 197, 219, 239, 259, 279, 299], h_slice = [22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 21, 21, 21, 21, 21], w_idx = [0], w_slice = [320], id = 9, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x320x320xf32>) -> tensor<1x32x320x320xf16> loc(#loc16)
        %146 = "tpu.Conv2D"(%145, %144, %142) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 255616, out_size = 3520, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 120, 130, 140, 150], h_slice = [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [160], id = 10, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x320x320xf16>, tensor<1x64x1x288xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x160x160xf16> loc(#loc6)
        %147 = "tpu.Store"(%146, %0) {ginfo = #tpu.lg<out_addr = 255616, out_size = 3520, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 11, 22, 33, 44, 55, 66, 77, 88, 99, 110, 120, 130, 140, 150], h_slice = [11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10], w_idx = [0], w_slice = [160], id = 11, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x64x160x160xf16>, none) -> tensor<1x64x160x160xf16, 4314570752 : i64> loc(#loc6)
        "tpu.Yield"(%147) : (tensor<1x64x160x160xf16, 4314570752 : i64>) -> () loc(#loc6)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 3, 1, 2, -2, 4, 0, -3, 5, -4, 7, 11, 6, -5, 9, 8, -6, 10], group_type = 0 : i64, hsecs = 15 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [-6, 0], run_core_id = [], self_down_overlap_op = [11], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x3x640x640xf32, 4309655552 : i64>) -> tensor<1x64x160x160xf16, 4314570752 : i64> loc(#loc6)
      %6 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 4295024640 : i64> loc(#loc17)
      %7 = "top.Weight"() : () -> tensor<1x32x1x64xf16, 4295028736 : i64> loc(#loc18)
      %8 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 4295032832 : i64> loc(#loc19)
      %9 = "top.Weight"() : () -> tensor<1x32x1x32xf16, 4295036928 : i64> loc(#loc20)
      %10 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 4295041024 : i64> loc(#loc21)
      %11 = "top.Weight"() : () -> tensor<1x32x1x288xf16, 4295045120 : i64> loc(#loc22)
      %12 = "top.Weight"() : () -> tensor<1x32x1x1xf32, 4295065600 : i64> loc(#loc23)
      %13 = "top.Weight"() : () -> tensor<1x32x1x64xf16, 4295069696 : i64> loc(#loc24)
      %14 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295073792 : i64> loc(#loc25)
      %15 = "top.Weight"() : () -> tensor<1x64x1x64xf16, 4295077888 : i64> loc(#loc26)
      %16 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4295086080 : i64> loc(#loc27)
      %17 = "top.Weight"() : () -> tensor<1x128x1x576xf16, 4295090176 : i64> loc(#loc28)
      %18 = "tpu.Group"(%5) ({
        %136 = "tpu.Load"(%5) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x160x160xf16, 4314570752 : i64>) -> tensor<1x64x160x160xf16> loc(#loc30)
        %137 = "tpu.Cast"(%136) {ginfo = #tpu.lg<out_addr = 0, out_size = 36480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 1, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf16>) -> tensor<1x64x160x160xf32> loc(#loc31)
        %138 = "tpu.Load"(%6) {do_bcast = false, ginfo = #tpu.lg<out_addr = 215296, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 2, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 4295024640 : i64>) -> tensor<1x32x1x1xf32> loc(#loc32)
        %139 = "tpu.Active"(%137) {ginfo = #tpu.lg<out_addr = 49152, out_size = 36480, buffer_addr = 114688, buffer_size = 73088, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 3, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf32> loc(#loc33)
        %140 = "tpu.Load"(%7) {do_bcast = false, ginfo = #tpu.lg<out_addr = 204736, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x64xf16, 4295028736 : i64>) -> tensor<1x32x1x64xf16> loc(#loc34)
        %141 = "tpu.Cast"(%139) {ginfo = #tpu.lg<out_addr = 0, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 5, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf16> loc(#loc35)
        %142 = "tpu.Conv2D"(%141, %140, %138) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<1x32x1x64xf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x160x160xf16> loc(#loc36)
        %143 = "tpu.Cast"(%142) {ginfo = #tpu.lg<out_addr = 18240, out_size = 36480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 7, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc37)
        %144 = "tpu.Load"(%8) {do_bcast = false, ginfo = #tpu.lg<out_addr = 215360, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 4295032832 : i64>) -> tensor<1x32x1x1xf32> loc(#loc38)
        %145 = "tpu.Active"(%143) {ginfo = #tpu.lg<out_addr = 65536, out_size = 36480, buffer_addr = 114688, buffer_size = 73088, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 9, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc39)
        %146 = "tpu.Load"(%9) {do_bcast = false, ginfo = #tpu.lg<out_addr = 205120, out_size = 64, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [32], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x32xf16, 4295036928 : i64>) -> tensor<1x32x1x32xf16> loc(#loc40)
        %147 = "tpu.Cast"(%145) {ginfo = #tpu.lg<out_addr = 18240, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 11, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc41)
        %148 = "tpu.Conv2D"(%147, %146, %144) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 12, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160xf16>, tensor<1x32x1x32xf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x160x160xf16> loc(#loc42)
        %149 = "tpu.Cast"(%148) {ginfo = #tpu.lg<out_addr = 36480, out_size = 36480, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 13, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc43)
        %150 = "tpu.Active"(%149) {ginfo = #tpu.lg<out_addr = 81920, out_size = 36480, buffer_addr = 131072, buffer_size = 73088, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 14, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc44)
        %151 = "tpu.Load"(%11) {do_bcast = false, ginfo = #tpu.lg<out_addr = 204160, out_size = 576, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [288], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x288xf16, 4295045120 : i64>) -> tensor<1x32x1x288xf16> loc(#loc45)
        %152 = "tpu.Load"(%10) {do_bcast = false, ginfo = #tpu.lg<out_addr = 215424, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 16, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 4295041024 : i64>) -> tensor<1x32x1x1xf32> loc(#loc46)
        %153 = "tpu.Cast"(%150) {ginfo = #tpu.lg<out_addr = 36480, out_size = 18240, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 52, 106], h_slice = [55, 57, 54], w_idx = [0], w_slice = [160], id = 17, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc47)
        %154 = "tpu.Conv2D"(%153, %151, %152) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x32x160x160xf16>, tensor<1x32x1x288xf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x160x160xf16> loc(#loc48)
        %155 = "tpu.Cast"(%154) {ginfo = #tpu.lg<out_addr = 36480, out_size = 35200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 19, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc49)
        %156 = "tpu.Load"(%12) {do_bcast = false, ginfo = #tpu.lg<out_addr = 215488, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 20, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x1xf32, 4295065600 : i64>) -> tensor<1x32x1x1xf32> loc(#loc50)
        %157 = "tpu.Active"(%155) {ginfo = #tpu.lg<out_addr = 81920, out_size = 35200, buffer_addr = 131072, buffer_size = 70528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 21, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc51)
        %158 = "tpu.Cast"(%157) {ginfo = #tpu.lg<out_addr = 49152, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 22, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc52)
        %159 = "tpu.Load"(%13) {do_bcast = false, ginfo = #tpu.lg<out_addr = 204864, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x32x1x64xf16, 4295069696 : i64>) -> tensor<1x32x1x64xf16> loc(#loc53)
        %160 = "tpu.Add"(%147, %158) {do_relu = false, ginfo = #tpu.lg<out_addr = 84352, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 24, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x160x160xf16>, tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf16> loc(#loc54)
        %161 = "tpu.Conv2D"(%141, %159, %156) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 25, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<1x32x1x64xf16>, tensor<1x32x1x1xf32>) -> tensor<1x32x160x160xf16> loc(#loc55)
        %162 = "tpu.Cast"(%161) {ginfo = #tpu.lg<out_addr = 0, out_size = 35200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 26, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf16>) -> tensor<1x32x160x160xf32> loc(#loc56)
        %163 = "tpu.Load"(%14) {do_bcast = false, ginfo = #tpu.lg<out_addr = 215552, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 27, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295073792 : i64>) -> tensor<1x64x1x1xf32> loc(#loc57)
        %164 = "tpu.Load"(%15) {do_bcast = false, ginfo = #tpu.lg<out_addr = 204992, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xf16, 4295077888 : i64>) -> tensor<1x64x1x64xf16> loc(#loc58)
        %165 = "tpu.Active"(%162) {ginfo = #tpu.lg<out_addr = 49152, out_size = 35200, buffer_addr = 101952, buffer_size = 70528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 29, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf32> loc(#loc59)
        %166 = "tpu.Cast"(%165) {ginfo = #tpu.lg<out_addr = 0, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [32], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 30, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x32x160x160xf32>) -> tensor<1x32x160x160xf16> loc(#loc60)
        %167 = "tpu.Concat"(%160, %166) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 31, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x32x160x160xf16>, tensor<1x32x160x160xf16>) -> tensor<1x64x160x160xf16> loc(#loc61)
        %168 = "tpu.Conv2D"(%167, %164, %163) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 32, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<1x64x1x64xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x160x160xf16> loc(#loc62)
        %169 = "tpu.Cast"(%168) {ginfo = #tpu.lg<out_addr = 0, out_size = 35200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 33, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf16>) -> tensor<1x64x160x160xf32> loc(#loc63)
        %170 = "tpu.Load"(%17) {do_bcast = false, ginfo = #tpu.lg<out_addr = 212992, out_size = 2304, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [576], id = 34, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x576xf16, 4295090176 : i64>) -> tensor<1x128x1x576xf16> loc(#loc64)
        %171 = "tpu.Load"(%16) {do_bcast = false, ginfo = #tpu.lg<out_addr = 229376, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 35, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4295086080 : i64>) -> tensor<1x128x1x1xf32> loc(#loc65)
        %172 = "tpu.Active"(%169) {ginfo = #tpu.lg<out_addr = 81920, out_size = 35200, buffer_addr = 131072, buffer_size = 70528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 36, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf32> loc(#loc66)
        %173 = "tpu.Cast"(%172) {ginfo = #tpu.lg<out_addr = 0, out_size = 17600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0, 53, 107], h_slice = [54, 55, 53], w_idx = [0], w_slice = [160], id = 37, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x160x160xf32>) -> tensor<1x64x160x160xf16> loc(#loc67)
        %174 = "tpu.Conv2D"(%173, %170, %171) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 8704, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 27, 54], h_slice = [27, 27, 26], w_idx = [0], w_slice = [80], id = 38, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x160x160xf16>, tensor<1x128x1x576xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x80x80xf16> loc(#loc29)
        %175 = "tpu.Store"(%174, %0) {ginfo = #tpu.lg<out_addr = 98304, out_size = 8704, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0, 27, 54], h_slice = [27, 27, 26], w_idx = [0], w_slice = [80], id = 39, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x80x80xf16>, none) -> tensor<1x128x80x80xf16, 4319473664 : i64> loc(#loc29)
        "tpu.Yield"(%175) : (tensor<1x128x80x80xf16, 4319473664 : i64>) -> () loc(#loc29)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 1, -2, 3, 39, 2, -3, 5, 4, -4, 6, -5, 7, -6, 9, 8, -7, 11, 10, -8, 12, -9, 13, -10, 14, -11, 17, 15, 16, -12, 18, -13, 19, -14, 21, 20, -15, 22, -16, 24, 23, -17, 25, -18, 26, -19, 29, 27, 28, -20, 30, -21, 31, -22, 32, -23, 33, -24, 36, 34, 35, 0, -25, 37, -26, 38], group_type = 0 : i64, hsecs = 3 : i64, nsecs = 1 : i64, other_down_overlap_op = [-19, 11], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [0], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x64x160x160xf16, 4314570752 : i64>) -> tensor<1x128x80x80xf16, 4319473664 : i64> loc(#loc29)
      %19 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295237632 : i64> loc(#loc68)
      %20 = "top.Weight"() : () -> tensor<1x64x1x128xf16, 4295241728 : i64> loc(#loc69)
      %21 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295258112 : i64> loc(#loc70)
      %22 = "top.Weight"() : () -> tensor<1x64x1x64xf16, 4295262208 : i64> loc(#loc71)
      %23 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295270400 : i64> loc(#loc72)
      %24 = "top.Weight"() : () -> tensor<1x64x1x576xf16, 4295274496 : i64> loc(#loc73)
      %25 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295348224 : i64> loc(#loc74)
      %26 = "top.Weight"() : () -> tensor<1x64x1x64xf16, 4295352320 : i64> loc(#loc75)
      %27 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295360512 : i64> loc(#loc76)
      %28 = "top.Weight"() : () -> tensor<1x64x1x576xf16, 4295364608 : i64> loc(#loc77)
      %29 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4295438336 : i64> loc(#loc78)
      %30 = "top.Weight"() : () -> tensor<1x64x1x128xf16, 4295442432 : i64> loc(#loc79)
      %31 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4295458816 : i64> loc(#loc80)
      %32 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4295462912 : i64> loc(#loc81)
      %33 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4295495680 : i64> loc(#loc82)
      %34 = "top.Weight"() : () -> tensor<1x256x1x1152xf16, 4295499776 : i64> loc(#loc83)
      %35 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296089600 : i64> loc(#loc84)
      %36 = "top.Weight"() : () -> tensor<1x128x1x256xf16, 4296093696 : i64> loc(#loc85)
      %37 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296159232 : i64> loc(#loc86)
      %38 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4296163328 : i64> loc(#loc87)
      %39 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296196096 : i64> loc(#loc88)
      %40 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4296200192 : i64> loc(#loc89)
      %41 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296495104 : i64> loc(#loc90)
      %42 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4296499200 : i64> loc(#loc91)
      %43 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296531968 : i64> loc(#loc92)
      %44 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4296536064 : i64> loc(#loc93)
      %45 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296830976 : i64> loc(#loc94)
      %46 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4296835072 : i64> loc(#loc95)
      %47 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4296867840 : i64> loc(#loc96)
      %48 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4296871936 : i64> loc(#loc97)
      %49 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4297166848 : i64> loc(#loc98)
      %50 = "top.Weight"() : () -> tensor<1x128x1x256xf16, 4297170944 : i64> loc(#loc99)
      %51 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4297236480 : i64> loc(#loc100)
      %52 = "top.Weight"() : () -> tensor<1x256x1x256xf16, 4297240576 : i64> loc(#loc101)
      %53 = "top.Weight"() : () -> tensor<1x512x1x1xf32, 4297371648 : i64> loc(#loc102)
      %54 = "top.Weight"() : () -> tensor<1x512x1x2304xf16, 4297375744 : i64> loc(#loc103)
      %55 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4299735040 : i64> loc(#loc104)
      %56 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4299739136 : i64> loc(#loc105)
      %57 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4300001280 : i64> loc(#loc106)
      %58 = "top.Weight"() : () -> tensor<1x256x1x256xf16, 4300005376 : i64> loc(#loc107)
      %59 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4300136448 : i64> loc(#loc108)
      %60 = "top.Weight"() : () -> tensor<1x256x1x2304xf16, 4300140544 : i64> loc(#loc109)
      %61 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4301320192 : i64> loc(#loc110)
      %62 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4301324288 : i64> loc(#loc111)
      %63 = "top.Weight"() : () -> tensor<1x512x1x1xf32, 4301586432 : i64> loc(#loc112)
      %64 = "top.Weight"() : () -> tensor<1x512x1x512xf16, 4301590528 : i64> loc(#loc113)
      %65 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4302114816 : i64> loc(#loc114)
      %66 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4302118912 : i64> loc(#loc115)
      %67 = "top.Weight"() : () -> tensor<1x512x1x1xf32, 4302381056 : i64> loc(#loc116)
      %68 = "top.Weight"() : () -> tensor<1x512x1x1024xf16, 4302385152 : i64> loc(#loc117)
      %69 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4303433728 : i64> loc(#loc118)
      %70 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4303437824 : i64> loc(#loc119)
      %71 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4303699968 : i64> loc(#loc120)
      %72 = "top.Weight"() : () -> tensor<1x128x1x512xf16, 4303704064 : i64> loc(#loc121)
      %73 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4303835136 : i64> loc(#loc122)
      %74 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4303839232 : i64> loc(#loc123)
      %75 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4303872000 : i64> loc(#loc124)
      %76 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4303876096 : i64> loc(#loc125)
      %77 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4304171008 : i64> loc(#loc126)
      %78 = "top.Weight"() : () -> tensor<1x128x1x512xf16, 4304175104 : i64> loc(#loc127)
      %79 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4304306176 : i64> loc(#loc128)
      %80 = "top.Weight"() : () -> tensor<1x256x1x256xf16, 4304310272 : i64> loc(#loc129)
      %81 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4304441344 : i64> loc(#loc130)
      %82 = "top.Weight"() : () -> tensor<1x128x1x256xf16, 4304445440 : i64> loc(#loc131)
      %83 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4304510976 : i64> loc(#loc132)
      %84 = "top.Weight"() : () -> tensor<1x64x1x256xf16, 4304515072 : i64> loc(#loc133)
      %85 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4304547840 : i64> loc(#loc134)
      %86 = "top.Weight"() : () -> tensor<1x64x1x64xf16, 4304551936 : i64> loc(#loc135)
      %87 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4304560128 : i64> loc(#loc136)
      %88 = "top.Weight"() : () -> tensor<1x64x1x576xf16, 4304564224 : i64> loc(#loc137)
      %89 = "top.Weight"() : () -> tensor<1x64x1x1xf32, 4304637952 : i64> loc(#loc138)
      %90 = "top.Weight"() : () -> tensor<1x64x1x256xf16, 4304642048 : i64> loc(#loc139)
      %91 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4304674816 : i64> loc(#loc140)
      %92 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4304678912 : i64> loc(#loc141)
      %93 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4304711680 : i64> loc(#loc142)
      %94 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4304715776 : i64> loc(#loc143)
      %95 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4305010688 : i64> loc(#loc144)
      %96 = "top.Weight"() : () -> tensor<1x128x1x256xf16, 4305014784 : i64> loc(#loc145)
      %97 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4305080320 : i64> loc(#loc146)
      %98 = "top.Weight"() : () -> tensor<1x128x1x128xf16, 4305084416 : i64> loc(#loc147)
      %99 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4305117184 : i64> loc(#loc148)
      %100 = "top.Weight"() : () -> tensor<1x128x1x1152xf16, 4305121280 : i64> loc(#loc149)
      %101 = "top.Weight"() : () -> tensor<1x128x1x1xf32, 4305416192 : i64> loc(#loc150)
      %102 = "top.Weight"() : () -> tensor<1x128x1x256xf16, 4305420288 : i64> loc(#loc151)
      %103 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4305485824 : i64> loc(#loc152)
      %104 = "top.Weight"() : () -> tensor<1x256x1x256xf16, 4305489920 : i64> loc(#loc153)
      %105 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4305620992 : i64> loc(#loc154)
      %106 = "top.Weight"() : () -> tensor<1x256x1x2304xf16, 4305625088 : i64> loc(#loc155)
      %107 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4306804736 : i64> loc(#loc156)
      %108 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4306808832 : i64> loc(#loc157)
      %109 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4307070976 : i64> loc(#loc158)
      %110 = "top.Weight"() : () -> tensor<1x256x1x256xf16, 4307075072 : i64> loc(#loc159)
      %111 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4307206144 : i64> loc(#loc160)
      %112 = "top.Weight"() : () -> tensor<1x256x1x2304xf16, 4307210240 : i64> loc(#loc161)
      %113 = "top.Weight"() : () -> tensor<1x256x1x1xf32, 4308389888 : i64> loc(#loc162)
      %114 = "top.Weight"() : () -> tensor<1x256x1x512xf16, 4308393984 : i64> loc(#loc163)
      %115 = "top.Weight"() : () -> tensor<1x512x1x1xf32, 4308656128 : i64> loc(#loc164)
      %116 = "top.Weight"() : () -> tensor<1x512x1x512xf16, 4308660224 : i64> loc(#loc165)
      %117:3 = "tpu.Group"(%18) ({
        %136 = "tpu.Load"(%18) {do_bcast = false, ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 0, stage = 0, slice_idx = 0, group_type = 0>, lmem_type = 1 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x80x80xf16, 4319473664 : i64>) -> tensor<1x128x80x80xf16> loc(#loc169)
        %137 = "tpu.Cast"(%136) {ginfo = #tpu.lg<out_addr = 114688, out_size = 51200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 1, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc170)
        %138 = "tpu.Active"(%137) {ginfo = #tpu.lg<out_addr = 180224, out_size = 51200, buffer_addr = 0, buffer_size = 102528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 2, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc171)
        %139 = "tpu.Load"(%20) {do_bcast = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 3, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xf16, 4295241728 : i64>) -> tensor<1x64x1x128xf16> loc(#loc172)
        %140 = "tpu.Load"(%19) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 4, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295237632 : i64>) -> tensor<1x64x1x1xf32> loc(#loc173)
        %141 = "tpu.Cast"(%138) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 5, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc174)
        %142 = "tpu.Conv2D"(%141, %139, %140) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 6, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x64x1x128xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc175)
        %143 = "tpu.Cast"(%142) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 7, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc176)
        %144 = "tpu.Load"(%21) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 8, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295258112 : i64>) -> tensor<1x64x1x1xf32> loc(#loc177)
        %145 = "tpu.Active"(%143) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 98304, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 9, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc178)
        %146 = "tpu.Load"(%22) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 10, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xf16, 4295262208 : i64>) -> tensor<1x64x1x64xf16> loc(#loc179)
        %147 = "tpu.Cast"(%145) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 11, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc180)
        %148 = "tpu.Conv2D"(%147, %146, %144) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 12, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x64xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc181)
        %149 = "tpu.Cast"(%148) {ginfo = #tpu.lg<out_addr = 38400, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 13, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc182)
        %150 = "tpu.Load"(%24) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [576], id = 14, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x576xf16, 4295274496 : i64>) -> tensor<1x64x1x576xf16> loc(#loc183)
        %151 = "tpu.Load"(%23) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 15, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295270400 : i64>) -> tensor<1x64x1x1xf32> loc(#loc184)
        %152 = "tpu.Active"(%149) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 16, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc185)
        %153 = "tpu.Cast"(%152) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 17, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc186)
        %154 = "tpu.Conv2D"(%153, %150, %151) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 18, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x576xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc187)
        %155 = "tpu.Cast"(%154) {ginfo = #tpu.lg<out_addr = 38400, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 19, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc188)
        %156 = "tpu.Load"(%25) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 20, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295348224 : i64>) -> tensor<1x64x1x1xf32> loc(#loc189)
        %157 = "tpu.Active"(%155) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 98304, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 21, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc190)
        %158 = "tpu.Cast"(%157) {ginfo = #tpu.lg<out_addr = 49152, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 22, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc191)
        %159 = "tpu.Load"(%26) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 23, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xf16, 4295352320 : i64>) -> tensor<1x64x1x64xf16> loc(#loc192)
        %160 = "tpu.Add"(%147, %158) {do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 24, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf16> loc(#loc193)
        %161 = "tpu.Conv2D"(%160, %159, %156) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 25, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x64xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc194)
        %162 = "tpu.Cast"(%161) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 26, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc195)
        %163 = "tpu.Load"(%28) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [576], id = 27, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x576xf16, 4295364608 : i64>) -> tensor<1x64x1x576xf16> loc(#loc196)
        %164 = "tpu.Load"(%27) {do_bcast = false, ginfo = #tpu.lg<out_addr = 196608, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 28, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295360512 : i64>) -> tensor<1x64x1x1xf32> loc(#loc197)
        %165 = "tpu.Active"(%162) {ginfo = #tpu.lg<out_addr = 78336, out_size = 25600, buffer_addr = 131072, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 29, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc198)
        %166 = "tpu.Cast"(%165) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 30, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc199)
        %167 = "tpu.Conv2D"(%166, %163, %164) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 78336, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 31, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x576xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc200)
        %168 = "tpu.Cast"(%167) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 32, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc201)
        %169 = "tpu.Load"(%29) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 33, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4295438336 : i64>) -> tensor<1x64x1x1xf32> loc(#loc202)
        %170 = "tpu.Active"(%168) {ginfo = #tpu.lg<out_addr = 78336, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 34, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc203)
        %171 = "tpu.Cast"(%170) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 35, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc204)
        %172 = "tpu.Load"(%30) {do_bcast = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 256, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 36, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x128xf16, 4295442432 : i64>) -> tensor<1x64x1x128xf16> loc(#loc205)
        %173 = "tpu.Add"(%160, %171) {do_relu = false, ginfo = #tpu.lg<out_addr = 116864, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 37, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf16> loc(#loc206)
        %174 = "tpu.Conv2D"(%141, %172, %169) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 38, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x64x1x128xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc207)
        %175 = "tpu.Cast"(%174) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 39, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc208)
        %176 = "tpu.Load"(%31) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 40, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4295458816 : i64>) -> tensor<1x128x1x1xf32> loc(#loc209)
        %177 = "tpu.Load"(%32) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 41, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4295462912 : i64>) -> tensor<1x128x1x128xf16> loc(#loc210)
        %178 = "tpu.Active"(%175) {ginfo = #tpu.lg<out_addr = 32768, out_size = 25600, buffer_addr = 65536, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 42, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc211)
        %179 = "tpu.Cast"(%178) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 43, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc212)
        %180 = "tpu.Concat"(%173, %179) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 44, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x128x80x80xf16> loc(#loc213)
        %181 = "tpu.Conv2D"(%180, %177, %176) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 45, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x80x80xf16> loc(#loc214)
        %182 = "tpu.Cast"(%181) {ginfo = #tpu.lg<out_addr = 0, out_size = 51200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 46, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc215)
        %183 = "tpu.Load"(%34) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 9216, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 47, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1152xf16, 4295499776 : i64>) -> tensor<1x256x1x1152xf16> loc(#loc216)
        %184 = "tpu.Load"(%33) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 48, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4295495680 : i64>) -> tensor<1x256x1x1xf32> loc(#loc217)
        %185 = "tpu.Active"(%182) {ginfo = #tpu.lg<out_addr = 65536, out_size = 51200, buffer_addr = 147520, buffer_size = 102528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 49, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc218)
        %186 = "tpu.Cast"(%185) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 50, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc219)
        %187 = "tpu.Load"(%54) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 36864, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 51, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x2304xf16, 4297375744 : i64>) -> tensor<1x512x1x2304xf16> loc(#loc220)
        %188 = "tpu.Conv2D"(%186, %183, %184) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 52, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x256x1x1152xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x40x40xf16> loc(#loc221)
        %189 = "tpu.Cast"(%188) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 53, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc222)
        %190 = "tpu.Load"(%35) {do_bcast = false, ginfo = #tpu.lg<out_addr = 217088, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 54, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296089600 : i64>) -> tensor<1x128x1x1xf32> loc(#loc223)
        %191 = "tpu.Load"(%36) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 55, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xf16, 4296093696 : i64>) -> tensor<1x128x1x256xf16> loc(#loc224)
        %192 = "tpu.Active"(%189) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 56, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc225)
        %193 = "tpu.Cast"(%192) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 57, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc226)
        %194 = "tpu.Conv2D"(%193, %191, %190) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 58, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x128x1x256xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc227)
        %195 = "tpu.Cast"(%194) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 59, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc228)
        %196 = "tpu.Load"(%37) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 60, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296159232 : i64>) -> tensor<1x128x1x1xf32> loc(#loc229)
        %197 = "tpu.Load"(%38) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 61, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4296163328 : i64>) -> tensor<1x128x1x128xf16> loc(#loc230)
        %198 = "tpu.Active"(%195) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 62, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc231)
        %199 = "tpu.Cast"(%198) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 63, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc232)
        %200 = "tpu.Conv2D"(%199, %197, %196) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 64, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc233)
        %201 = "tpu.Cast"(%200) {ginfo = #tpu.lg<out_addr = 44800, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 65, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc234)
        %202 = "tpu.Load"(%40) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 66, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4296200192 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc235)
        %203 = "tpu.Load"(%39) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 67, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296196096 : i64>) -> tensor<1x128x1x1xf32> loc(#loc236)
        %204 = "tpu.Active"(%201) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 68, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc237)
        %205 = "tpu.Cast"(%204) {ginfo = #tpu.lg<out_addr = 44800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 69, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc238)
        %206 = "tpu.Conv2D"(%205, %202, %203) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 70, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc239)
        %207 = "tpu.Cast"(%206) {ginfo = #tpu.lg<out_addr = 44800, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 71, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc240)
        %208 = "tpu.Load"(%41) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 72, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296495104 : i64>) -> tensor<1x128x1x1xf32> loc(#loc241)
        %209 = "tpu.Load"(%42) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 73, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4296499200 : i64>) -> tensor<1x128x1x128xf16> loc(#loc242)
        %210 = "tpu.Active"(%207) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 81920, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 74, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc243)
        %211 = "tpu.Cast"(%210) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 75, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc244)
        %212 = "tpu.Add"(%199, %211) {do_relu = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 76, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc245)
        %213 = "tpu.Conv2D"(%212, %209, %208) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 77, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc246)
        %214 = "tpu.Cast"(%213) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 78, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc247)
        %215 = "tpu.Load"(%44) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 79, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4296536064 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc248)
        %216 = "tpu.Load"(%43) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 80, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296531968 : i64>) -> tensor<1x128x1x1xf32> loc(#loc249)
        %217 = "tpu.Active"(%214) {ginfo = #tpu.lg<out_addr = 71936, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 81, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc250)
        %218 = "tpu.Cast"(%217) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 82, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc251)
        %219 = "tpu.Conv2D"(%218, %215, %216) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 71936, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 83, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc252)
        %220 = "tpu.Cast"(%219) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 84, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc253)
        %221 = "tpu.Load"(%45) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 85, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296830976 : i64>) -> tensor<1x128x1x1xf32> loc(#loc254)
        %222 = "tpu.Load"(%46) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 86, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4296835072 : i64>) -> tensor<1x128x1x128xf16> loc(#loc255)
        %223 = "tpu.Active"(%220) {ginfo = #tpu.lg<out_addr = 71936, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 87, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc256)
        %224 = "tpu.Cast"(%223) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 88, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc257)
        %225 = "tpu.Add"(%212, %224) {do_relu = false, ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 89, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc258)
        %226 = "tpu.Conv2D"(%225, %222, %221) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 90, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc259)
        %227 = "tpu.Cast"(%226) {ginfo = #tpu.lg<out_addr = 44800, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 91, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc260)
        %228 = "tpu.Load"(%48) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 92, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4296871936 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc261)
        %229 = "tpu.Load"(%47) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 93, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4296867840 : i64>) -> tensor<1x128x1x1xf32> loc(#loc262)
        %230 = "tpu.Active"(%227) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 94, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc263)
        %231 = "tpu.Cast"(%230) {ginfo = #tpu.lg<out_addr = 44800, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 95, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc264)
        %232 = "tpu.Conv2D"(%231, %228, %229) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 96, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc265)
        %233 = "tpu.Cast"(%232) {ginfo = #tpu.lg<out_addr = 44800, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 97, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc266)
        %234 = "tpu.Load"(%49) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 98, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4297166848 : i64>) -> tensor<1x128x1x1xf32> loc(#loc267)
        %235 = "tpu.Load"(%50) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 99, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xf16, 4297170944 : i64>) -> tensor<1x128x1x256xf16> loc(#loc268)
        %236 = "tpu.Active"(%233) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 100, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc269)
        %237 = "tpu.Cast"(%236) {ginfo = #tpu.lg<out_addr = 82944, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 101, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc270)
        %238 = "tpu.Add"(%225, %237) {do_relu = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 102, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf16> loc(#loc271)
        %239 = "tpu.Conv2D"(%193, %235, %234) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 55552, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 103, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x128x1x256xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc272)
        %240 = "tpu.Cast"(%239) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 104, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc273)
        %241 = "tpu.Load"(%51) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 105, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4297236480 : i64>) -> tensor<1x256x1x1xf32> loc(#loc274)
        %242 = "tpu.Load"(%52) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 106, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xf16, 4297240576 : i64>) -> tensor<1x256x1x256xf16> loc(#loc275)
        %243 = "tpu.Active"(%240) {ginfo = #tpu.lg<out_addr = 55552, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 107, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc276)
        %244 = "tpu.Cast"(%243) {ginfo = #tpu.lg<out_addr = 83968, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 108, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc277)
        %245 = "tpu.Concat"(%238, %244) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 109, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc278)
        %246 = "tpu.Conv2D"(%245, %242, %241) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 110, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x256x1x256xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x40x40xf16> loc(#loc279)
        %247 = "tpu.Cast"(%246) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 111, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc280)
        %248 = "tpu.Load"(%53) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 32, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 112, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x1xf32, 4297371648 : i64>) -> tensor<1x512x1x1xf32> loc(#loc281)
        %249 = "tpu.Active"(%247) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 98304, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 113, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc282)
        %250 = "tpu.Cast"(%249) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 114, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc283)
        %251 = "tpu.Load"(%60) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 115, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x2304xf16, 4300140544 : i64>) -> tensor<1x256x1x2304xf16> loc(#loc284)
        %252 = "tpu.Load"(%68) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 16384, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1024], id = 116, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x1024xf16, 4302385152 : i64>) -> tensor<1x512x1x1024xf16> loc(#loc285)
        %253 = "tpu.Conv2D"(%250, %187, %248) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 117, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x512x1x2304xf16>, tensor<1x512x1x1xf32>) -> tensor<1x512x20x20xf16> loc(#loc286)
        %254 = "tpu.Cast"(%253) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 118, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc287)
        %255 = "tpu.Load"(%55) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 119, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4299735040 : i64>) -> tensor<1x256x1x1xf32> loc(#loc288)
        %256 = "tpu.Load"(%56) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 120, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4299739136 : i64>) -> tensor<1x256x1x512xf16> loc(#loc289)
        %257 = "tpu.Active"(%254) {ginfo = #tpu.lg<out_addr = 100352, out_size = 12800, buffer_addr = 131072, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 121, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc290)
        %258 = "tpu.Cast"(%257) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 122, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc291)
        %259 = "tpu.Load"(%57) {do_bcast = false, ginfo = #tpu.lg<out_addr = 51456, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 123, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4300001280 : i64>) -> tensor<1x256x1x1xf32> loc(#loc292)
        %260 = "tpu.Conv2D"(%258, %256, %255) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 100352, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 124, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc293)
        %261 = "tpu.Cast"(%260) {ginfo = #tpu.lg<out_addr = 45056, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 125, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc294)
        %262 = "tpu.Load"(%58) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 126, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xf16, 4300005376 : i64>) -> tensor<1x256x1x256xf16> loc(#loc295)
        %263 = "tpu.Active"(%261) {ginfo = #tpu.lg<out_addr = 100352, out_size = 6400, buffer_addr = 114688, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 127, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc296)
        %264 = "tpu.Cast"(%263) {ginfo = #tpu.lg<out_addr = 45056, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 128, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc297)
        %265 = "tpu.Conv2D"(%264, %262, %259) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 100352, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 129, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<1x256x1x256xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc298)
        %266 = "tpu.Cast"(%265) {ginfo = #tpu.lg<out_addr = 48384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 130, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc299)
        %267 = "tpu.Load"(%59) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 131, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4300136448 : i64>) -> tensor<1x256x1x1xf32> loc(#loc300)
        %268 = "tpu.Active"(%266) {ginfo = #tpu.lg<out_addr = 100352, out_size = 6400, buffer_addr = 114688, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 132, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc301)
        %269 = "tpu.Cast"(%268) {ginfo = #tpu.lg<out_addr = 48384, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 133, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc302)
        %270 = "tpu.Load"(%61) {do_bcast = false, ginfo = #tpu.lg<out_addr = 122880, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 134, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4301320192 : i64>) -> tensor<1x256x1x1xf32> loc(#loc303)
        %271 = "tpu.Load"(%63) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 32, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 135, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x1xf32, 4301586432 : i64>) -> tensor<1x512x1x1xf32> loc(#loc304)
        %272 = "tpu.Load"(%64) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 136, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x512xf16, 4301590528 : i64>) -> tensor<1x512x1x512xf16> loc(#loc305)
        %273 = "tpu.Conv2D"(%269, %251, %267) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 137, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<1x256x1x2304xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc306)
        %274 = "tpu.Cast"(%273) {ginfo = #tpu.lg<out_addr = 48384, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 138, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc307)
        %275 = "tpu.Load"(%62) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 139, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4301324288 : i64>) -> tensor<1x256x1x512xf16> loc(#loc308)
        %276 = "tpu.Active"(%274) {ginfo = #tpu.lg<out_addr = 122944, out_size = 6400, buffer_addr = 98304, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 140, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc309)
        %277 = "tpu.Cast"(%276) {ginfo = #tpu.lg<out_addr = 86016, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 141, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc310)
        %278 = "tpu.Add"(%264, %277) {do_relu = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 142, stage = 1, slice_idx = 0, group_type = 0>, is_scalar = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc311)
        %279 = "tpu.Conv2D"(%258, %275, %270) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 52480, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 143, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc312)
        %280 = "tpu.Cast"(%279) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 144, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc313)
        %281 = "tpu.Active"(%280) {ginfo = #tpu.lg<out_addr = 52480, out_size = 6400, buffer_addr = 81920, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 145, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc314)
        %282 = "tpu.Cast"(%281) {ginfo = #tpu.lg<out_addr = 81920, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 146, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc315)
        %283 = "tpu.Concat"(%278, %282) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 38400, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 147, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc316)
        %284 = "tpu.Load"(%67) {do_bcast = false, ginfo = #tpu.lg<out_addr = 102400, out_size = 32, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 148, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x1xf32, 4302381056 : i64>) -> tensor<1x512x1x1xf32> loc(#loc317)
        %285 = "tpu.Conv2D"(%283, %272, %271) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 149, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x512x1x512xf16>, tensor<1x512x1x1xf32>) -> tensor<1x512x20x20xf16> loc(#loc318)
        %286 = "tpu.Cast"(%285) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 150, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc319)
        %287 = "tpu.Load"(%65) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 151, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4302114816 : i64>) -> tensor<1x256x1x1xf32> loc(#loc320)
        %288 = "tpu.Load"(%66) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 152, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4302118912 : i64>) -> tensor<1x256x1x512xf16> loc(#loc321)
        %289 = "tpu.Active"(%286) {ginfo = #tpu.lg<out_addr = 81920, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 153, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc322)
        %290 = "tpu.Cast"(%289) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 154, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc323)
        %291 = "tpu.Conv2D"(%290, %288, %287) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 155, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc324)
        %292 = "tpu.Cast"(%291) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 156, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc325)
        %293 = "tpu.Active"(%292) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 81920, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 157, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc326)
        %294 = "tpu.Cast"(%293) {ginfo = #tpu.lg<out_addr = 81920, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 158, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc327)
        %295 = "tpu.Pool2D"(%294) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 98304, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 159, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc328)
        %296 = "tpu.Pool2D"(%295) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 114688, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 160, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc329)
        %297 = "tpu.Pool2D"(%296) {count_include_pad = false, do_relu = false, first_round_mode = #tpu<round_mode HalfAwayFromZero>, ginfo = #tpu.lg<out_addr = 131072, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 161, stage = 1, slice_idx = 0, group_type = 0>, is_adaptive = false, keepdims = true, kernel_shape = [5, 5], pad_value = 0 : i64, pads = [2, 2, 2, 2], pool_mode = #tpu<pool_mode Max>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfAwayFromZero>, strides = [1, 1]} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf16> loc(#loc330)
        %298 = "tpu.Concat"(%294, %295, %296, %297) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 38400, out_size = 13312, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [1024], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 162, stage = 1, slice_idx = 0, group_type = 0>, only_merge = false, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x1024x20x20xf16> loc(#loc331)
        %299 = "tpu.Conv2D"(%298, %252, %284) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 163, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x1024x20x20xf16>, tensor<1x512x1x1024xf16>, tensor<1x512x1x1xf32>) -> tensor<1x512x20x20xf16> loc(#loc332)
        %300 = "tpu.Cast"(%299) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 164, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc333)
        %301 = "tpu.Load"(%69) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 165, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4303433728 : i64>) -> tensor<1x256x1x1xf32> loc(#loc334)
        %302 = "tpu.Load"(%70) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 166, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4303437824 : i64>) -> tensor<1x256x1x512xf16> loc(#loc335)
        %303 = "tpu.Active"(%300) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 167, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc336)
        %304 = "tpu.Cast"(%303) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 168, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc337)
        %305 = "tpu.Conv2D"(%304, %302, %301) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 169, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc338)
        %306 = "tpu.Cast"(%305) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 170, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc339)
        %307 = "tpu.Load"(%71) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 171, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4303699968 : i64>) -> tensor<1x128x1x1xf32> loc(#loc340)
        %308 = "tpu.Load"(%72) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 172, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x512xf16, 4303704064 : i64>) -> tensor<1x128x1x512xf16> loc(#loc341)
        %309 = "tpu.Active"(%306) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 65536, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 173, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc342)
        %310 = "tpu.Cast"(%309) {ginfo = #tpu.lg<out_addr = 252032, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 174, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc343)
        %311 = "tpu.Upsample"(%310) {do_relu = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 175, stage = 1, slice_idx = 0, group_type = 0>, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x256x20x20xf16>) -> tensor<1x256x40x40xf16> loc(#loc344)
        %312 = "tpu.Concat"(%311, %250) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 176, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x40x40xf16>, tensor<1x256x40x40xf16>) -> tensor<1x512x40x40xf16> loc(#loc345)
        %313 = "tpu.Conv2D"(%312, %308, %307) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 177, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40xf16>, tensor<1x128x1x512xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc346)
        %314 = "tpu.Cast"(%313) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 178, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc347)
        %315 = "tpu.Load"(%73) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 179, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4303835136 : i64>) -> tensor<1x128x1x1xf32> loc(#loc348)
        %316 = "tpu.Load"(%74) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 180, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4303839232 : i64>) -> tensor<1x128x1x128xf16> loc(#loc349)
        %317 = "tpu.Active"(%314) {ginfo = #tpu.lg<out_addr = 74752, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 181, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc350)
        %318 = "tpu.Cast"(%317) {ginfo = #tpu.lg<out_addr = 25600, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 182, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc351)
        %319 = "tpu.Conv2D"(%318, %316, %315) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 74752, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 183, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc352)
        %320 = "tpu.Cast"(%319) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 184, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc353)
        %321 = "tpu.Load"(%76) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 185, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4303876096 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc354)
        %322 = "tpu.Load"(%75) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 186, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4303872000 : i64>) -> tensor<1x128x1x1xf32> loc(#loc355)
        %323 = "tpu.Active"(%320) {ginfo = #tpu.lg<out_addr = 74752, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 187, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc356)
        %324 = "tpu.Cast"(%323) {ginfo = #tpu.lg<out_addr = 25600, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 188, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc357)
        %325 = "tpu.Conv2D"(%324, %321, %322) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 74752, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 189, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc358)
        %326 = "tpu.Cast"(%325) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 190, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc359)
        %327 = "tpu.Load"(%77) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 191, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4304171008 : i64>) -> tensor<1x128x1x1xf32> loc(#loc360)
        %328 = "tpu.Load"(%78) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 192, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x512xf16, 4304175104 : i64>) -> tensor<1x128x1x512xf16> loc(#loc361)
        %329 = "tpu.Active"(%326) {ginfo = #tpu.lg<out_addr = 74752, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 193, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc362)
        %330 = "tpu.Cast"(%329) {ginfo = #tpu.lg<out_addr = 25600, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 194, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc363)
        %331 = "tpu.Conv2D"(%312, %328, %327) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 195, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x40x40xf16>, tensor<1x128x1x512xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc364)
        %332 = "tpu.Cast"(%331) {ginfo = #tpu.lg<out_addr = 32000, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 196, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc365)
        %333 = "tpu.Load"(%79) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 197, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4304306176 : i64>) -> tensor<1x256x1x1xf32> loc(#loc366)
        %334 = "tpu.Load"(%80) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 198, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xf16, 4304310272 : i64>) -> tensor<1x256x1x256xf16> loc(#loc367)
        %335 = "tpu.Active"(%332) {ginfo = #tpu.lg<out_addr = 49152, out_size = 12800, buffer_addr = 65536, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 199, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc368)
        %336 = "tpu.Cast"(%335) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 200, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc369)
        %337 = "tpu.Concat"(%330, %336) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 201, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc370)
        %338 = "tpu.Conv2D"(%337, %334, %333) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 202, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x256x1x256xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x40x40xf16> loc(#loc371)
        %339 = "tpu.Cast"(%338) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 203, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc372)
        %340 = "tpu.Load"(%81) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 204, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4304441344 : i64>) -> tensor<1x128x1x1xf32> loc(#loc373)
        %341 = "tpu.Load"(%82) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 205, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xf16, 4304445440 : i64>) -> tensor<1x128x1x256xf16> loc(#loc374)
        %342 = "tpu.Active"(%339) {ginfo = #tpu.lg<out_addr = 65536, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 206, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc375)
        %343 = "tpu.Cast"(%342) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 207, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc376)
        %344 = "tpu.Conv2D"(%343, %341, %340) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 208, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x128x1x256xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc377)
        %345 = "tpu.Cast"(%344) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 209, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc378)
        %346 = "tpu.Load"(%83) {do_bcast = false, ginfo = #tpu.lg<out_addr = 123904, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 210, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4304510976 : i64>) -> tensor<1x64x1x1xf32> loc(#loc379)
        %347 = "tpu.Active"(%345) {ginfo = #tpu.lg<out_addr = 49152, out_size = 12800, buffer_addr = 65536, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 211, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc380)
        %348 = "tpu.Cast"(%347) {ginfo = #tpu.lg<out_addr = 255360, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 212, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc381)
        %349 = "tpu.Upsample"(%348) {do_relu = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 213, stage = 1, slice_idx = 0, group_type = 0>, relu_limit = -1.000000e+00 : f64, scale_h = 2 : i64, scale_w = 2 : i64} : (tensor<1x128x40x40xf16>) -> tensor<1x128x80x80xf16> loc(#loc382)
        %350 = "tpu.Load"(%84) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 214, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x256xf16, 4304515072 : i64>) -> tensor<1x64x1x256xf16> loc(#loc383)
        %351 = "tpu.Concat"(%349, %186) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 51200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 215, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x80x80xf16>, tensor<1x128x80x80xf16>) -> tensor<1x256x80x80xf16> loc(#loc384)
        %352 = "tpu.Conv2D"(%351, %350, %346) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 216, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80xf16>, tensor<1x64x1x256xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc385)
        %353 = "tpu.Cast"(%352) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 217, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc386)
        %354 = "tpu.Load"(%85) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 218, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4304547840 : i64>) -> tensor<1x64x1x1xf32> loc(#loc387)
        %355 = "tpu.Active"(%353) {ginfo = #tpu.lg<out_addr = 83968, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 219, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc388)
        %356 = "tpu.Load"(%86) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 128, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [64], id = 220, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x64xf16, 4304551936 : i64>) -> tensor<1x64x1x64xf16> loc(#loc389)
        %357 = "tpu.Cast"(%355) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 221, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc390)
        %358 = "tpu.Conv2D"(%357, %356, %354) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 83968, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 222, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x64xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc391)
        %359 = "tpu.Cast"(%358) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 223, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc392)
        %360 = "tpu.Load"(%88) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 1152, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [576], id = 224, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x576xf16, 4304564224 : i64>) -> tensor<1x64x1x576xf16> loc(#loc393)
        %361 = "tpu.Load"(%87) {do_bcast = false, ginfo = #tpu.lg<out_addr = 196608, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 225, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4304560128 : i64>) -> tensor<1x64x1x1xf32> loc(#loc394)
        %362 = "tpu.Active"(%359) {ginfo = #tpu.lg<out_addr = 83968, out_size = 25600, buffer_addr = 131072, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 226, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc395)
        %363 = "tpu.Cast"(%362) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 227, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc396)
        %364 = "tpu.Conv2D"(%363, %360, %361) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 83968, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 228, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x64x80x80xf16>, tensor<1x64x1x576xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc397)
        %365 = "tpu.Cast"(%364) {ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 229, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc398)
        %366 = "tpu.Load"(%89) {do_bcast = false, ginfo = #tpu.lg<out_addr = 180224, out_size = 4, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 230, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x1xf32, 4304637952 : i64>) -> tensor<1x64x1x1xf32> loc(#loc399)
        %367 = "tpu.Active"(%365) {ginfo = #tpu.lg<out_addr = 83968, out_size = 25600, buffer_addr = 114688, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 231, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc400)
        %368 = "tpu.Load"(%90) {do_bcast = false, ginfo = #tpu.lg<out_addr = 16384, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 232, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x64x1x256xf16, 4304642048 : i64>) -> tensor<1x64x1x256xf16> loc(#loc401)
        %369 = "tpu.Cast"(%367) {ginfo = #tpu.lg<out_addr = 0, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 233, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc402)
        %370 = "tpu.Conv2D"(%351, %368, %366) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 234, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x80x80xf16>, tensor<1x64x1x256xf16>, tensor<1x64x1x1xf32>) -> tensor<1x64x80x80xf16> loc(#loc403)
        %371 = "tpu.Cast"(%370) {ginfo = #tpu.lg<out_addr = 12800, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 235, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf16>) -> tensor<1x64x80x80xf32> loc(#loc404)
        %372 = "tpu.Load"(%91) {do_bcast = false, ginfo = #tpu.lg<out_addr = 163840, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 236, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4304674816 : i64>) -> tensor<1x128x1x1xf32> loc(#loc405)
        %373 = "tpu.Load"(%92) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 237, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4304678912 : i64>) -> tensor<1x128x1x128xf16> loc(#loc406)
        %374 = "tpu.Active"(%371) {ginfo = #tpu.lg<out_addr = 49152, out_size = 25600, buffer_addr = 98304, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 238, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf32> loc(#loc407)
        %375 = "tpu.Cast"(%374) {ginfo = #tpu.lg<out_addr = 16384, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [64], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 239, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x64x80x80xf32>) -> tensor<1x64x80x80xf16> loc(#loc408)
        %376 = "tpu.Concat"(%369, %375) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 240, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x64x80x80xf16>, tensor<1x64x80x80xf16>) -> tensor<1x128x80x80xf16> loc(#loc409)
        %377 = "tpu.Conv2D"(%376, %373, %372) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 0, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 241, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x80x80xf16> loc(#loc410)
        %378 = "tpu.Cast"(%377) {ginfo = #tpu.lg<out_addr = 32768, out_size = 51200, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 242, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf16>) -> tensor<1x128x80x80xf32> loc(#loc411)
        %379 = "tpu.Load"(%94) {do_bcast = false, ginfo = #tpu.lg<out_addr = 83968, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 243, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4304715776 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc412)
        %380 = "tpu.Load"(%93) {do_bcast = false, ginfo = #tpu.lg<out_addr = 88576, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 244, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4304711680 : i64>) -> tensor<1x128x1x1xf32> loc(#loc413)
        %381 = "tpu.Active"(%378) {ginfo = #tpu.lg<out_addr = 98304, out_size = 51200, buffer_addr = 149504, buffer_size = 102528, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 245, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf32> loc(#loc414)
        %382 = "tpu.Cast"(%381) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 246, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x80x80xf32>) -> tensor<1x128x80x80xf16> loc(#loc166)
        %383 = "tpu.Store"(%382, %0) {ginfo = #tpu.lg<out_addr = 25600, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [80], w_idx = [0], w_slice = [80], id = 247, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x128x80x80xf16>, none) -> tensor<1x128x80x80xf16, 4314570752 : i64> loc(#loc166)
        %384 = "tpu.Conv2D"(%382, %379, %380) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 248, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc415)
        %385 = "tpu.Cast"(%384) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 249, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc416)
        %386 = "tpu.Load"(%95) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 250, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4305010688 : i64>) -> tensor<1x128x1x1xf32> loc(#loc417)
        %387 = "tpu.Load"(%96) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 251, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xf16, 4305014784 : i64>) -> tensor<1x128x1x256xf16> loc(#loc418)
        %388 = "tpu.Active"(%385) {ginfo = #tpu.lg<out_addr = 49152, out_size = 12800, buffer_addr = 65536, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 252, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc419)
        %389 = "tpu.Cast"(%388) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 253, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc420)
        %390 = "tpu.Concat"(%389, %348) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 254, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc421)
        %391 = "tpu.Conv2D"(%390, %387, %386) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 255, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x128x1x256xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc422)
        %392 = "tpu.Cast"(%391) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 256, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc423)
        %393 = "tpu.Load"(%97) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 257, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4305080320 : i64>) -> tensor<1x128x1x1xf32> loc(#loc424)
        %394 = "tpu.Load"(%98) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 512, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [128], id = 258, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x128xf16, 4305084416 : i64>) -> tensor<1x128x1x128xf16> loc(#loc425)
        %395 = "tpu.Active"(%392) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 98304, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 259, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc426)
        %396 = "tpu.Cast"(%395) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 260, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc427)
        %397 = "tpu.Conv2D"(%396, %394, %393) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 261, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x128xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc428)
        %398 = "tpu.Cast"(%397) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 262, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc429)
        %399 = "tpu.Load"(%100) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 4608, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1152], id = 263, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1152xf16, 4305121280 : i64>) -> tensor<1x128x1x1152xf16> loc(#loc430)
        %400 = "tpu.Load"(%99) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 264, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4305117184 : i64>) -> tensor<1x128x1x1xf32> loc(#loc431)
        %401 = "tpu.Active"(%398) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 81920, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 265, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc432)
        %402 = "tpu.Cast"(%401) {ginfo = #tpu.lg<out_addr = 38400, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 266, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc433)
        %403 = "tpu.Load"(%106) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 267, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x2304xf16, 4305625088 : i64>) -> tensor<1x256x1x2304xf16> loc(#loc434)
        %404 = "tpu.Conv2D"(%402, %399, %400) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 268, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x40x40xf16>, tensor<1x128x1x1152xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc435)
        %405 = "tpu.Cast"(%404) {ginfo = #tpu.lg<out_addr = 38400, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 269, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc436)
        %406 = "tpu.Load"(%101) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 8, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 270, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x1xf32, 4305416192 : i64>) -> tensor<1x128x1x1xf32> loc(#loc437)
        %407 = "tpu.Load"(%102) {do_bcast = false, ginfo = #tpu.lg<out_addr = 100352, out_size = 1024, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 271, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x128x1x256xf16, 4305420288 : i64>) -> tensor<1x128x1x256xf16> loc(#loc438)
        %408 = "tpu.Active"(%405) {ginfo = #tpu.lg<out_addr = 65536, out_size = 12800, buffer_addr = 114688, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 272, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc439)
        %409 = "tpu.Cast"(%408) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 273, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc440)
        %410 = "tpu.Conv2D"(%390, %407, %406) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 55552, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 274, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x128x1x256xf16>, tensor<1x128x1x1xf32>) -> tensor<1x128x40x40xf16> loc(#loc441)
        %411 = "tpu.Cast"(%410) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 275, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf16>) -> tensor<1x128x40x40xf32> loc(#loc442)
        %412 = "tpu.Load"(%103) {do_bcast = false, ginfo = #tpu.lg<out_addr = 147456, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 276, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4305485824 : i64>) -> tensor<1x256x1x1xf32> loc(#loc443)
        %413 = "tpu.Load"(%104) {do_bcast = false, ginfo = #tpu.lg<out_addr = 131072, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 277, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xf16, 4305489920 : i64>) -> tensor<1x256x1x256xf16> loc(#loc444)
        %414 = "tpu.Active"(%411) {ginfo = #tpu.lg<out_addr = 55552, out_size = 12800, buffer_addr = 100352, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 278, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf32> loc(#loc445)
        %415 = "tpu.Cast"(%414) {ginfo = #tpu.lg<out_addr = 100352, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [128], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 279, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x128x40x40xf32>) -> tensor<1x128x40x40xf16> loc(#loc446)
        %416 = "tpu.Concat"(%409, %415) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 280, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x128x40x40xf16>, tensor<1x128x40x40xf16>) -> tensor<1x256x40x40xf16> loc(#loc447)
        %417 = "tpu.Load"(%105) {do_bcast = false, ginfo = #tpu.lg<out_addr = 189440, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 281, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4305620992 : i64>) -> tensor<1x256x1x1xf32> loc(#loc448)
        %418 = "tpu.Conv2D"(%416, %413, %412) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 100352, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 282, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x256x1x256xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x40x40xf16> loc(#loc449)
        %419 = "tpu.Cast"(%418) {ginfo = #tpu.lg<out_addr = 49152, out_size = 25600, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 283, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf16>) -> tensor<1x256x40x40xf32> loc(#loc450)
        %420 = "tpu.Load"(%112) {do_bcast = false, ginfo = #tpu.lg<out_addr = 25600, out_size = 18432, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [2304], id = 284, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x2304xf16, 4307210240 : i64>) -> tensor<1x256x1x2304xf16> loc(#loc451)
        %421 = "tpu.Active"(%419) {ginfo = #tpu.lg<out_addr = 163840, out_size = 25600, buffer_addr = 100352, buffer_size = 51328, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 285, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf32> loc(#loc452)
        %422 = "tpu.Cast"(%421) {ginfo = #tpu.lg<out_addr = 44032, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 286, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x40x40xf32>) -> tensor<1x256x40x40xf16> loc(#loc167)
        %423 = "tpu.Load"(%107) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 288, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4306804736 : i64>) -> tensor<1x256x1x1xf32> loc(#loc453)
        %424 = "tpu.Store"(%422, %0) {ginfo = #tpu.lg<out_addr = 44032, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [40], w_idx = [0], w_slice = [40], id = 287, stage = 1, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x256x40x40xf16>, none) -> tensor<1x256x40x40xf16, 4317835264 : i64> loc(#loc167)
        %425 = "tpu.Conv2D"(%422, %403, %417) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 289, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [2, 2], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16>, tensor<1x256x1x2304xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc454)
        %426 = "tpu.Cast"(%425) {ginfo = #tpu.lg<out_addr = 44032, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 290, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc455)
        %427 = "tpu.Load"(%108) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 291, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4306808832 : i64>) -> tensor<1x256x1x512xf16> loc(#loc456)
        %428 = "tpu.Active"(%426) {ginfo = #tpu.lg<out_addr = 98304, out_size = 6400, buffer_addr = 81920, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 292, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc457)
        %429 = "tpu.Cast"(%428) {ginfo = #tpu.lg<out_addr = 69632, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 293, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc458)
        %430 = "tpu.Concat"(%429, %310) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 44032, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 294, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc459)
        %431 = "tpu.Load"(%109) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 295, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4307070976 : i64>) -> tensor<1x256x1x1xf32> loc(#loc460)
        %432 = "tpu.Conv2D"(%430, %427, %423) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 296, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc461)
        %433 = "tpu.Cast"(%432) {ginfo = #tpu.lg<out_addr = 50688, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 297, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc462)
        %434 = "tpu.Load"(%110) {do_bcast = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 2048, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [256], id = 298, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x256xf16, 4307075072 : i64>) -> tensor<1x256x1x256xf16> loc(#loc463)
        %435 = "tpu.Active"(%433) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 98368, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 299, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc464)
        %436 = "tpu.Cast"(%435) {ginfo = #tpu.lg<out_addr = 50688, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 300, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc465)
        %437 = "tpu.Conv2D"(%436, %434, %431) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 301, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<1x256x1x256xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc466)
        %438 = "tpu.Cast"(%437) {ginfo = #tpu.lg<out_addr = 50688, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 302, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc467)
        %439 = "tpu.Load"(%111) {do_bcast = false, ginfo = #tpu.lg<out_addr = 114688, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 303, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4307206144 : i64>) -> tensor<1x256x1x1xf32> loc(#loc468)
        %440 = "tpu.Active"(%438) {ginfo = #tpu.lg<out_addr = 65536, out_size = 6400, buffer_addr = 81920, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 304, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc469)
        %441 = "tpu.Cast"(%440) {ginfo = #tpu.lg<out_addr = 50688, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 305, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc470)
        %442 = "tpu.Load"(%113) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98368, out_size = 16, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 306, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x1xf32, 4308389888 : i64>) -> tensor<1x256x1x1xf32> loc(#loc471)
        %443 = "tpu.Load"(%115) {do_bcast = false, ginfo = #tpu.lg<out_addr = 98304, out_size = 32, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [1], id = 307, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x1xf32, 4308656128 : i64>) -> tensor<1x512x1x1xf32> loc(#loc472)
        %444 = "tpu.Load"(%116) {do_bcast = false, ginfo = #tpu.lg<out_addr = 65536, out_size = 8192, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 308, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x512x1x512xf16, 4308660224 : i64>) -> tensor<1x512x1x512xf16> loc(#loc473)
        %445 = "tpu.Conv2D"(%441, %420, %439) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 309, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [3, 3], kernel_zp = 0 : i64, pads = [1, 1, 1, 1], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x20x20xf16>, tensor<1x256x1x2304xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc474)
        %446 = "tpu.Cast"(%445) {ginfo = #tpu.lg<out_addr = 25600, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 310, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc475)
        %447 = "tpu.Load"(%114) {do_bcast = false, ginfo = #tpu.lg<out_addr = 73728, out_size = 4096, buffer_addr = 0, buffer_size = 0, eu_align = false, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [1], w_idx = [0], w_slice = [512], id = 311, stage = 1, slice_idx = 0, group_type = 0>, lmem_type = 0 : i64, support_compress = true, use_3ic_optimize = 0 : i64} : (tensor<1x256x1x512xf16, 4308393984 : i64>) -> tensor<1x256x1x512xf16> loc(#loc476)
        %448 = "tpu.Active"(%446) {ginfo = #tpu.lg<out_addr = 32768, out_size = 6400, buffer_addr = 50688, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 312, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc477)
        %449 = "tpu.Cast"(%448) {ginfo = #tpu.lg<out_addr = 25600, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 313, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc478)
        %450 = "tpu.Conv2D"(%430, %447, %442) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 81920, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 314, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x256x1x512xf16>, tensor<1x256x1x1xf32>) -> tensor<1x256x20x20xf16> loc(#loc479)
        %451 = "tpu.Cast"(%450) {ginfo = #tpu.lg<out_addr = 28928, out_size = 6400, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 315, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf16>) -> tensor<1x256x20x20xf32> loc(#loc480)
        %452 = "tpu.Active"(%451) {ginfo = #tpu.lg<out_addr = 49152, out_size = 6400, buffer_addr = 73728, buffer_size = 12928, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 316, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf32> loc(#loc481)
        %453 = "tpu.Cast"(%452) {ginfo = #tpu.lg<out_addr = 73728, out_size = 3328, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [256], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 317, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x256x20x20xf32>) -> tensor<1x256x20x20xf16> loc(#loc482)
        %454 = "tpu.Concat"(%449, %453) {axis = 1 : si32, do_relu = false, ginfo = #tpu.lg<out_addr = 32768, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 318, stage = 1, slice_idx = 0, group_type = 0>, only_merge = true, relu_limit = -1.000000e+00 : f64} : (tensor<1x256x20x20xf16>, tensor<1x256x20x20xf16>) -> tensor<1x512x20x20xf16> loc(#loc483)
        %455 = "tpu.Conv2D"(%454, %444, %443) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, ginfo = #tpu.lg<out_addr = 49152, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 319, stage = 1, slice_idx = 0, group_type = 0>, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16>, tensor<1x512x1x512xf16>, tensor<1x512x1x1xf32>) -> tensor<1x512x20x20xf16> loc(#loc484)
        %456 = "tpu.Cast"(%455) {ginfo = #tpu.lg<out_addr = 25600, out_size = 12800, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 320, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf16>) -> tensor<1x512x20x20xf32> loc(#loc485)
        %457 = "tpu.Active"(%456) {ginfo = #tpu.lg<out_addr = 49152, out_size = 12800, buffer_addr = 65536, buffer_size = 25728, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 321, stage = 1, slice_idx = 0, group_type = 0>, mode = #tpu<active_mode SILU>} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf32> loc(#loc486)
        %458 = "tpu.Cast"(%457) {ginfo = #tpu.lg<out_addr = 245760, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 322, stage = 1, slice_idx = 0, group_type = 0>, round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x512x20x20xf32>) -> tensor<1x512x20x20xf16> loc(#loc168)
        %459 = "tpu.Store"(%458, %0) {ginfo = #tpu.lg<out_addr = 245760, out_size = 6656, buffer_addr = 0, buffer_size = 0, eu_align = true, can_merge = false, in_hslice_offset = [], n_idx = [0], n_slice = [1], c_idx = [0], c_slice = [512], d_idx = [0], d_slice = [1], h_idx = [0], h_slice = [20], w_idx = [0], w_slice = [20], id = 323, stage = 2, slice_idx = 0, group_type = 0>, support_compress = true} : (tensor<1x512x20x20xf16>, none) -> tensor<1x512x20x20xf16, 4318654464 : i64> loc(#loc168)
        "tpu.Yield"(%383, %424, %459) : (tensor<1x128x80x80xf16, 4314570752 : i64>, tensor<1x256x40x40xf16, 4317835264 : i64>, tensor<1x512x20x20xf16, 4318654464 : i64>) -> () loc(#loc505)
      }) {core_slice_ncdhw = [], csecs = 1 : i64, dsecs = 1 : i64, flow = [-1, 1, -2, 2, 323, -3, 5, 3, 4, -4, 6, -5, 7, -6, 9, 8, -7, 11, 10, -8, 12, -9, 13, -10, 16, 14, 15, -11, 17, -12, 18, -13, 19, -14, 21, 20, -15, 22, -16, 24, 23, -17, 25, -18, 26, -19, 29, 27, 28, -20, 30, -21, 31, -22, 32, -23, 34, 33, -24, 35, -25, 37, 36, -26, 38, -27, 39, -28, 42, 40, 41, -29, 43, -30, 44, -31, 45, -32, 46, -33, 49, 47, 48, -34, 50, -35, 52, 51, -36, 53, -37, 56, 54, 55, -38, 57, -39, 58, -40, 59, -41, 62, 60, 61, -42, 63, -43, 64, -44, 65, -45, 68, 66, 67, -46, 69, -47, 70, -48, 71, -49, 74, 72, 73, -50, 75, -51, 76, -52, 77, -53, 78, -54, 81, 79, 80, -55, 82, -56, 83, -57, 84, -58, 87, 85, 86, -59, 88, -60, 89, -61, 90, -62, 91, -63, 94, 92, 93, -64, 95, -65, 96, -66, 97, -67, 100, 98, 99, -68, 101, -69, 102, -70, 103, -71, 104, -72, 107, 105, 106, -73, 108, -74, 109, -75, 110, -76, 111, -77, 113, 112, -78, 114, -79, 117, 115, 116, -80, 118, -81, 121, 119, 120, -82, 122, -83, 124, 123, -84, 125, -85, 127, 126, -86, 128, -87, 129, -88, 130, -89, 132, 131, -90, 133, -91, 137, 134, 135, 136, -92, 138, -93, 140, 139, -94, 141, -95, 142, -96, 143, -97, 144, -98, 145, -99, 146, -100, 147, -101, 149, 148, -102, 150, -103, 153, 151, 152, -104, 154, -105, 155, -106, 156, -107, 157, -108, 158, -109, 159, -110, 160, -111, 161, -112, 162, -113, 163, -114, 164, -115, 167, 165, 166, -116, 168, -117, 169, -118, 170, -119, 173, 171, 172, -120, 174, -121, 175, -122, 176, -123, 177, -124, 178, -125, 181, 179, 180, -126, 182, -127, 183, -128, 184, -129, 187, 185, 186, -130, 188, -131, 189, -132, 190, -133, 193, 191, 192, -134, 194, -135, 195, -136, 196, -137, 199, 197, 198, -138, 200, -139, 201, -140, 202, -141, 203, -142, 206, 204, 205, -143, 207, -144, 208, -145, 209, -146, 211, 210, -147, 212, -148, 213, -149, 215, 214, -150, 216, -151, 217, -152, 219, 218, -153, 221, 220, -154, 222, -155, 223, -156, 226, 224, 225, -157, 227, -158, 228, -159, 229, -160, 231, 230, -161, 233, 232, -162, 234, -163, 235, -164, 238, 236, 237, -165, 239, -166, 240, -167, 241, -168, 242, -169, 245, 243, 244, 0, -170, 246, -171, 248, 247, -172, 249, -173, 252, 250, 251, -174, 253, -175, 254, -176, 255, -177, 256, -178, 259, 257, 258, -179, 260, -180, 261, -181, 262, -182, 265, 263, 264, -183, 266, -184, 268, 267, -185, 269, -186, 272, 270, 271, -187, 273, -188, 274, -189, 275, -190, 278, 276, 277, -191, 279, -192, 280, -193, 282, 281, -194, 283, -195, 285, 284, -196, 286, -197, 289, 287, 288, -198, 290, -199, 292, 291, -200, 293, -201, 294, -202, 296, 295, -203, 297, -204, 299, 298, -205, 300, -206, 301, -207, 302, -208, 304, 303, -209, 305, -210, 309, 306, 307, 308, -211, 310, -212, 312, 311, -213, 313, -214, 314, -215, 315, -216, 316, -217, 317, -218, 318, -219, 319, -220, 320, -221, 321, -222, 322], group_type = 0 : i64, hsecs = 1 : i64, nsecs = 1 : i64, other_down_overlap_op = [], other_up_overlap_op = [], run_core_id = [], self_down_overlap_op = [], self_up_overlap_op = [], support_compress = true, swpipl_stage_num = 3 : i64, wsecs = 1 : i64} : (tensor<1x128x80x80xf16, 4319473664 : i64>) -> (tensor<1x128x80x80xf16, 4314570752 : i64>, tensor<1x256x40x40xf16, 4317835264 : i64>, tensor<1x512x20x20xf16, 4318654464 : i64>) loc(#loc505)
      %118 = "top.Weight"() : () -> tensor<1x255x1x1xf32, 4309184512 : i64> loc(#loc487)
      %119 = "top.Weight"() : () -> tensor<1x255x1x128xf16, 4309188608 : i64> loc(#loc488)
      %120 = "tpu.Conv2D"(%117#0, %119, %118) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x128x80x80xf16, 4314570752 : i64>, tensor<1x255x1x128xf16, 4309188608 : i64>, tensor<1x255x1x1xf32, 4309184512 : i64>) -> tensor<1x255x80x80xf16, 4319064064 : i64> loc(#loc489)
      %121 = "tpu.Reshape"(%120) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 80, 80]} : (tensor<1x255x80x80xf16, 4319064064 : i64>) -> tensor<1x3x85x80x80xf16, 4319064064 : i64> loc(#loc490)
      %122 = "tpu.Permute"(%121, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x80x80xf16, 4319064064 : i64>, none) -> tensor<1x3x80x80x85xf16, 4314570752 : i64> loc(#loc491)
      %123 = "tpu.Cast"(%122) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x80x80x85xf16, 4314570752 : i64>) -> tensor<1x3x80x80x85xf32, 4319064064 : i64> loc(#loc492)
      %124 = "top.Weight"() : () -> tensor<1x255x1x1xf32, 4309254144 : i64> loc(#loc493)
      %125 = "top.Weight"() : () -> tensor<1x255x1x256xf16, 4309258240 : i64> loc(#loc494)
      %126 = "tpu.Conv2D"(%117#1, %125, %124) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x256x40x40xf16, 4317835264 : i64>, tensor<1x255x1x256xf16, 4309258240 : i64>, tensor<1x255x1x1xf32, 4309254144 : i64>) -> tensor<1x255x40x40xf16, 4314570752 : i64> loc(#loc495)
      %127 = "tpu.Reshape"(%126) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 40, 40]} : (tensor<1x255x40x40xf16, 4314570752 : i64>) -> tensor<1x3x85x40x40xf16, 4314570752 : i64> loc(#loc496)
      %128 = "tpu.Permute"(%127, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x40x40xf16, 4314570752 : i64>, none) -> tensor<1x3x40x40x85xf16, 4315389952 : i64> loc(#loc497)
      %129 = "tpu.Cast"(%128) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x40x40x85xf16, 4315389952 : i64>) -> tensor<1x3x40x40x85xf32, 4316209152 : i64> loc(#loc498)
      %130 = "top.Weight"() : () -> tensor<1x255x1x1xf32, 4309389312 : i64> loc(#loc499)
      %131 = "top.Weight"() : () -> tensor<1x255x1x512xf16, 4309393408 : i64> loc(#loc500)
      %132 = "tpu.Conv2D"(%117#2, %131, %130) {coeff_merged = false, dilations = [1, 1], do_relu = false, dynweight_reorderd = false, group = 1 : i64, kernel_shape = [1, 1], kernel_zp = 0 : i64, pads = [0, 0, 0, 0], quant_mode = #tpu<rq_mode MultiplierShift>, relu_limit = -1.000000e+00 : f64, round_mode = #tpu<round_mode HalfUp>, strides = [1, 1], support_compress = true, use_3ic_optimize = 0 : i64, weight_is_coeff = 1 : i64, with_bias = true} : (tensor<1x512x20x20xf16, 4318654464 : i64>, tensor<1x255x1x512xf16, 4309393408 : i64>, tensor<1x255x1x1xf32, 4309389312 : i64>) -> tensor<1x255x20x20xf16, 4314570752 : i64> loc(#loc501)
      %133 = "tpu.Reshape"(%132) {flatten_start_dim = -1 : i64, shape = [1, 3, 85, 20, 20]} : (tensor<1x255x20x20xf16, 4314570752 : i64>) -> tensor<1x3x85x20x20xf16, 4314570752 : i64> loc(#loc502)
      %134 = "tpu.Permute"(%133, %0) {order = [0, 1, 3, 4, 2]} : (tensor<1x3x85x20x20xf16, 4314570752 : i64>, none) -> tensor<1x3x20x20x85xf16, 4314775552 : i64> loc(#loc503)
      %135 = "tpu.Cast"(%134) {round_mode = #tpu<round_mode HalfAwayFromZero>, with_scale = true} : (tensor<1x3x20x20x85xf16, 4314775552 : i64>) -> tensor<1x3x20x20x85xf32, 4314980352 : i64> loc(#loc504)
      return %123, %129, %135 : tensor<1x3x80x80x85xf32, 4319064064 : i64>, tensor<1x3x40x40x85xf32, 4316209152 : i64>, tensor<1x3x20x20x85xf32, 4314980352 : i64> loc(#loc)
    } loc(#loc)
  } loc(#loc)
} loc(#loc)
#loc2 = loc("model.0.conv.bias")
#loc3 = loc("122_Conv_filter_reorderd")
#loc4 = loc("model.1.conv.bias")
#loc5 = loc("125_Conv_filter_reorderd")
#loc6 = loc("125_Conv")
#loc7 = loc("load_0")
#loc8 = loc("load_122_Conv_filter_reorderd")
#loc9 = loc("load_model.0.conv.bias")
#loc10 = loc("images122_Conv_f16")
#loc11 = loc("122_Conv")
#loc12 = loc("122_Conv124_Mul_f32")
#loc13 = loc("load_model.1.conv.bias")
#loc14 = loc("124_Mul")
#loc15 = loc("load_125_Conv_filter_reorderd")
#loc16 = loc("124_Mul125_Conv_f16")
#loc17 = loc("model.2.cv1.conv.bias")
#loc18 = loc("128_Conv_filter_reorderd")
#loc19 = loc("model.2.m.0.cv1.conv.bias")
#loc20 = loc("131_Conv_filter_reorderd")
#loc21 = loc("model.2.m.0.cv2.conv.bias")
#loc22 = loc("134_Conv_filter_reorderd")
#loc23 = loc("model.2.cv2.conv.bias")
#loc24 = loc("138_Conv_filter_reorderd")
#loc25 = loc("model.2.cv3.conv.bias")
#loc26 = loc("142_Conv_filter_reorderd")
#loc27 = loc("model.3.conv.bias")
#loc28 = loc("145_Conv_filter_reorderd")
#loc29 = loc("145_Conv")
#loc30 = loc("load_125_Conv")
#loc31 = loc("125_Conv127_Mul_f32")
#loc32 = loc("load_model.2.cv1.conv.bias")
#loc33 = loc("127_Mul")
#loc34 = loc("load_128_Conv_filter_reorderd")
#loc35 = loc("127_Mul128_Conv_f16")
#loc36 = loc("128_Conv")
#loc37 = loc("128_Conv130_Mul_f32")
#loc38 = loc("load_model.2.m.0.cv1.conv.bias")
#loc39 = loc("130_Mul")
#loc40 = loc("load_131_Conv_filter_reorderd")
#loc41 = loc("130_Mul131_Conv_f16")
#loc42 = loc("131_Conv")
#loc43 = loc("131_Conv133_Mul_f32")
#loc44 = loc("133_Mul")
#loc45 = loc("load_134_Conv_filter_reorderd")
#loc46 = loc("load_model.2.m.0.cv2.conv.bias")
#loc47 = loc("133_Mul134_Conv_f16")
#loc48 = loc("134_Conv")
#loc49 = loc("134_Conv136_Mul_f32")
#loc50 = loc("load_model.2.cv2.conv.bias")
#loc51 = loc("136_Mul")
#loc52 = loc("136_Mul137_Add_f16")
#loc53 = loc("load_138_Conv_filter_reorderd")
#loc54 = loc("137_Add")
#loc55 = loc("138_Conv")
#loc56 = loc("138_Conv140_Mul_f32")
#loc57 = loc("load_model.2.cv3.conv.bias")
#loc58 = loc("load_142_Conv_filter_reorderd")
#loc59 = loc("140_Mul")
#loc60 = loc("140_Mul141_Concat_f16")
#loc61 = loc("141_Concat")
#loc62 = loc("142_Conv")
#loc63 = loc("142_Conv144_Mul_f32")
#loc64 = loc("load_145_Conv_filter_reorderd")
#loc65 = loc("load_model.3.conv.bias")
#loc66 = loc("144_Mul")
#loc67 = loc("144_Mul145_Conv_f16")
#loc68 = loc("model.4.cv1.conv.bias")
#loc69 = loc("148_Conv_filter_reorderd")
#loc70 = loc("model.4.m.0.cv1.conv.bias")
#loc71 = loc("151_Conv_filter_reorderd")
#loc72 = loc("model.4.m.0.cv2.conv.bias")
#loc73 = loc("154_Conv_filter_reorderd")
#loc74 = loc("model.4.m.1.cv1.conv.bias")
#loc75 = loc("158_Conv_filter_reorderd")
#loc76 = loc("model.4.m.1.cv2.conv.bias")
#loc77 = loc("161_Conv_filter_reorderd")
#loc78 = loc("model.4.cv2.conv.bias")
#loc79 = loc("165_Conv_filter_reorderd")
#loc80 = loc("model.4.cv3.conv.bias")
#loc81 = loc("169_Conv_filter_reorderd")
#loc82 = loc("model.5.conv.bias")
#loc83 = loc("172_Conv_filter_reorderd")
#loc84 = loc("model.6.cv1.conv.bias")
#loc85 = loc("175_Conv_filter_reorderd")
#loc86 = loc("model.6.m.0.cv1.conv.bias")
#loc87 = loc("178_Conv_filter_reorderd")
#loc88 = loc("model.6.m.0.cv2.conv.bias")
#loc89 = loc("181_Conv_filter_reorderd")
#loc90 = loc("model.6.m.1.cv1.conv.bias")
#loc91 = loc("185_Conv_filter_reorderd")
#loc92 = loc("model.6.m.1.cv2.conv.bias")
#loc93 = loc("188_Conv_filter_reorderd")
#loc94 = loc("model.6.m.2.cv1.conv.bias")
#loc95 = loc("192_Conv_filter_reorderd")
#loc96 = loc("model.6.m.2.cv2.conv.bias")
#loc97 = loc("195_Conv_filter_reorderd")
#loc98 = loc("model.6.cv2.conv.bias")
#loc99 = loc("199_Conv_filter_reorderd")
#loc100 = loc("model.6.cv3.conv.bias")
#loc101 = loc("203_Conv_filter_reorderd")
#loc102 = loc("model.7.conv.bias")
#loc103 = loc("206_Conv_filter_reorderd")
#loc104 = loc("model.8.cv1.conv.bias")
#loc105 = loc("209_Conv_filter_reorderd")
#loc106 = loc("model.8.m.0.cv1.conv.bias")
#loc107 = loc("212_Conv_filter_reorderd")
#loc108 = loc("model.8.m.0.cv2.conv.bias")
#loc109 = loc("215_Conv_filter_reorderd")
#loc110 = loc("model.8.cv2.conv.bias")
#loc111 = loc("219_Conv_filter_reorderd")
#loc112 = loc("model.8.cv3.conv.bias")
#loc113 = loc("223_Conv_filter_reorderd")
#loc114 = loc("model.9.cv1.conv.bias")
#loc115 = loc("226_Conv_filter_reorderd")
#loc116 = loc("model.9.cv2.conv.bias")
#loc117 = loc("233_Conv_filter_reorderd")
#loc118 = loc("model.10.conv.bias")
#loc119 = loc("236_Conv_filter_reorderd")
#loc120 = loc("model.13.cv1.conv.bias")
#loc121 = loc("245_Conv_filter_reorderd")
#loc122 = loc("model.13.m.0.cv1.conv.bias")
#loc123 = loc("248_Conv_filter_reorderd")
#loc124 = loc("model.13.m.0.cv2.conv.bias")
#loc125 = loc("251_Conv_filter_reorderd")
#loc126 = loc("model.13.cv2.conv.bias")
#loc127 = loc("254_Conv_filter_reorderd")
#loc128 = loc("model.13.cv3.conv.bias")
#loc129 = loc("258_Conv_filter_reorderd")
#loc130 = loc("model.14.conv.bias")
#loc131 = loc("261_Conv_filter_reorderd")
#loc132 = loc("model.17.cv1.conv.bias")
#loc133 = loc("270_Conv_filter_reorderd")
#loc134 = loc("model.17.m.0.cv1.conv.bias")
#loc135 = loc("273_Conv_filter_reorderd")
#loc136 = loc("model.17.m.0.cv2.conv.bias")
#loc137 = loc("276_Conv_filter_reorderd")
#loc138 = loc("model.17.cv2.conv.bias")
#loc139 = loc("279_Conv_filter_reorderd")
#loc140 = loc("model.17.cv3.conv.bias")
#loc141 = loc("283_Conv_filter_reorderd")
#loc142 = loc("model.18.conv.bias")
#loc143 = loc("286_Conv_filter_reorderd")
#loc144 = loc("model.20.cv1.conv.bias")
#loc145 = loc("290_Conv_filter_reorderd")
#loc146 = loc("model.20.m.0.cv1.conv.bias")
#loc147 = loc("293_Conv_filter_reorderd")
#loc148 = loc("model.20.m.0.cv2.conv.bias")
#loc149 = loc("296_Conv_filter_reorderd")
#loc150 = loc("model.20.cv2.conv.bias")
#loc151 = loc("299_Conv_filter_reorderd")
#loc152 = loc("model.20.cv3.conv.bias")
#loc153 = loc("303_Conv_filter_reorderd")
#loc154 = loc("model.21.conv.bias")
#loc155 = loc("306_Conv_filter_reorderd")
#loc156 = loc("model.23.cv1.conv.bias")
#loc157 = loc("310_Conv_filter_reorderd")
#loc158 = loc("model.23.m.0.cv1.conv.bias")
#loc159 = loc("313_Conv_filter_reorderd")
#loc160 = loc("model.23.m.0.cv2.conv.bias")
#loc161 = loc("316_Conv_filter_reorderd")
#loc162 = loc("model.23.cv2.conv.bias")
#loc163 = loc("319_Conv_filter_reorderd")
#loc164 = loc("model.23.cv3.conv.bias")
#loc165 = loc("323_Conv_filter_reorderd")
#loc166 = loc("285_Mul286_Conv_f16")
#loc167 = loc("305_Mul306_Conv_f16")
#loc168 = loc("325_Mul622_Conv_f16")
#loc169 = loc("load_145_Conv")
#loc170 = loc("145_Conv147_Mul_f32")
#loc171 = loc("147_Mul")
#loc172 = loc("load_148_Conv_filter_reorderd")
#loc173 = loc("load_model.4.cv1.conv.bias")
#loc174 = loc("147_Mul148_Conv_f16")
#loc175 = loc("148_Conv")
#loc176 = loc("148_Conv150_Mul_f32")
#loc177 = loc("load_model.4.m.0.cv1.conv.bias")
#loc178 = loc("150_Mul")
#loc179 = loc("load_151_Conv_filter_reorderd")
#loc180 = loc("150_Mul151_Conv_f16")
#loc181 = loc("151_Conv")
#loc182 = loc("151_Conv153_Mul_f32")
#loc183 = loc("load_154_Conv_filter_reorderd")
#loc184 = loc("load_model.4.m.0.cv2.conv.bias")
#loc185 = loc("153_Mul")
#loc186 = loc("153_Mul154_Conv_f16")
#loc187 = loc("154_Conv")
#loc188 = loc("154_Conv156_Mul_f32")
#loc189 = loc("load_model.4.m.1.cv1.conv.bias")
#loc190 = loc("156_Mul")
#loc191 = loc("156_Mul157_Add_f16")
#loc192 = loc("load_158_Conv_filter_reorderd")
#loc193 = loc("157_Add")
#loc194 = loc("158_Conv")
#loc195 = loc("158_Conv160_Mul_f32")
#loc196 = loc("load_161_Conv_filter_reorderd")
#loc197 = loc("load_model.4.m.1.cv2.conv.bias")
#loc198 = loc("160_Mul")
#loc199 = loc("160_Mul161_Conv_f16")
#loc200 = loc("161_Conv")
#loc201 = loc("161_Conv163_Mul_f32")
#loc202 = loc("load_model.4.cv2.conv.bias")
#loc203 = loc("163_Mul")
#loc204 = loc("163_Mul164_Add_f16")
#loc205 = loc("load_165_Conv_filter_reorderd")
#loc206 = loc("164_Add")
#loc207 = loc("165_Conv")
#loc208 = loc("165_Conv167_Mul_f32")
#loc209 = loc("load_model.4.cv3.conv.bias")
#loc210 = loc("load_169_Conv_filter_reorderd")
#loc211 = loc("167_Mul")
#loc212 = loc("167_Mul168_Concat_f16")
#loc213 = loc("168_Concat")
#loc214 = loc("169_Conv")
#loc215 = loc("169_Conv171_Mul_f32")
#loc216 = loc("load_172_Conv_filter_reorderd")
#loc217 = loc("load_model.5.conv.bias")
#loc218 = loc("171_Mul")
#loc219 = loc("171_Mul172_Conv_f16")
#loc220 = loc("load_206_Conv_filter_reorderd")
#loc221 = loc("172_Conv")
#loc222 = loc("172_Conv174_Mul_f32")
#loc223 = loc("load_model.6.cv1.conv.bias")
#loc224 = loc("load_175_Conv_filter_reorderd")
#loc225 = loc("174_Mul")
#loc226 = loc("174_Mul175_Conv_f16")
#loc227 = loc("175_Conv")
#loc228 = loc("175_Conv177_Mul_f32")
#loc229 = loc("load_model.6.m.0.cv1.conv.bias")
#loc230 = loc("load_178_Conv_filter_reorderd")
#loc231 = loc("177_Mul")
#loc232 = loc("177_Mul178_Conv_f16")
#loc233 = loc("178_Conv")
#loc234 = loc("178_Conv180_Mul_f32")
#loc235 = loc("load_181_Conv_filter_reorderd")
#loc236 = loc("load_model.6.m.0.cv2.conv.bias")
#loc237 = loc("180_Mul")
#loc238 = loc("180_Mul181_Conv_f16")
#loc239 = loc("181_Conv")
#loc240 = loc("181_Conv183_Mul_f32")
#loc241 = loc("load_model.6.m.1.cv1.conv.bias")
#loc242 = loc("load_185_Conv_filter_reorderd")
#loc243 = loc("183_Mul")
#loc244 = loc("183_Mul184_Add_f16")
#loc245 = loc("184_Add")
#loc246 = loc("185_Conv")
#loc247 = loc("185_Conv187_Mul_f32")
#loc248 = loc("load_188_Conv_filter_reorderd")
#loc249 = loc("load_model.6.m.1.cv2.conv.bias")
#loc250 = loc("187_Mul")
#loc251 = loc("187_Mul188_Conv_f16")
#loc252 = loc("188_Conv")
#loc253 = loc("188_Conv190_Mul_f32")
#loc254 = loc("load_model.6.m.2.cv1.conv.bias")
#loc255 = loc("load_192_Conv_filter_reorderd")
#loc256 = loc("190_Mul")
#loc257 = loc("190_Mul191_Add_f16")
#loc258 = loc("191_Add")
#loc259 = loc("192_Conv")
#loc260 = loc("192_Conv194_Mul_f32")
#loc261 = loc("load_195_Conv_filter_reorderd")
#loc262 = loc("load_model.6.m.2.cv2.conv.bias")
#loc263 = loc("194_Mul")
#loc264 = loc("194_Mul195_Conv_f16")
#loc265 = loc("195_Conv")
#loc266 = loc("195_Conv197_Mul_f32")
#loc267 = loc("load_model.6.cv2.conv.bias")
#loc268 = loc("load_199_Conv_filter_reorderd")
#loc269 = loc("197_Mul")
#loc270 = loc("197_Mul198_Add_f16")
#loc271 = loc("198_Add")
#loc272 = loc("199_Conv")
#loc273 = loc("199_Conv201_Mul_f32")
#loc274 = loc("load_model.6.cv3.conv.bias")
#loc275 = loc("load_203_Conv_filter_reorderd")
#loc276 = loc("201_Mul")
#loc277 = loc("201_Mul202_Concat_f16")
#loc278 = loc("202_Concat")
#loc279 = loc("203_Conv")
#loc280 = loc("203_Conv205_Mul_f32")
#loc281 = loc("load_model.7.conv.bias")
#loc282 = loc("205_Mul")
#loc283 = loc("205_Mul206_Conv_f16")
#loc284 = loc("load_215_Conv_filter_reorderd")
#loc285 = loc("load_233_Conv_filter_reorderd")
#loc286 = loc("206_Conv")
#loc287 = loc("206_Conv208_Mul_f32")
#loc288 = loc("load_model.8.cv1.conv.bias")
#loc289 = loc("load_209_Conv_filter_reorderd")
#loc290 = loc("208_Mul")
#loc291 = loc("208_Mul209_Conv_f16")
#loc292 = loc("load_model.8.m.0.cv1.conv.bias")
#loc293 = loc("209_Conv")
#loc294 = loc("209_Conv211_Mul_f32")
#loc295 = loc("load_212_Conv_filter_reorderd")
#loc296 = loc("211_Mul")
#loc297 = loc("211_Mul212_Conv_f16")
#loc298 = loc("212_Conv")
#loc299 = loc("212_Conv214_Mul_f32")
#loc300 = loc("load_model.8.m.0.cv2.conv.bias")
#loc301 = loc("214_Mul")
#loc302 = loc("214_Mul215_Conv_f16")
#loc303 = loc("load_model.8.cv2.conv.bias")
#loc304 = loc("load_model.8.cv3.conv.bias")
#loc305 = loc("load_223_Conv_filter_reorderd")
#loc306 = loc("215_Conv")
#loc307 = loc("215_Conv217_Mul_f32")
#loc308 = loc("load_219_Conv_filter_reorderd")
#loc309 = loc("217_Mul")
#loc310 = loc("217_Mul218_Add_f16")
#loc311 = loc("218_Add")
#loc312 = loc("219_Conv")
#loc313 = loc("219_Conv221_Mul_f32")
#loc314 = loc("221_Mul")
#loc315 = loc("221_Mul222_Concat_f16")
#loc316 = loc("222_Concat")
#loc317 = loc("load_model.9.cv2.conv.bias")
#loc318 = loc("223_Conv")
#loc319 = loc("223_Conv225_Mul_f32")
#loc320 = loc("load_model.9.cv1.conv.bias")
#loc321 = loc("load_226_Conv_filter_reorderd")
#loc322 = loc("225_Mul")
#loc323 = loc("225_Mul226_Conv_f16")
#loc324 = loc("226_Conv")
#loc325 = loc("226_Conv228_Mul_f32")
#loc326 = loc("228_Mul")
#loc327 = loc("228_Mul229_MaxPool_f16")
#loc328 = loc("229_MaxPool")
#loc329 = loc("230_MaxPool")
#loc330 = loc("231_MaxPool")
#loc331 = loc("232_Concat")
#loc332 = loc("233_Conv")
#loc333 = loc("233_Conv235_Mul_f32")
#loc334 = loc("load_model.10.conv.bias")
#loc335 = loc("load_236_Conv_filter_reorderd")
#loc336 = loc("235_Mul")
#loc337 = loc("235_Mul236_Conv_f16")
#loc338 = loc("236_Conv")
#loc339 = loc("236_Conv238_Mul_f32")
#loc340 = loc("load_model.13.cv1.conv.bias")
#loc341 = loc("load_245_Conv_filter_reorderd")
#loc342 = loc("238_Mul")
#loc343 = loc("238_Mul243_Resize_f16")
#loc344 = loc("243_Resize")
#loc345 = loc("244_Concat")
#loc346 = loc("245_Conv")
#loc347 = loc("245_Conv247_Mul_f32")
#loc348 = loc("load_model.13.m.0.cv1.conv.bias")
#loc349 = loc("load_248_Conv_filter_reorderd")
#loc350 = loc("247_Mul")
#loc351 = loc("247_Mul248_Conv_f16")
#loc352 = loc("248_Conv")
#loc353 = loc("248_Conv250_Mul_f32")
#loc354 = loc("load_251_Conv_filter_reorderd")
#loc355 = loc("load_model.13.m.0.cv2.conv.bias")
#loc356 = loc("250_Mul")
#loc357 = loc("250_Mul251_Conv_f16")
#loc358 = loc("251_Conv")
#loc359 = loc("251_Conv253_Mul_f32")
#loc360 = loc("load_model.13.cv2.conv.bias")
#loc361 = loc("load_254_Conv_filter_reorderd")
#loc362 = loc("253_Mul")
#loc363 = loc("253_Mul257_Concat_f16")
#loc364 = loc("254_Conv")
#loc365 = loc("254_Conv256_Mul_f32")
#loc366 = loc("load_model.13.cv3.conv.bias")
#loc367 = loc("load_258_Conv_filter_reorderd")
#loc368 = loc("256_Mul")
#loc369 = loc("256_Mul257_Concat_f16")
#loc370 = loc("257_Concat")
#loc371 = loc("258_Conv")
#loc372 = loc("258_Conv260_Mul_f32")
#loc373 = loc("load_model.14.conv.bias")
#loc374 = loc("load_261_Conv_filter_reorderd")
#loc375 = loc("260_Mul")
#loc376 = loc("260_Mul261_Conv_f16")
#loc377 = loc("261_Conv")
#loc378 = loc("261_Conv263_Mul_f32")
#loc379 = loc("load_model.17.cv1.conv.bias")
#loc380 = loc("263_Mul")
#loc381 = loc("263_Mul268_Resize_f16")
#loc382 = loc("268_Resize")
#loc383 = loc("load_270_Conv_filter_reorderd")
#loc384 = loc("269_Concat")
#loc385 = loc("270_Conv")
#loc386 = loc("270_Conv272_Mul_f32")
#loc387 = loc("load_model.17.m.0.cv1.conv.bias")
#loc388 = loc("272_Mul")
#loc389 = loc("load_273_Conv_filter_reorderd")
#loc390 = loc("272_Mul273_Conv_f16")
#loc391 = loc("273_Conv")
#loc392 = loc("273_Conv275_Mul_f32")
#loc393 = loc("load_276_Conv_filter_reorderd")
#loc394 = loc("load_model.17.m.0.cv2.conv.bias")
#loc395 = loc("275_Mul")
#loc396 = loc("275_Mul276_Conv_f16")
#loc397 = loc("276_Conv")
#loc398 = loc("276_Conv278_Mul_f32")
#loc399 = loc("load_model.17.cv2.conv.bias")
#loc400 = loc("278_Mul")
#loc401 = loc("load_279_Conv_filter_reorderd")
#loc402 = loc("278_Mul282_Concat_f16")
#loc403 = loc("279_Conv")
#loc404 = loc("279_Conv281_Mul_f32")
#loc405 = loc("load_model.17.cv3.conv.bias")
#loc406 = loc("load_283_Conv_filter_reorderd")
#loc407 = loc("281_Mul")
#loc408 = loc("281_Mul282_Concat_f16")
#loc409 = loc("282_Concat")
#loc410 = loc("283_Conv")
#loc411 = loc("283_Conv285_Mul_f32")
#loc412 = loc("load_286_Conv_filter_reorderd")
#loc413 = loc("load_model.18.conv.bias")
#loc414 = loc("285_Mul")
#loc415 = loc("286_Conv")
#loc416 = loc("286_Conv288_Mul_f32")
#loc417 = loc("load_model.20.cv1.conv.bias")
#loc418 = loc("load_290_Conv_filter_reorderd")
#loc419 = loc("288_Mul")
#loc420 = loc("288_Mul289_Concat_f16")
#loc421 = loc("289_Concat")
#loc422 = loc("290_Conv")
#loc423 = loc("290_Conv292_Mul_f32")
#loc424 = loc("load_model.20.m.0.cv1.conv.bias")
#loc425 = loc("load_293_Conv_filter_reorderd")
#loc426 = loc("292_Mul")
#loc427 = loc("292_Mul293_Conv_f16")
#loc428 = loc("293_Conv")
#loc429 = loc("293_Conv295_Mul_f32")
#loc430 = loc("load_296_Conv_filter_reorderd")
#loc431 = loc("load_model.20.m.0.cv2.conv.bias")
#loc432 = loc("295_Mul")
#loc433 = loc("295_Mul296_Conv_f16")
#loc434 = loc("load_306_Conv_filter_reorderd")
#loc435 = loc("296_Conv")
#loc436 = loc("296_Conv298_Mul_f32")
#loc437 = loc("load_model.20.cv2.conv.bias")
#loc438 = loc("load_299_Conv_filter_reorderd")
#loc439 = loc("298_Mul")
#loc440 = loc("298_Mul302_Concat_f16")
#loc441 = loc("299_Conv")
#loc442 = loc("299_Conv301_Mul_f32")
#loc443 = loc("load_model.20.cv3.conv.bias")
#loc444 = loc("load_303_Conv_filter_reorderd")
#loc445 = loc("301_Mul")
#loc446 = loc("301_Mul302_Concat_f16")
#loc447 = loc("302_Concat")
#loc448 = loc("load_model.21.conv.bias")
#loc449 = loc("303_Conv")
#loc450 = loc("303_Conv305_Mul_f32")
#loc451 = loc("load_316_Conv_filter_reorderd")
#loc452 = loc("305_Mul")
#loc453 = loc("load_model.23.cv1.conv.bias")
#loc454 = loc("306_Conv")
#loc455 = loc("306_Conv308_Mul_f32")
#loc456 = loc("load_310_Conv_filter_reorderd")
#loc457 = loc("308_Mul")
#loc458 = loc("308_Mul309_Concat_f16")
#loc459 = loc("309_Concat")
#loc460 = loc("load_model.23.m.0.cv1.conv.bias")
#loc461 = loc("310_Conv")
#loc462 = loc("310_Conv312_Mul_f32")
#loc463 = loc("load_313_Conv_filter_reorderd")
#loc464 = loc("312_Mul")
#loc465 = loc("312_Mul313_Conv_f16")
#loc466 = loc("313_Conv")
#loc467 = loc("313_Conv315_Mul_f32")
#loc468 = loc("load_model.23.m.0.cv2.conv.bias")
#loc469 = loc("315_Mul")
#loc470 = loc("315_Mul316_Conv_f16")
#loc471 = loc("load_model.23.cv2.conv.bias")
#loc472 = loc("load_model.23.cv3.conv.bias")
#loc473 = loc("load_323_Conv_filter_reorderd")
#loc474 = loc("316_Conv")
#loc475 = loc("316_Conv318_Mul_f32")
#loc476 = loc("load_319_Conv_filter_reorderd")
#loc477 = loc("318_Mul")
#loc478 = loc("318_Mul322_Concat_f16")
#loc479 = loc("319_Conv")
#loc480 = loc("319_Conv321_Mul_f32")
#loc481 = loc("321_Mul")
#loc482 = loc("321_Mul322_Concat_f16")
#loc483 = loc("322_Concat")
#loc484 = loc("323_Conv")
#loc485 = loc("323_Conv325_Mul_f32")
#loc486 = loc("325_Mul")
#loc487 = loc("model.24.m.0.bias")
#loc488 = loc("326_Conv_filter_reorderd")
#loc489 = loc("326_Conv")
#loc490 = loc("349_Reshape")
#loc491 = loc("350_Transpose")
#loc492 = loc("350_Transpose_f32")
#loc493 = loc("model.24.m.1.bias")
#loc494 = loc("474_Conv_filter_reorderd")
#loc495 = loc("474_Conv")
#loc496 = loc("497_Reshape")
#loc497 = loc("498_Transpose")
#loc498 = loc("498_Transpose_f32")
#loc499 = loc("model.24.m.2.bias")
#loc500 = loc("622_Conv_filter_reorderd")
#loc501 = loc("622_Conv")
#loc502 = loc("645_Reshape")
#loc503 = loc("646_Transpose")
#loc504 = loc("646_Transpose_f32")
#loc505 = loc(fused[#loc166, #loc167, #loc168])

